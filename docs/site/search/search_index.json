{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"reference/definitions/","text":"","title":"Definitions"},{"location":"reference/defs/_azureml/definitions/","text":"","title":"Definitions"},{"location":"reference/defs/_powerbi/definitions/","text":"","title":"Definitions"},{"location":"reference/defs/dbt/definitions/","text":"defs () Returns set of definitions explicitly available and loadable by Dagster tools. Will be automatically dectectd and loaded by the load_defs function in the root definitions file. @definitions decorator will provides lazy loading so that the assets are only instantiated when needed. Source code in data_platform\\defs\\dbt\\definitions.py 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 @definitions def defs () -> Definitions : \"\"\"Returns set of definitions explicitly available and loadable by Dagster tools. Will be automatically dectectd and loaded by the load_defs function in the root definitions file. @definitions decorator will provides lazy loading so that the assets are only instantiated when needed. \"\"\" project_dir = Path ( __file__ ) . joinpath ( * [ \"..\" ] * 4 , \"dbt/\" ) . resolve () state_path = \"state/\" def dbt () -> DbtProject : project = DbtProject ( project_dir = project_dir , target = os . getenv ( \"TARGET\" , \"prod\" ), state_path = state_path , profile = \"dbt\" , ) if os . getenv ( \"PREPARE_IF_DEV\" ) == \"1\" : project . prepare_if_dev () return project return DagsterDbtFactory . build_definitions ( dbt )","title":"Definitions"},{"location":"reference/defs/dbt/definitions/#data_platform.defs.dbt.definitions.defs","text":"Returns set of definitions explicitly available and loadable by Dagster tools. Will be automatically dectectd and loaded by the load_defs function in the root definitions file. @definitions decorator will provides lazy loading so that the assets are only instantiated when needed. Source code in data_platform\\defs\\dbt\\definitions.py 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 @definitions def defs () -> Definitions : \"\"\"Returns set of definitions explicitly available and loadable by Dagster tools. Will be automatically dectectd and loaded by the load_defs function in the root definitions file. @definitions decorator will provides lazy loading so that the assets are only instantiated when needed. \"\"\" project_dir = Path ( __file__ ) . joinpath ( * [ \"..\" ] * 4 , \"dbt/\" ) . resolve () state_path = \"state/\" def dbt () -> DbtProject : project = DbtProject ( project_dir = project_dir , target = os . getenv ( \"TARGET\" , \"prod\" ), state_path = state_path , profile = \"dbt\" , ) if os . getenv ( \"PREPARE_IF_DEV\" ) == \"1\" : project . prepare_if_dev () return project return DagsterDbtFactory . build_definitions ( dbt )","title":"defs"},{"location":"reference/defs/dlthub/resources/","text":"defs () Returns set of definitions explicitly available and loadable by Dagster tools. Will be automatically dectectd and loaded by the load_defs function in the root definitions file. Assets and asset checks for dltHub are defined in the dlthub subfolder in the definitions.py file for each resource. @definitions decorator will provides lazy loading so that the assets are only instantiated when needed. Source code in data_platform\\defs\\dlthub\\resources.py 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 @definitions def defs () -> Definitions : \"\"\"Returns set of definitions explicitly available and loadable by Dagster tools. Will be automatically dectectd and loaded by the load_defs function in the root definitions file. Assets and asset checks for dltHub are defined in the dlthub subfolder in the definitions.py file for each resource. @definitions decorator will provides lazy loading so that the assets are only instantiated when needed. \"\"\" import os import dagster as dg from dagster_dlt import DagsterDltResource from ...utils.keyvault_stub import SecretClient kv = SecretClient ( vault_url = os . getenv ( \"AZURE_KEYVAULT_URL\" ), credential = os . getenv ( \"AZURE_KEYVAULT_CREDENTIAL\" ), ) os . environ [ \"DESTINATION__SNOWFLAKE__CREDENTIALS__HOST\" ] = kv . get_secret ( \"DESTINATION__SNOWFLAKE__HOST\" ) os . environ [ \"DESTINATION__SNOWFLAKE__CREDENTIALS__USERNAME\" ] = kv . get_secret ( \"DESTINATION__SNOWFLAKE__USER\" ) os . environ [ \"DESTINATION__SNOWFLAKE__CREDENTIALS__PASSWORD\" ] = kv . get_secret ( \"DESTINATION__SNOWFLAKE__PASSWORD\" ) os . environ [ \"DESTINATION__SNOWFLAKE__CREDENTIALS__DATABASE\" ] = kv . get_secret ( \"DESTINATION__SNOWFLAKE__DATABASE\" ) os . environ [ \"DESTINATION__SNOWFLAKE__CREDENTIALS__ROLE\" ] = kv . get_secret ( \"DESTINATION__SNOWFLAKE__ROLE\" ) os . environ [ \"DESTINATION__SNOWFLAKE__CREDENTIALS__WAREHOUSE\" ] = kv . get_secret ( \"DESTINATION__SNOWFLAKE__WAREHOUSE\" ) os . environ [ \"ENABLE_DATASET_NAME_NORMALIZATION\" ] = \"false\" return dg . Definitions ( resources = { \"dlt\" : DagsterDltResource ()})","title":"Resources"},{"location":"reference/defs/dlthub/resources/#data_platform.defs.dlthub.resources.defs","text":"Returns set of definitions explicitly available and loadable by Dagster tools. Will be automatically dectectd and loaded by the load_defs function in the root definitions file. Assets and asset checks for dltHub are defined in the dlthub subfolder in the definitions.py file for each resource. @definitions decorator will provides lazy loading so that the assets are only instantiated when needed. Source code in data_platform\\defs\\dlthub\\resources.py 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 @definitions def defs () -> Definitions : \"\"\"Returns set of definitions explicitly available and loadable by Dagster tools. Will be automatically dectectd and loaded by the load_defs function in the root definitions file. Assets and asset checks for dltHub are defined in the dlthub subfolder in the definitions.py file for each resource. @definitions decorator will provides lazy loading so that the assets are only instantiated when needed. \"\"\" import os import dagster as dg from dagster_dlt import DagsterDltResource from ...utils.keyvault_stub import SecretClient kv = SecretClient ( vault_url = os . getenv ( \"AZURE_KEYVAULT_URL\" ), credential = os . getenv ( \"AZURE_KEYVAULT_CREDENTIAL\" ), ) os . environ [ \"DESTINATION__SNOWFLAKE__CREDENTIALS__HOST\" ] = kv . get_secret ( \"DESTINATION__SNOWFLAKE__HOST\" ) os . environ [ \"DESTINATION__SNOWFLAKE__CREDENTIALS__USERNAME\" ] = kv . get_secret ( \"DESTINATION__SNOWFLAKE__USER\" ) os . environ [ \"DESTINATION__SNOWFLAKE__CREDENTIALS__PASSWORD\" ] = kv . get_secret ( \"DESTINATION__SNOWFLAKE__PASSWORD\" ) os . environ [ \"DESTINATION__SNOWFLAKE__CREDENTIALS__DATABASE\" ] = kv . get_secret ( \"DESTINATION__SNOWFLAKE__DATABASE\" ) os . environ [ \"DESTINATION__SNOWFLAKE__CREDENTIALS__ROLE\" ] = kv . get_secret ( \"DESTINATION__SNOWFLAKE__ROLE\" ) os . environ [ \"DESTINATION__SNOWFLAKE__CREDENTIALS__WAREHOUSE\" ] = kv . get_secret ( \"DESTINATION__SNOWFLAKE__WAREHOUSE\" ) os . environ [ \"ENABLE_DATASET_NAME_NORMALIZATION\" ] = \"false\" return dg . Definitions ( resources = { \"dlt\" : DagsterDltResource ()})","title":"defs"},{"location":"reference/defs/dlthub/dlthub/exchange_rate/data/","text":"get_exchange_rate ( currency ) Return a generator that will yield responses from an api with daily exchange rates for the selected currency Source code in data_platform\\defs\\dlthub\\dlthub\\exchange_rate\\data.py 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 def get_exchange_rate ( currency : str ) -> Callable [[], Any ]: \"\"\"Return a generator that will yield responses from an api with daily exchange rates for the selected currency \"\"\" uri = ( \"https://cdn.jsdelivr.net/npm/@fawazahmed0/currency-api\" \"@latest\" \"/v1/\" f \"currencies/ { currency } .json\" ) def exchange_api () -> Generator [ Any , Any , None ]: response = requests . get ( uri ) yield response . json () while next_uri := response . json () . get ( \"next_page\" ): response = requests . get ( next_uri ) yield response . json () return exchange_api","title":"Data"},{"location":"reference/defs/dlthub/dlthub/exchange_rate/data/#data_platform.defs.dlthub.dlthub.exchange_rate.data.get_exchange_rate","text":"Return a generator that will yield responses from an api with daily exchange rates for the selected currency Source code in data_platform\\defs\\dlthub\\dlthub\\exchange_rate\\data.py 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 def get_exchange_rate ( currency : str ) -> Callable [[], Any ]: \"\"\"Return a generator that will yield responses from an api with daily exchange rates for the selected currency \"\"\" uri = ( \"https://cdn.jsdelivr.net/npm/@fawazahmed0/currency-api\" \"@latest\" \"/v1/\" f \"currencies/ { currency } .json\" ) def exchange_api () -> Generator [ Any , Any , None ]: response = requests . get ( uri ) yield response . json () while next_uri := response . json () . get ( \"next_page\" ): response = requests . get ( next_uri ) yield response . json () return exchange_api","title":"get_exchange_rate"},{"location":"reference/defs/dlthub/dlthub/exchange_rate/definitions/","text":"","title":"Definitions"},{"location":"reference/defs/dlthub/dlthub/facebook_ads/data/","text":"get_campaigns () A generator that will yield responses from a stub representing an api to download data from facebook ads. Source code in data_platform\\defs\\dlthub\\dlthub\\facebook_ads\\data.py 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 def get_campaigns () -> Generator [ list [ dict [ str , Any ]], Any , None ]: \"\"\"A generator that will yield responses from a stub representing an api to download data from facebook ads. \"\"\" response = [ { \"id\" : 90009 , \"name\" : \"summer_sale\" , \"start_date\" : \"2024-06-01\" , \"updated\" : \"2025-07-02 21:14:03\" , }, { \"id\" : 80008 , \"name\" : \"winter_sale\" , \"start_date\" : \"2024-01-01\" , \"updated\" : \"2025-07-02 21:14:03\" , }, { \"id\" : 70008 , \"name\" : \"LAPTOP\" , \"start_date\" : \"2024-01-01\" , \"updated\" : \"2025-07-03 21:14:03\" , }, { \"id\" : 60008 , \"name\" : \"greenfrog\" , \"start_date\" : \"2024-01-01\" , \"updated\" : \"2025-07-04 21:14:03\" , }, { \"id\" : 50008 , \"name\" : \"blowout\" , \"start_date\" : \"2024-01-01\" , \"updated\" : \"2025-07-05 21:14:03\" , }, { \"id\" : 40008 , \"name\" : \"raindays\" , \"start_date\" : \"2024-01-01\" , \"updated\" : \"2025-07-06 21:14:03\" , }, { \"id\" : 30008 , \"name\" : \"powersale\" , \"start_date\" : \"2024-01-01\" , \"updated\" : \"2025-07-07 21:14:03\" , }, { \"id\" : 20008 , \"name\" : \"sale11111\" , \"start_date\" : \"2024-01-01\" , \"updated\" : \"2025-07-08 21:14:03\" , }, { \"id\" : 10008 , \"name\" : \"sale11112\" , \"start_date\" : \"2024-01-01\" , \"updated\" : \"2025-07-09 21:14:03\" , }, { \"id\" : 11008 , \"name\" : \"sale11113\" , \"start_date\" : \"2024-01-01\" , \"updated\" : \"2025-07-10 21:14:03\" , }, { \"id\" : 12008 , \"name\" : \"sale11114\" , \"start_date\" : \"2024-01-01\" , \"updated\" : \"2025-07-11 21:14:03\" , }, { \"id\" : 13008 , \"name\" : \"sale11115\" , \"start_date\" : \"2024-01-01\" , \"updated\" : \"2025-07-12 21:14:03\" , }, { \"id\" : 14008 , \"name\" : \"sale11116\" , \"start_date\" : \"2024-01-01\" , \"updated\" : \"2025-07-13 21:14:03\" , }, { \"id\" : 15008 , \"name\" : \"sale11117\" , \"start_date\" : \"2024-01-01\" , \"updated\" : \"2025-07-14 21:14:03\" , }, { \"id\" : 16008 , \"name\" : \"sale11118\" , \"start_date\" : \"2024-01-01\" , \"updated\" : \"2025-07-15 21:14:03\" , }, { \"id\" : 17008 , \"name\" : \"sale11119\" , \"start_date\" : \"2024-01-01\" , \"updated\" : \"2025-07-16 21:14:03\" , }, { \"id\" : 18008 , \"name\" : \"sale11110\" , \"start_date\" : \"2024-01-01\" , \"updated\" : \"2025-07-17 21:14:03\" , }, { \"id\" : 19008 , \"name\" : \"sale11121\" , \"start_date\" : \"2024-01-01\" , \"updated\" : \"2025-07-18 21:14:03\" , }, { \"id\" : 11001 , \"name\" : \"sale11122\" , \"start_date\" : \"2024-01-01\" , \"updated\" : \"2025-07-19 21:14:03\" , }, { \"id\" : 11002 , \"name\" : \"sale11123\" , \"start_date\" : \"2024-01-01\" , \"updated\" : \"2025-07-20 21:14:03\" , }, { \"id\" : 11003 , \"name\" : \"sale11124\" , \"start_date\" : \"2024-01-01\" , \"updated\" : \"2025-07-21 21:14:03\" , }, { \"id\" : 11004 , \"name\" : \"sale11125\" , \"start_date\" : \"2024-01-01\" , \"updated\" : \"2025-07-22 21:14:03\" , }, { \"id\" : 11005 , \"name\" : \"sale11126\" , \"start_date\" : \"2024-01-01\" , \"updated\" : \"2025-07-23 21:14:03\" , }, { \"id\" : 11006 , \"name\" : \"sale11127\" , \"start_date\" : \"2024-01-01\" , \"updated\" : \"2025-07-24 21:14:03\" , }, { \"id\" : 66007 , \"name\" : \"sale11128\" , \"start_date\" : \"2024-01-01\" , \"updated\" : \"2025-07-25 21:14:03\" , }, { \"id\" : 43008 , \"name\" : \"sale11129\" , \"start_date\" : \"2024-01-01\" , \"updated\" : \"2025-07-26 21:14:03\" , }, { \"id\" : 76009 , \"name\" : \"sale11131\" , \"start_date\" : \"2024-01-01\" , \"updated\" : \"2025-07-27 21:14:03\" , }, { \"id\" : 65008 , \"name\" : \"sale11132\" , \"start_date\" : \"2024-01-01\" , \"updated\" : \"2025-07-28 21:14:03\" , }, { \"id\" : 54007 , \"name\" : \"sale11133\" , \"start_date\" : \"2024-01-01\" , \"updated\" : \"2025-07-29 21:14:03\" , }, { \"id\" : 43006 , \"name\" : \"sale11134\" , \"start_date\" : \"2024-01-01\" , \"updated\" : \"2025-07-31 21:14:03\" , }, { \"id\" : 32005 , \"name\" : \"sale11135\" , \"start_date\" : \"2024-01-01\" , \"updated\" : \"2025-08-01 21:14:03\" , }, { \"id\" : 21004 , \"name\" : \"sale11136\" , \"start_date\" : \"2024-01-01\" , \"updated\" : \"2025-08-02 21:14:03\" , }, { \"id\" : 54003 , \"name\" : \"sale11137\" , \"start_date\" : \"2024-01-01\" , \"updated\" : \"2025-08-03 21:14:03\" , }, { \"id\" : 66002 , \"name\" : \"sale11138\" , \"start_date\" : \"2024-01-01\" , \"updated\" : \"2025-08-04 21:14:03\" , }, { \"id\" : 77001 , \"name\" : \"sale11139\" , \"start_date\" : \"2024-01-01\" , \"updated\" : \"2025-08-05 21:14:03\" , }, { \"id\" : 88044 , \"name\" : \"sale11141\" , \"start_date\" : \"2024-01-01\" , \"updated\" : \"2025-08-06 21:14:03\" , }, { \"id\" : 99033 , \"name\" : \"sale11142\" , \"start_date\" : \"2024-01-01\" , \"updated\" : \"2025-08-07 21:14:03\" , }, { \"id\" : 33220 , \"name\" : \"sale11143\" , \"start_date\" : \"2024-01-01\" , \"updated\" : \"2025-08-08 21:14:03\" , }, ] yield response","title":"Data"},{"location":"reference/defs/dlthub/dlthub/facebook_ads/data/#data_platform.defs.dlthub.dlthub.facebook_ads.data.get_campaigns","text":"A generator that will yield responses from a stub representing an api to download data from facebook ads. Source code in data_platform\\defs\\dlthub\\dlthub\\facebook_ads\\data.py 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 def get_campaigns () -> Generator [ list [ dict [ str , Any ]], Any , None ]: \"\"\"A generator that will yield responses from a stub representing an api to download data from facebook ads. \"\"\" response = [ { \"id\" : 90009 , \"name\" : \"summer_sale\" , \"start_date\" : \"2024-06-01\" , \"updated\" : \"2025-07-02 21:14:03\" , }, { \"id\" : 80008 , \"name\" : \"winter_sale\" , \"start_date\" : \"2024-01-01\" , \"updated\" : \"2025-07-02 21:14:03\" , }, { \"id\" : 70008 , \"name\" : \"LAPTOP\" , \"start_date\" : \"2024-01-01\" , \"updated\" : \"2025-07-03 21:14:03\" , }, { \"id\" : 60008 , \"name\" : \"greenfrog\" , \"start_date\" : \"2024-01-01\" , \"updated\" : \"2025-07-04 21:14:03\" , }, { \"id\" : 50008 , \"name\" : \"blowout\" , \"start_date\" : \"2024-01-01\" , \"updated\" : \"2025-07-05 21:14:03\" , }, { \"id\" : 40008 , \"name\" : \"raindays\" , \"start_date\" : \"2024-01-01\" , \"updated\" : \"2025-07-06 21:14:03\" , }, { \"id\" : 30008 , \"name\" : \"powersale\" , \"start_date\" : \"2024-01-01\" , \"updated\" : \"2025-07-07 21:14:03\" , }, { \"id\" : 20008 , \"name\" : \"sale11111\" , \"start_date\" : \"2024-01-01\" , \"updated\" : \"2025-07-08 21:14:03\" , }, { \"id\" : 10008 , \"name\" : \"sale11112\" , \"start_date\" : \"2024-01-01\" , \"updated\" : \"2025-07-09 21:14:03\" , }, { \"id\" : 11008 , \"name\" : \"sale11113\" , \"start_date\" : \"2024-01-01\" , \"updated\" : \"2025-07-10 21:14:03\" , }, { \"id\" : 12008 , \"name\" : \"sale11114\" , \"start_date\" : \"2024-01-01\" , \"updated\" : \"2025-07-11 21:14:03\" , }, { \"id\" : 13008 , \"name\" : \"sale11115\" , \"start_date\" : \"2024-01-01\" , \"updated\" : \"2025-07-12 21:14:03\" , }, { \"id\" : 14008 , \"name\" : \"sale11116\" , \"start_date\" : \"2024-01-01\" , \"updated\" : \"2025-07-13 21:14:03\" , }, { \"id\" : 15008 , \"name\" : \"sale11117\" , \"start_date\" : \"2024-01-01\" , \"updated\" : \"2025-07-14 21:14:03\" , }, { \"id\" : 16008 , \"name\" : \"sale11118\" , \"start_date\" : \"2024-01-01\" , \"updated\" : \"2025-07-15 21:14:03\" , }, { \"id\" : 17008 , \"name\" : \"sale11119\" , \"start_date\" : \"2024-01-01\" , \"updated\" : \"2025-07-16 21:14:03\" , }, { \"id\" : 18008 , \"name\" : \"sale11110\" , \"start_date\" : \"2024-01-01\" , \"updated\" : \"2025-07-17 21:14:03\" , }, { \"id\" : 19008 , \"name\" : \"sale11121\" , \"start_date\" : \"2024-01-01\" , \"updated\" : \"2025-07-18 21:14:03\" , }, { \"id\" : 11001 , \"name\" : \"sale11122\" , \"start_date\" : \"2024-01-01\" , \"updated\" : \"2025-07-19 21:14:03\" , }, { \"id\" : 11002 , \"name\" : \"sale11123\" , \"start_date\" : \"2024-01-01\" , \"updated\" : \"2025-07-20 21:14:03\" , }, { \"id\" : 11003 , \"name\" : \"sale11124\" , \"start_date\" : \"2024-01-01\" , \"updated\" : \"2025-07-21 21:14:03\" , }, { \"id\" : 11004 , \"name\" : \"sale11125\" , \"start_date\" : \"2024-01-01\" , \"updated\" : \"2025-07-22 21:14:03\" , }, { \"id\" : 11005 , \"name\" : \"sale11126\" , \"start_date\" : \"2024-01-01\" , \"updated\" : \"2025-07-23 21:14:03\" , }, { \"id\" : 11006 , \"name\" : \"sale11127\" , \"start_date\" : \"2024-01-01\" , \"updated\" : \"2025-07-24 21:14:03\" , }, { \"id\" : 66007 , \"name\" : \"sale11128\" , \"start_date\" : \"2024-01-01\" , \"updated\" : \"2025-07-25 21:14:03\" , }, { \"id\" : 43008 , \"name\" : \"sale11129\" , \"start_date\" : \"2024-01-01\" , \"updated\" : \"2025-07-26 21:14:03\" , }, { \"id\" : 76009 , \"name\" : \"sale11131\" , \"start_date\" : \"2024-01-01\" , \"updated\" : \"2025-07-27 21:14:03\" , }, { \"id\" : 65008 , \"name\" : \"sale11132\" , \"start_date\" : \"2024-01-01\" , \"updated\" : \"2025-07-28 21:14:03\" , }, { \"id\" : 54007 , \"name\" : \"sale11133\" , \"start_date\" : \"2024-01-01\" , \"updated\" : \"2025-07-29 21:14:03\" , }, { \"id\" : 43006 , \"name\" : \"sale11134\" , \"start_date\" : \"2024-01-01\" , \"updated\" : \"2025-07-31 21:14:03\" , }, { \"id\" : 32005 , \"name\" : \"sale11135\" , \"start_date\" : \"2024-01-01\" , \"updated\" : \"2025-08-01 21:14:03\" , }, { \"id\" : 21004 , \"name\" : \"sale11136\" , \"start_date\" : \"2024-01-01\" , \"updated\" : \"2025-08-02 21:14:03\" , }, { \"id\" : 54003 , \"name\" : \"sale11137\" , \"start_date\" : \"2024-01-01\" , \"updated\" : \"2025-08-03 21:14:03\" , }, { \"id\" : 66002 , \"name\" : \"sale11138\" , \"start_date\" : \"2024-01-01\" , \"updated\" : \"2025-08-04 21:14:03\" , }, { \"id\" : 77001 , \"name\" : \"sale11139\" , \"start_date\" : \"2024-01-01\" , \"updated\" : \"2025-08-05 21:14:03\" , }, { \"id\" : 88044 , \"name\" : \"sale11141\" , \"start_date\" : \"2024-01-01\" , \"updated\" : \"2025-08-06 21:14:03\" , }, { \"id\" : 99033 , \"name\" : \"sale11142\" , \"start_date\" : \"2024-01-01\" , \"updated\" : \"2025-08-07 21:14:03\" , }, { \"id\" : 33220 , \"name\" : \"sale11143\" , \"start_date\" : \"2024-01-01\" , \"updated\" : \"2025-08-08 21:14:03\" , }, ] yield response","title":"get_campaigns"},{"location":"reference/defs/dlthub/dlthub/facebook_ads/definitions/","text":"","title":"Definitions"},{"location":"reference/defs/dlthub/dlthub/google_ads/data/","text":"google_ads ( endpoint ) Return a generator that will yield responses from a stub representing an api to download data from google ads. Source code in data_platform\\defs\\dlthub\\dlthub\\google_ads\\data.py 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 def google_ads ( endpoint ) -> Callable [[], Generator [ list [ dict [ str , Any ]], Any , None ]]: \"\"\"Return a generator that will yield responses from a stub representing an api to download data from google ads. \"\"\" if endpoint == \"get_campaigns\" : def get_campaigns () -> Generator [ list [ dict [ str , Any ]], Any , None ]: response = [ { \"id\" : 10001 , \"name\" : \"summer_sale\" , \"start_date\" : \"2024-06-01\" , \"criteria\" : [{ \"id\" : 1 }, { \"id\" : 2 }], }, { \"id\" : 20002 , \"name\" : \"winter_sale\" , \"start_date\" : \"2024-01-01\" , \"criteria\" : [{ \"id\" : 2 }], }, ] yield response return get_campaigns if endpoint == \"get_criterion\" : def get_criterion () -> Generator [ list [ dict [ str , Any ]], Any , None ]: response = [ { \"id\" : 1 , \"type\" : \"audience\" , \"value\" : \"summer_shoppers\" }, { \"id\" : 2 , \"type\" : \"age\" , \"value\" : \"20-35\" }, ] yield response return get_criterion else : raise KeyError ( f \"Endpoint ' { endpoint } ' is not implemented.\" )","title":"Data"},{"location":"reference/defs/dlthub/dlthub/google_ads/data/#data_platform.defs.dlthub.dlthub.google_ads.data.google_ads","text":"Return a generator that will yield responses from a stub representing an api to download data from google ads. Source code in data_platform\\defs\\dlthub\\dlthub\\google_ads\\data.py 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 def google_ads ( endpoint ) -> Callable [[], Generator [ list [ dict [ str , Any ]], Any , None ]]: \"\"\"Return a generator that will yield responses from a stub representing an api to download data from google ads. \"\"\" if endpoint == \"get_campaigns\" : def get_campaigns () -> Generator [ list [ dict [ str , Any ]], Any , None ]: response = [ { \"id\" : 10001 , \"name\" : \"summer_sale\" , \"start_date\" : \"2024-06-01\" , \"criteria\" : [{ \"id\" : 1 }, { \"id\" : 2 }], }, { \"id\" : 20002 , \"name\" : \"winter_sale\" , \"start_date\" : \"2024-01-01\" , \"criteria\" : [{ \"id\" : 2 }], }, ] yield response return get_campaigns if endpoint == \"get_criterion\" : def get_criterion () -> Generator [ list [ dict [ str , Any ]], Any , None ]: response = [ { \"id\" : 1 , \"type\" : \"audience\" , \"value\" : \"summer_shoppers\" }, { \"id\" : 2 , \"type\" : \"age\" , \"value\" : \"20-35\" }, ] yield response return get_criterion else : raise KeyError ( f \"Endpoint ' { endpoint } ' is not implemented.\" )","title":"google_ads"},{"location":"reference/defs/dlthub/dlthub/google_ads/definitions/","text":"","title":"Definitions"},{"location":"reference/defs/sling/definitions/","text":"defs () Returns set of definitions explicitly available and loadable by Dagster tools. Will be automatically dectectd and loaded by the load_defs function in the root definitions file. @definitions decorator will provides lazy loading so that the assets are only instantiated when needed. Source code in data_platform\\defs\\sling\\definitions.py 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 @definitions def defs () -> Definitions : \"\"\"Returns set of definitions explicitly available and loadable by Dagster tools. Will be automatically dectectd and loaded by the load_defs function in the root definitions file. @definitions decorator will provides lazy loading so that the assets are only instantiated when needed. \"\"\" from pathlib import Path from ...lib.sling import DagsterSlingFactory config_dir = Path ( __file__ ) . joinpath ( * [ \"..\" ], \"sling\" ) . resolve () return DagsterSlingFactory . build_definitions ( config_dir )","title":"Definitions"},{"location":"reference/defs/sling/definitions/#data_platform.defs.sling.definitions.defs","text":"Returns set of definitions explicitly available and loadable by Dagster tools. Will be automatically dectectd and loaded by the load_defs function in the root definitions file. @definitions decorator will provides lazy loading so that the assets are only instantiated when needed. Source code in data_platform\\defs\\sling\\definitions.py 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 @definitions def defs () -> Definitions : \"\"\"Returns set of definitions explicitly available and loadable by Dagster tools. Will be automatically dectectd and loaded by the load_defs function in the root definitions file. @definitions decorator will provides lazy loading so that the assets are only instantiated when needed. \"\"\" from pathlib import Path from ...lib.sling import DagsterSlingFactory config_dir = Path ( __file__ ) . joinpath ( * [ \"..\" ], \"sling\" ) . resolve () return DagsterSlingFactory . build_definitions ( config_dir )","title":"defs"},{"location":"reference/lib/__init__/","text":"","title":"  init  "},{"location":"reference/lib/dbt/__init__/","text":"DagsterDbtFactory Factory to generate dagster definitions from a dbt project. Source code in data_platform\\lib\\dbt\\__init__.py 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 class DagsterDbtFactory : \"\"\"Factory to generate dagster definitions from a dbt project.\"\"\" @cache @staticmethod def build_definitions ( dbt : Callable [[], DbtProject ]) -> dg . Definitions : \"\"\"Returns a Definitions object from a dbt project.\"\"\" assets = [ DagsterDbtFactory . _get_assets ( \"dbt_partitioned_models\" , dbt = dbt , select = TIME_PARTITION_SELECTOR , partitioned = True , ), DagsterDbtFactory . _get_assets ( \"dbt_non_partitioned_models\" , dbt = dbt , exclude = TIME_PARTITION_SELECTOR , partitioned = False , ), ] freshness_checks = build_freshness_checks_from_dbt_assets ( dbt_assets = assets ) freshness_sensor = dg . build_sensor_for_freshness_checks ( freshness_checks = freshness_checks , name = \"dbt_freshness_checks_sensor\" ) return dg . Definitions ( resources = { \"dbt\" : DbtCliResource ( project_dir = dbt (), ) }, assets = assets , asset_checks = freshness_checks , sensors = [ freshness_sensor ], ) @cache @staticmethod def _get_assets ( name : str | None , dbt : Callable [[], DbtProject ], partitioned : bool = False , select : str = DBT_DEFAULT_SELECT , exclude : str | None = None , ) -> dg . AssetsDefinition : \"\"\"Returns a AssetsDefinition with different execution for partitioned and non-partitioned models so that they can be ran on the same job. \"\"\" dbt_project = dbt () assert dbt_project @dbt_assets ( name = name , manifest = dbt_project . manifest_path , select = select , exclude = exclude , dagster_dbt_translator = CustomDagsterDbtTranslator ( settings = DagsterDbtTranslatorSettings ( enable_duplicate_source_asset_keys = True , ) ), backfill_policy = dg . BackfillPolicy . single_run (), project = dbt_project , pool = \"dbt\" , ) def assets ( context : dg . AssetExecutionContext , dbt : DbtCliResource , config : DbtConfig ) -> Generator [ DbtEventIterator , Any , Any ]: args = [ \"build\" ] if config . full_refresh : args . append ( \"--full-refresh\" ) if config . defer_to_prod : args . extend ( dbt . get_defer_args ()) if config . favor_state : args . append ( \"--favor-state\" ) if partitioned : time_window = context . partition_time_window format = \"%Y-%m- %d %H:%M:%S\" dbt_vars = { \"min_date\" : time_window . start . strftime ( format ), \"max_date\" : time_window . end . strftime ( format ), } args . extend (( \"--vars\" , json . dumps ( dbt_vars ))) yield from dbt . cli ( args , context = context ) . stream () # .with_insights() # type: ignore else : yield from dbt . cli ( args , context = context ) . stream () # .with_insights() # type: ignore return assets build_definitions ( dbt ) cached staticmethod Returns a Definitions object from a dbt project. Source code in data_platform\\lib\\dbt\\__init__.py 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 @cache @staticmethod def build_definitions ( dbt : Callable [[], DbtProject ]) -> dg . Definitions : \"\"\"Returns a Definitions object from a dbt project.\"\"\" assets = [ DagsterDbtFactory . _get_assets ( \"dbt_partitioned_models\" , dbt = dbt , select = TIME_PARTITION_SELECTOR , partitioned = True , ), DagsterDbtFactory . _get_assets ( \"dbt_non_partitioned_models\" , dbt = dbt , exclude = TIME_PARTITION_SELECTOR , partitioned = False , ), ] freshness_checks = build_freshness_checks_from_dbt_assets ( dbt_assets = assets ) freshness_sensor = dg . build_sensor_for_freshness_checks ( freshness_checks = freshness_checks , name = \"dbt_freshness_checks_sensor\" ) return dg . Definitions ( resources = { \"dbt\" : DbtCliResource ( project_dir = dbt (), ) }, assets = assets , asset_checks = freshness_checks , sensors = [ freshness_sensor ], ) DbtConfig Bases: Config Exposes configuration options to end users in the Dagster launchpad. Source code in data_platform\\lib\\dbt\\__init__.py 24 25 26 27 28 29 30 31 class DbtConfig ( dg . Config ): \"\"\"Exposes configuration options to end users in the Dagster launchpad. \"\"\" full_refresh : bool = False defer_to_prod : bool = defer_to_prod favor_state : bool = False","title":"  init  "},{"location":"reference/lib/dbt/__init__/#data_platform.lib.dbt.DagsterDbtFactory","text":"Factory to generate dagster definitions from a dbt project. Source code in data_platform\\lib\\dbt\\__init__.py 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 class DagsterDbtFactory : \"\"\"Factory to generate dagster definitions from a dbt project.\"\"\" @cache @staticmethod def build_definitions ( dbt : Callable [[], DbtProject ]) -> dg . Definitions : \"\"\"Returns a Definitions object from a dbt project.\"\"\" assets = [ DagsterDbtFactory . _get_assets ( \"dbt_partitioned_models\" , dbt = dbt , select = TIME_PARTITION_SELECTOR , partitioned = True , ), DagsterDbtFactory . _get_assets ( \"dbt_non_partitioned_models\" , dbt = dbt , exclude = TIME_PARTITION_SELECTOR , partitioned = False , ), ] freshness_checks = build_freshness_checks_from_dbt_assets ( dbt_assets = assets ) freshness_sensor = dg . build_sensor_for_freshness_checks ( freshness_checks = freshness_checks , name = \"dbt_freshness_checks_sensor\" ) return dg . Definitions ( resources = { \"dbt\" : DbtCliResource ( project_dir = dbt (), ) }, assets = assets , asset_checks = freshness_checks , sensors = [ freshness_sensor ], ) @cache @staticmethod def _get_assets ( name : str | None , dbt : Callable [[], DbtProject ], partitioned : bool = False , select : str = DBT_DEFAULT_SELECT , exclude : str | None = None , ) -> dg . AssetsDefinition : \"\"\"Returns a AssetsDefinition with different execution for partitioned and non-partitioned models so that they can be ran on the same job. \"\"\" dbt_project = dbt () assert dbt_project @dbt_assets ( name = name , manifest = dbt_project . manifest_path , select = select , exclude = exclude , dagster_dbt_translator = CustomDagsterDbtTranslator ( settings = DagsterDbtTranslatorSettings ( enable_duplicate_source_asset_keys = True , ) ), backfill_policy = dg . BackfillPolicy . single_run (), project = dbt_project , pool = \"dbt\" , ) def assets ( context : dg . AssetExecutionContext , dbt : DbtCliResource , config : DbtConfig ) -> Generator [ DbtEventIterator , Any , Any ]: args = [ \"build\" ] if config . full_refresh : args . append ( \"--full-refresh\" ) if config . defer_to_prod : args . extend ( dbt . get_defer_args ()) if config . favor_state : args . append ( \"--favor-state\" ) if partitioned : time_window = context . partition_time_window format = \"%Y-%m- %d %H:%M:%S\" dbt_vars = { \"min_date\" : time_window . start . strftime ( format ), \"max_date\" : time_window . end . strftime ( format ), } args . extend (( \"--vars\" , json . dumps ( dbt_vars ))) yield from dbt . cli ( args , context = context ) . stream () # .with_insights() # type: ignore else : yield from dbt . cli ( args , context = context ) . stream () # .with_insights() # type: ignore return assets","title":"DagsterDbtFactory"},{"location":"reference/lib/dbt/__init__/#data_platform.lib.dbt.DagsterDbtFactory.build_definitions","text":"Returns a Definitions object from a dbt project. Source code in data_platform\\lib\\dbt\\__init__.py 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 @cache @staticmethod def build_definitions ( dbt : Callable [[], DbtProject ]) -> dg . Definitions : \"\"\"Returns a Definitions object from a dbt project.\"\"\" assets = [ DagsterDbtFactory . _get_assets ( \"dbt_partitioned_models\" , dbt = dbt , select = TIME_PARTITION_SELECTOR , partitioned = True , ), DagsterDbtFactory . _get_assets ( \"dbt_non_partitioned_models\" , dbt = dbt , exclude = TIME_PARTITION_SELECTOR , partitioned = False , ), ] freshness_checks = build_freshness_checks_from_dbt_assets ( dbt_assets = assets ) freshness_sensor = dg . build_sensor_for_freshness_checks ( freshness_checks = freshness_checks , name = \"dbt_freshness_checks_sensor\" ) return dg . Definitions ( resources = { \"dbt\" : DbtCliResource ( project_dir = dbt (), ) }, assets = assets , asset_checks = freshness_checks , sensors = [ freshness_sensor ], )","title":"build_definitions"},{"location":"reference/lib/dbt/__init__/#data_platform.lib.dbt.DbtConfig","text":"Bases: Config Exposes configuration options to end users in the Dagster launchpad. Source code in data_platform\\lib\\dbt\\__init__.py 24 25 26 27 28 29 30 31 class DbtConfig ( dg . Config ): \"\"\"Exposes configuration options to end users in the Dagster launchpad. \"\"\" full_refresh : bool = False defer_to_prod : bool = defer_to_prod favor_state : bool = False","title":"DbtConfig"},{"location":"reference/lib/dbt/constants/","text":"Constant values that are useful in selecting dbt models.","title":"Constants"},{"location":"reference/lib/dbt/translator/","text":"CustomDagsterDbtTranslator Bases: DagsterDbtTranslator Overrides methods of the standard translator. Holds a set of methods that derive Dagster asset definition metadata given a representation of a dbt resource (models, tests, sources, etc). Methods are overriden to customize the implementation. See parent class for details on the purpose of each override Source code in data_platform\\lib\\dbt\\translator.py 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 class CustomDagsterDbtTranslator ( DagsterDbtTranslator ): \"\"\"Overrides methods of the standard translator. Holds a set of methods that derive Dagster asset definition metadata given a representation of a dbt resource (models, tests, sources, etc). Methods are overriden to customize the implementation. See parent class for details on the purpose of each override\"\"\" @override def get_asset_key ( self , dbt_resource_props : Mapping [ str , Any ]) -> dg . AssetKey : meta = dbt_resource_props . get ( \"config\" , {}) . get ( \"meta\" , {} ) or dbt_resource_props . get ( \"meta\" , {}) meta_dagster = meta . get ( \"dagster\" ) or {} asset_key_config = meta_dagster . get ( \"asset_key\" ) if asset_key_config : return dg . AssetKey ( asset_key_config ) prop_key = \"name\" if dbt_resource_props . get ( \"version\" ): prop_key = \"alias\" if dbt_resource_props [ \"resource_type\" ] == \"source\" : schema = dbt_resource_props [ \"source_name\" ] table = dbt_resource_props [ \"name\" ] step = \"raw\" return dg . AssetKey ([ schema , step , table ]) parsed_name = re . search ( \"(.*?)_(.*)__(.*)\" , dbt_resource_props [ prop_key ]) if parsed_name : schema = parsed_name . group ( 2 ) table = parsed_name . group ( 3 ) step = parsed_name . group ( 1 ) return dg . AssetKey ([ schema , step , table ]) return super () . get_asset_key ( dbt_resource_props ) @override def get_group_name ( self , dbt_resource_props : Mapping [ str , Any ]) -> str | None : prop_key = \"name\" if dbt_resource_props . get ( \"version\" ): prop_key = \"alias\" parsed_name = re . search ( \"(.*?)_(.*)__(.*)\" , dbt_resource_props [ prop_key ]) if parsed_name : schema = parsed_name . group ( 2 ) return schema return super () . get_group_name ( dbt_resource_props ) @override def get_partitions_def ( self , dbt_resource_props : Mapping [ str , Any ] ) -> dg . PartitionsDefinition | None : meta = dbt_resource_props . get ( \"config\" , {}) . get ( \"meta\" , {}) . get ( \"dagster\" , {}) return get_partitions_def_from_meta ( meta ) @override def get_automation_condition ( self , dbt_resource_props : Mapping [ str , Any ] ) -> dg . AutomationCondition | None : meta = dbt_resource_props . get ( \"config\" , {}) . get ( \"meta\" , {}) . get ( \"dagster\" , {}) automation_condition = get_automation_condition_from_meta ( meta ) if automation_condition : return automation_condition # default settings for resource types resource_type = dbt_resource_props . get ( \"resource_type\" ) if resource_type == \"snapshot\" : return CustomAutomationCondition . eager_with_deps_checks () if resource_type == \"seed\" : return CustomAutomationCondition . code_version_changed () else : return CustomAutomationCondition . lazy () @override def get_tags ( self , dbt_resource_props : Mapping [ str , Any ]) -> Mapping [ str , str ]: tags = super () . get_tags ( dbt_resource_props ) return tags","title":"Translator"},{"location":"reference/lib/dbt/translator/#data_platform.lib.dbt.translator.CustomDagsterDbtTranslator","text":"Bases: DagsterDbtTranslator Overrides methods of the standard translator. Holds a set of methods that derive Dagster asset definition metadata given a representation of a dbt resource (models, tests, sources, etc). Methods are overriden to customize the implementation. See parent class for details on the purpose of each override Source code in data_platform\\lib\\dbt\\translator.py 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 class CustomDagsterDbtTranslator ( DagsterDbtTranslator ): \"\"\"Overrides methods of the standard translator. Holds a set of methods that derive Dagster asset definition metadata given a representation of a dbt resource (models, tests, sources, etc). Methods are overriden to customize the implementation. See parent class for details on the purpose of each override\"\"\" @override def get_asset_key ( self , dbt_resource_props : Mapping [ str , Any ]) -> dg . AssetKey : meta = dbt_resource_props . get ( \"config\" , {}) . get ( \"meta\" , {} ) or dbt_resource_props . get ( \"meta\" , {}) meta_dagster = meta . get ( \"dagster\" ) or {} asset_key_config = meta_dagster . get ( \"asset_key\" ) if asset_key_config : return dg . AssetKey ( asset_key_config ) prop_key = \"name\" if dbt_resource_props . get ( \"version\" ): prop_key = \"alias\" if dbt_resource_props [ \"resource_type\" ] == \"source\" : schema = dbt_resource_props [ \"source_name\" ] table = dbt_resource_props [ \"name\" ] step = \"raw\" return dg . AssetKey ([ schema , step , table ]) parsed_name = re . search ( \"(.*?)_(.*)__(.*)\" , dbt_resource_props [ prop_key ]) if parsed_name : schema = parsed_name . group ( 2 ) table = parsed_name . group ( 3 ) step = parsed_name . group ( 1 ) return dg . AssetKey ([ schema , step , table ]) return super () . get_asset_key ( dbt_resource_props ) @override def get_group_name ( self , dbt_resource_props : Mapping [ str , Any ]) -> str | None : prop_key = \"name\" if dbt_resource_props . get ( \"version\" ): prop_key = \"alias\" parsed_name = re . search ( \"(.*?)_(.*)__(.*)\" , dbt_resource_props [ prop_key ]) if parsed_name : schema = parsed_name . group ( 2 ) return schema return super () . get_group_name ( dbt_resource_props ) @override def get_partitions_def ( self , dbt_resource_props : Mapping [ str , Any ] ) -> dg . PartitionsDefinition | None : meta = dbt_resource_props . get ( \"config\" , {}) . get ( \"meta\" , {}) . get ( \"dagster\" , {}) return get_partitions_def_from_meta ( meta ) @override def get_automation_condition ( self , dbt_resource_props : Mapping [ str , Any ] ) -> dg . AutomationCondition | None : meta = dbt_resource_props . get ( \"config\" , {}) . get ( \"meta\" , {}) . get ( \"dagster\" , {}) automation_condition = get_automation_condition_from_meta ( meta ) if automation_condition : return automation_condition # default settings for resource types resource_type = dbt_resource_props . get ( \"resource_type\" ) if resource_type == \"snapshot\" : return CustomAutomationCondition . eager_with_deps_checks () if resource_type == \"seed\" : return CustomAutomationCondition . code_version_changed () else : return CustomAutomationCondition . lazy () @override def get_tags ( self , dbt_resource_props : Mapping [ str , Any ]) -> Mapping [ str , str ]: tags = super () . get_tags ( dbt_resource_props ) return tags","title":"CustomDagsterDbtTranslator"},{"location":"reference/lib/dlthub/__init__/","text":"ConfigurableDltResource Bases: DltResource Wrapper class to add aditional attributes to the DltResource class. These attributes are used in the factory to add aditional configuration such as automation conditions, asset checks, tags, and upstream external assets. Source code in data_platform\\lib\\dlthub\\__init__.py 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 class ConfigurableDltResource ( DltResource ): \"\"\"Wrapper class to add aditional attributes to the DltResource class. These attributes are used in the factory to add aditional configuration such as automation conditions, asset checks, tags, and upstream external assets. \"\"\" meta : dict | None tags : list | None kinds : set | None @staticmethod def config ( resource : DltResource , meta : dict | None = None , tags : list [ str ] | None = None , kinds : set [ str ] | None = None , ) -> \"ConfigurableDltResource\" : \"\"\"Returns a ConfigurableDltResource wrapped DltResource with aditional attributes used by the factory to generate enhanced definitions. \"\"\" resource = ConfigurableDltResource . _convert ( resource , meta , tags , kinds ) return resource # type: ignore @staticmethod def _convert ( dlt_resource : DltResource , meta : dict | None , tags : list [ str ] | None , kinds : set [ str ] | None , ) -> \"ConfigurableDltResource\" : \"\"\"Helper method to wrap a DltResource\"\"\" dlt_resource . tags = tags # type: ignore dlt_resource . meta = meta # type: ignore dlt_resource . kinds = kinds # type: ignore return dlt_resource # type: ignore config ( resource , meta = None , tags = None , kinds = None ) staticmethod Returns a ConfigurableDltResource wrapped DltResource with aditional attributes used by the factory to generate enhanced definitions. Source code in data_platform\\lib\\dlthub\\__init__.py 27 28 29 30 31 32 33 34 35 36 37 38 @staticmethod def config ( resource : DltResource , meta : dict | None = None , tags : list [ str ] | None = None , kinds : set [ str ] | None = None , ) -> \"ConfigurableDltResource\" : \"\"\"Returns a ConfigurableDltResource wrapped DltResource with aditional attributes used by the factory to generate enhanced definitions. \"\"\" resource = ConfigurableDltResource . _convert ( resource , meta , tags , kinds ) return resource # type: ignore","title":"  init  "},{"location":"reference/lib/dlthub/__init__/#data_platform.lib.dlthub.ConfigurableDltResource","text":"Bases: DltResource Wrapper class to add aditional attributes to the DltResource class. These attributes are used in the factory to add aditional configuration such as automation conditions, asset checks, tags, and upstream external assets. Source code in data_platform\\lib\\dlthub\\__init__.py 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 class ConfigurableDltResource ( DltResource ): \"\"\"Wrapper class to add aditional attributes to the DltResource class. These attributes are used in the factory to add aditional configuration such as automation conditions, asset checks, tags, and upstream external assets. \"\"\" meta : dict | None tags : list | None kinds : set | None @staticmethod def config ( resource : DltResource , meta : dict | None = None , tags : list [ str ] | None = None , kinds : set [ str ] | None = None , ) -> \"ConfigurableDltResource\" : \"\"\"Returns a ConfigurableDltResource wrapped DltResource with aditional attributes used by the factory to generate enhanced definitions. \"\"\" resource = ConfigurableDltResource . _convert ( resource , meta , tags , kinds ) return resource # type: ignore @staticmethod def _convert ( dlt_resource : DltResource , meta : dict | None , tags : list [ str ] | None , kinds : set [ str ] | None , ) -> \"ConfigurableDltResource\" : \"\"\"Helper method to wrap a DltResource\"\"\" dlt_resource . tags = tags # type: ignore dlt_resource . meta = meta # type: ignore dlt_resource . kinds = kinds # type: ignore return dlt_resource # type: ignore","title":"ConfigurableDltResource"},{"location":"reference/lib/dlthub/__init__/#data_platform.lib.dlthub.ConfigurableDltResource.config","text":"Returns a ConfigurableDltResource wrapped DltResource with aditional attributes used by the factory to generate enhanced definitions. Source code in data_platform\\lib\\dlthub\\__init__.py 27 28 29 30 31 32 33 34 35 36 37 38 @staticmethod def config ( resource : DltResource , meta : dict | None = None , tags : list [ str ] | None = None , kinds : set [ str ] | None = None , ) -> \"ConfigurableDltResource\" : \"\"\"Returns a ConfigurableDltResource wrapped DltResource with aditional attributes used by the factory to generate enhanced definitions. \"\"\" resource = ConfigurableDltResource . _convert ( resource , meta , tags , kinds ) return resource # type: ignore","title":"config"},{"location":"reference/lib/dlthub/translator/","text":"CustomDagsterDltTranslator Bases: DagsterDltTranslator Overrides methods of the standard translator. Holds a set of methods that derive Dagster asset definition metadata given a representation of dltHub resource (resources, pipes, etc). Methods are overriden to customize the implementation. See parent class for details on the purpose of each override Source code in data_platform\\lib\\dlthub\\translator.py 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 class CustomDagsterDltTranslator ( DagsterDltTranslator ): \"\"\"Overrides methods of the standard translator. Holds a set of methods that derive Dagster asset definition metadata given a representation of dltHub resource (resources, pipes, etc). Methods are overriden to customize the implementation. See parent class for details on the purpose of each override \"\"\" @override def get_asset_spec ( self , data : DltResourceTranslatorData ) -> dg . AssetSpec : return dg . AssetSpec ( key = self . _resolve_back_compat_method ( \"get_asset_key\" , self . _default_asset_key_fn , data . resource ), automation_condition = self . _resolve_back_compat_method ( \"get_automation_condition\" , self . _default_automation_condition_fn , data . resource , ), deps = self . _resolve_back_compat_method ( \"get_deps_asset_keys\" , self . _default_deps_fn , data . resource ), description = self . _resolve_back_compat_method ( \"get_description\" , self . _default_description_fn , data . resource ), group_name = self . _resolve_back_compat_method ( \"get_group_name\" , self . _default_group_name_fn , data . resource ), metadata = self . _resolve_back_compat_method ( \"get_metadata\" , self . _default_metadata_fn , data . resource ), owners = self . _resolve_back_compat_method ( \"get_owners\" , self . _default_owners_fn , data . resource ), tags = self . _resolve_back_compat_method ( \"get_tags\" , self . _default_tags_fn , data . resource ), kinds = self . _resolve_back_compat_method ( \"get_kinds\" , self . _default_kinds_fn , data . resource , data . destination ), partitions_def = self . get_partitions_def ( data . resource ), ) @override def get_deps_asset_keys ( self , resource : DltResource ) -> Iterable [ dg . AssetKey ]: name : str | None = None if resource . is_transformer : pipe = resource . _pipe while pipe . has_parent : pipe = pipe . parent name = pipe . schema . name # type: ignore else : name = resource . name if name : schema , table = name . split ( \".\" ) asset_key = [ schema , \"src\" , table ] return [ dg . AssetKey ( asset_key )] return super () . get_deps_asset_keys ( resource ) @override def get_asset_key ( self , resource : DltResource ) -> dg . AssetKey : schema , table = resource . name . split ( \".\" ) asset_key = [ schema , \"raw\" , table ] return dg . AssetKey ( asset_key ) @override def get_group_name ( self , resource : DltResource ) -> str : group = resource . name . split ( \".\" )[ 0 ] return group def get_partitions_def ( self , resource : DltResource ) -> dg . PartitionsDefinition | None : try : meta = resource . meta . get ( \"dagster\" ) # type: ignore return get_partitions_def_from_meta ( meta ) except Exception : ... return None @override def get_automation_condition ( self , resource : DltResource ) -> dg . AutomationCondition [ Any ] | None : try : meta = resource . meta . get ( \"dagster\" ) # type: ignore automation_condition = get_automation_condition_from_meta ( meta ) if automation_condition : return automation_condition except Exception : ... return super () . get_automation_condition ( resource ) @override def get_tags ( self , resource : DltResource ) -> Mapping [ str , Any ]: try : tags = resource . tags # type: ignore return { tag : \"\" for tag in tags if is_valid_tag_key ( tag )} except Exception : ... return {}","title":"Translator"},{"location":"reference/lib/dlthub/translator/#data_platform.lib.dlthub.translator.CustomDagsterDltTranslator","text":"Bases: DagsterDltTranslator Overrides methods of the standard translator. Holds a set of methods that derive Dagster asset definition metadata given a representation of dltHub resource (resources, pipes, etc). Methods are overriden to customize the implementation. See parent class for details on the purpose of each override Source code in data_platform\\lib\\dlthub\\translator.py 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 class CustomDagsterDltTranslator ( DagsterDltTranslator ): \"\"\"Overrides methods of the standard translator. Holds a set of methods that derive Dagster asset definition metadata given a representation of dltHub resource (resources, pipes, etc). Methods are overriden to customize the implementation. See parent class for details on the purpose of each override \"\"\" @override def get_asset_spec ( self , data : DltResourceTranslatorData ) -> dg . AssetSpec : return dg . AssetSpec ( key = self . _resolve_back_compat_method ( \"get_asset_key\" , self . _default_asset_key_fn , data . resource ), automation_condition = self . _resolve_back_compat_method ( \"get_automation_condition\" , self . _default_automation_condition_fn , data . resource , ), deps = self . _resolve_back_compat_method ( \"get_deps_asset_keys\" , self . _default_deps_fn , data . resource ), description = self . _resolve_back_compat_method ( \"get_description\" , self . _default_description_fn , data . resource ), group_name = self . _resolve_back_compat_method ( \"get_group_name\" , self . _default_group_name_fn , data . resource ), metadata = self . _resolve_back_compat_method ( \"get_metadata\" , self . _default_metadata_fn , data . resource ), owners = self . _resolve_back_compat_method ( \"get_owners\" , self . _default_owners_fn , data . resource ), tags = self . _resolve_back_compat_method ( \"get_tags\" , self . _default_tags_fn , data . resource ), kinds = self . _resolve_back_compat_method ( \"get_kinds\" , self . _default_kinds_fn , data . resource , data . destination ), partitions_def = self . get_partitions_def ( data . resource ), ) @override def get_deps_asset_keys ( self , resource : DltResource ) -> Iterable [ dg . AssetKey ]: name : str | None = None if resource . is_transformer : pipe = resource . _pipe while pipe . has_parent : pipe = pipe . parent name = pipe . schema . name # type: ignore else : name = resource . name if name : schema , table = name . split ( \".\" ) asset_key = [ schema , \"src\" , table ] return [ dg . AssetKey ( asset_key )] return super () . get_deps_asset_keys ( resource ) @override def get_asset_key ( self , resource : DltResource ) -> dg . AssetKey : schema , table = resource . name . split ( \".\" ) asset_key = [ schema , \"raw\" , table ] return dg . AssetKey ( asset_key ) @override def get_group_name ( self , resource : DltResource ) -> str : group = resource . name . split ( \".\" )[ 0 ] return group def get_partitions_def ( self , resource : DltResource ) -> dg . PartitionsDefinition | None : try : meta = resource . meta . get ( \"dagster\" ) # type: ignore return get_partitions_def_from_meta ( meta ) except Exception : ... return None @override def get_automation_condition ( self , resource : DltResource ) -> dg . AutomationCondition [ Any ] | None : try : meta = resource . meta . get ( \"dagster\" ) # type: ignore automation_condition = get_automation_condition_from_meta ( meta ) if automation_condition : return automation_condition except Exception : ... return super () . get_automation_condition ( resource ) @override def get_tags ( self , resource : DltResource ) -> Mapping [ str , Any ]: try : tags = resource . tags # type: ignore return { tag : \"\" for tag in tags if is_valid_tag_key ( tag )} except Exception : ... return {}","title":"CustomDagsterDltTranslator"},{"location":"reference/lib/sling/__init__/","text":"DagsterSlingFactory Factory to generate dagster definitions from Sling yaml config files. Source code in data_platform\\lib\\sling\\__init__.py 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 class DagsterSlingFactory : \"\"\"Factory to generate dagster definitions from Sling yaml config files.\"\"\" @cache @staticmethod def build_definitions ( config_dir : Path ) -> dg . Definitions : \"\"\"Returns a Definitions object for a path that contains Sling yaml configs.\"\"\" connections = [] assets = [] freshness_checks = [] kind_map = {} for config_path in os . listdir ( config_dir ): if config_path . endswith ( \".yaml\" ) or config_path . endswith ( \".yml\" ): config_path = config_dir . joinpath ( config_path ) . resolve () with open ( config_path ) as file : config = yaml . load ( file , Loader = yaml . FullLoader ) if not config : continue if connection_configs := config . get ( \"connections\" ): connections , kind_map = DagsterSlingFactory . _get_connections ( connection_configs , connections , kind_map ) if replication_configs := config . get ( \"replications\" ): assets , freshness_checks = DagsterSlingFactory . _get_replications ( replication_configs , freshness_checks , kind_map , assets ) return dg . Definitions ( resources = { \"sling\" : SlingResource ( connections = connections )}, assets = assets , asset_checks = freshness_checks , sensors = [ dg . build_sensor_for_freshness_checks ( freshness_checks = freshness_checks , name = \"sling_freshness_checks_sensor\" , ) ], ) @staticmethod def _get_connections ( connection_configs , connections , kind_map ) -> tuple [ list [ SlingConnectionResource ], dict [ str , str ]]: \"\"\"Returns a list of SlingConnectionResource for connections in the Sling yaml file. \"\"\" for connection_config in connection_configs : if connection := DagsterSlingFactory . _get_connection ( connection_config ): source = connection_config . get ( \"name\" ) kind = connection_config . get ( \"type\" ) kind_map [ source ] = kind connections . append ( connection ) return connections , kind_map @staticmethod def _get_connection ( connection_config : dict ) -> SlingConnectionResource | None : \"\"\"Returns a SlingConnectionResource for a connection in the Sling yaml file.\"\"\" for k , v in connection_config . items (): if isinstance ( v , dict ): secret_name = list ( v . keys ())[ 0 ] display_type = list ( v . values ())[ 0 ] if display_type == \"show\" : connection_config [ k ] = get_secret ( secret_name ) . get_value () else : connection_config [ k ] = get_secret ( secret_name ) connection = SlingConnectionResource ( ** connection_config ) return connection @staticmethod def _get_replications ( replication_configs , freshness_checks , kind_map , assets ) -> tuple [ list [ dg . AssetsDefinition ], list [ dg . AssetChecksDefinition ]]: \"\"\"Returns a list of AssetsDefinitions for replications in a Sling yaml file \"\"\" for replication_config in replication_configs : if bool ( os . getenv ( \"DAGSTER_IS_DEV_CLI\" )): replication_config = DagsterSlingFactory . _set_dev_schema ( replication_config ) assets_definition = DagsterSlingFactory . _get_replication ( replication_config ) kind = kind_map . get ( replication_config . get ( \"source\" , None ), None ) dep_asset_specs = DagsterSlingFactory . _get_sling_deps ( replication_config , kind ) asset_freshness_checks = DagsterSlingFactory . _get_freshness_checks ( replication_config ) if asset_freshness_checks : freshness_checks . extend ( asset_freshness_checks ) if assets_definition : assets . append ( assets_definition ) if dep_asset_specs : assets . extend ( dep_asset_specs ) return assets , freshness_checks @staticmethod def _get_replication ( config : dict ) -> dg . AssetsDefinition : \"\"\"Returns a AssetsDefinition for replication in a Sling yaml file \"\"\" @sling_assets ( name = config [ \"source\" ] + \"_assets\" , replication_config = config , backfill_policy = dg . BackfillPolicy . single_run (), dagster_sling_translator = CustomDagsterSlingTranslator (), pool = \"sling\" , ) def assets ( context : dg . AssetExecutionContext , sling : SlingResource ) -> Generator [ SlingEventType , Any , None ]: if \"defaults\" not in config : config [ \"defaults\" ] = {} try : # to inject start and end dates for partitioned runs time_window = context . partition_time_window if time_window : if \"source_options\" not in config [ \"defaults\" ]: config [ \"defaults\" ][ \"source_options\" ] = {} format = \"%Y-%m- %d %H:%M:%S\" start = time_window . start . strftime ( format ) end = time_window . end . strftime ( format ) config [ \"defaults\" ][ \"source_options\" ][ \"range\" ] = f \" { start } , { end } \" except Exception : # run normal run if time window not provided pass yield from sling . replicate ( context = context , replication_config = config , dagster_sling_translator = CustomDagsterSlingTranslator (), ) for row in sling . stream_raw_logs (): context . log . info ( row ) return assets @staticmethod def _set_dev_schema ( replication_config : dict ) -> dict : \"\"\"Override the desination schema set in the yaml file when the environment is set to dev to point to a unique schema based on the developer. \"\"\" user = os . environ [ \"DESTINATION__SNOWFLAKE__CREDENTIALS__USERNAME\" ] . upper () if default_object := replication_config [ \"defaults\" ][ \"object\" ]: schema , table = default_object . split ( \".\" ) replication_config [ \"defaults\" ][ \"object\" ] = f \" { schema } __ { user } . { table } \" for stream , stream_config in list ( replication_config . get ( \"streams\" , {}) . items () ): stream_config = stream_config or {} if stream_object := stream_config . get ( \"object\" ): schema , table = stream_object . split ( \".\" ) replication_config [ \"streams\" ][ stream ][ \"object\" ] = ( f \" { schema } __ { user } . { table } \" ) return replication_config @staticmethod def _get_sling_deps ( replication_config : dict , kind : str | None ) -> list [ dg . AssetSpec ] | None : \"\"\"Create an external asset that is placed in the same prefix as the asset, and assigned the correct resource kind. \"\"\" kinds = { kind } if kind else None deps = [] for k in replication_config [ \"streams\" ]: schema , table = k . split ( \".\" ) dep = dg . AssetSpec ( key = [ schema , \"src\" , table ], group_name = schema , kinds = kinds ) deps . append ( dep ) return deps @staticmethod def _get_freshness_checks ( replication_config : dict , ) -> list [ dg . AssetChecksDefinition ]: \"\"\"Returns a list of AssetChecksDefinition for replication configs. Configs supplied on the stream will take priority, otherwise the default will be used. \"\"\" freshness_checks = [] default_freshness_check_config = ( get_nested ( replication_config , [ \"defaults\" , \"meta\" , \"dagster\" , \"freshness_check\" ] ) or {} ) default_partition = get_nested ( replication_config , [ \"defaults\" , \"meta\" , \"dagster\" , \"partition\" ] ) streams = replication_config . get ( \"streams\" , {}) for stream_name , steam_config in streams . items (): freshness_check_config = ( get_nested ( steam_config , [ \"meta\" , \"dagster\" , \"freshness_check\" ]) or {} ) partition = get_nested ( steam_config , [ \"meta\" , \"dagster\" , \"partition\" ]) freshness_check_config = ( freshness_check_config | default_freshness_check_config ) partition = partition or default_partition if freshness_check_config : if lower_bound_delta_seconds := freshness_check_config . pop ( \"lower_bound_delta_seconds\" , None ): lower_bound_delta = timedelta ( seconds = float ( lower_bound_delta_seconds ) ) freshness_check_config [ \"lower_bound_delta\" ] = lower_bound_delta schema , table_name = stream_name . split ( \".\" ) asset_key = [ schema , \"raw\" , table_name ] freshness_check_config [ \"assets\" ] = [ asset_key ] try : if partition in [ \"hourly\" , \"daily\" , \"weekly\" , \"monthly\" ]: freshness_check_config = sanitize_input_signature ( dg . build_time_partition_freshness_checks , freshness_check_config , ) time_partition_update_freshness_checks = ( dg . build_time_partition_freshness_checks ( ** freshness_check_config ) ) freshness_checks . extend ( time_partition_update_freshness_checks ) else : freshness_check_config = sanitize_input_signature ( dg . build_last_update_freshness_checks , freshness_check_config , ) last_update_freshness_checks = ( dg . build_last_update_freshness_checks ( ** freshness_check_config ) ) freshness_checks . extend ( last_update_freshness_checks ) except TypeError as e : raise TypeError ( \"Error creating freshness check, check your configuration for \" f \"' { asset_key } '. Supplied arguments: { freshness_check_config } \" ) from e return freshness_checks build_definitions ( config_dir ) cached staticmethod Returns a Definitions object for a path that contains Sling yaml configs. Source code in data_platform\\lib\\sling\\__init__.py 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 @cache @staticmethod def build_definitions ( config_dir : Path ) -> dg . Definitions : \"\"\"Returns a Definitions object for a path that contains Sling yaml configs.\"\"\" connections = [] assets = [] freshness_checks = [] kind_map = {} for config_path in os . listdir ( config_dir ): if config_path . endswith ( \".yaml\" ) or config_path . endswith ( \".yml\" ): config_path = config_dir . joinpath ( config_path ) . resolve () with open ( config_path ) as file : config = yaml . load ( file , Loader = yaml . FullLoader ) if not config : continue if connection_configs := config . get ( \"connections\" ): connections , kind_map = DagsterSlingFactory . _get_connections ( connection_configs , connections , kind_map ) if replication_configs := config . get ( \"replications\" ): assets , freshness_checks = DagsterSlingFactory . _get_replications ( replication_configs , freshness_checks , kind_map , assets ) return dg . Definitions ( resources = { \"sling\" : SlingResource ( connections = connections )}, assets = assets , asset_checks = freshness_checks , sensors = [ dg . build_sensor_for_freshness_checks ( freshness_checks = freshness_checks , name = \"sling_freshness_checks_sensor\" , ) ], )","title":"  init  "},{"location":"reference/lib/sling/__init__/#data_platform.lib.sling.DagsterSlingFactory","text":"Factory to generate dagster definitions from Sling yaml config files. Source code in data_platform\\lib\\sling\\__init__.py 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 class DagsterSlingFactory : \"\"\"Factory to generate dagster definitions from Sling yaml config files.\"\"\" @cache @staticmethod def build_definitions ( config_dir : Path ) -> dg . Definitions : \"\"\"Returns a Definitions object for a path that contains Sling yaml configs.\"\"\" connections = [] assets = [] freshness_checks = [] kind_map = {} for config_path in os . listdir ( config_dir ): if config_path . endswith ( \".yaml\" ) or config_path . endswith ( \".yml\" ): config_path = config_dir . joinpath ( config_path ) . resolve () with open ( config_path ) as file : config = yaml . load ( file , Loader = yaml . FullLoader ) if not config : continue if connection_configs := config . get ( \"connections\" ): connections , kind_map = DagsterSlingFactory . _get_connections ( connection_configs , connections , kind_map ) if replication_configs := config . get ( \"replications\" ): assets , freshness_checks = DagsterSlingFactory . _get_replications ( replication_configs , freshness_checks , kind_map , assets ) return dg . Definitions ( resources = { \"sling\" : SlingResource ( connections = connections )}, assets = assets , asset_checks = freshness_checks , sensors = [ dg . build_sensor_for_freshness_checks ( freshness_checks = freshness_checks , name = \"sling_freshness_checks_sensor\" , ) ], ) @staticmethod def _get_connections ( connection_configs , connections , kind_map ) -> tuple [ list [ SlingConnectionResource ], dict [ str , str ]]: \"\"\"Returns a list of SlingConnectionResource for connections in the Sling yaml file. \"\"\" for connection_config in connection_configs : if connection := DagsterSlingFactory . _get_connection ( connection_config ): source = connection_config . get ( \"name\" ) kind = connection_config . get ( \"type\" ) kind_map [ source ] = kind connections . append ( connection ) return connections , kind_map @staticmethod def _get_connection ( connection_config : dict ) -> SlingConnectionResource | None : \"\"\"Returns a SlingConnectionResource for a connection in the Sling yaml file.\"\"\" for k , v in connection_config . items (): if isinstance ( v , dict ): secret_name = list ( v . keys ())[ 0 ] display_type = list ( v . values ())[ 0 ] if display_type == \"show\" : connection_config [ k ] = get_secret ( secret_name ) . get_value () else : connection_config [ k ] = get_secret ( secret_name ) connection = SlingConnectionResource ( ** connection_config ) return connection @staticmethod def _get_replications ( replication_configs , freshness_checks , kind_map , assets ) -> tuple [ list [ dg . AssetsDefinition ], list [ dg . AssetChecksDefinition ]]: \"\"\"Returns a list of AssetsDefinitions for replications in a Sling yaml file \"\"\" for replication_config in replication_configs : if bool ( os . getenv ( \"DAGSTER_IS_DEV_CLI\" )): replication_config = DagsterSlingFactory . _set_dev_schema ( replication_config ) assets_definition = DagsterSlingFactory . _get_replication ( replication_config ) kind = kind_map . get ( replication_config . get ( \"source\" , None ), None ) dep_asset_specs = DagsterSlingFactory . _get_sling_deps ( replication_config , kind ) asset_freshness_checks = DagsterSlingFactory . _get_freshness_checks ( replication_config ) if asset_freshness_checks : freshness_checks . extend ( asset_freshness_checks ) if assets_definition : assets . append ( assets_definition ) if dep_asset_specs : assets . extend ( dep_asset_specs ) return assets , freshness_checks @staticmethod def _get_replication ( config : dict ) -> dg . AssetsDefinition : \"\"\"Returns a AssetsDefinition for replication in a Sling yaml file \"\"\" @sling_assets ( name = config [ \"source\" ] + \"_assets\" , replication_config = config , backfill_policy = dg . BackfillPolicy . single_run (), dagster_sling_translator = CustomDagsterSlingTranslator (), pool = \"sling\" , ) def assets ( context : dg . AssetExecutionContext , sling : SlingResource ) -> Generator [ SlingEventType , Any , None ]: if \"defaults\" not in config : config [ \"defaults\" ] = {} try : # to inject start and end dates for partitioned runs time_window = context . partition_time_window if time_window : if \"source_options\" not in config [ \"defaults\" ]: config [ \"defaults\" ][ \"source_options\" ] = {} format = \"%Y-%m- %d %H:%M:%S\" start = time_window . start . strftime ( format ) end = time_window . end . strftime ( format ) config [ \"defaults\" ][ \"source_options\" ][ \"range\" ] = f \" { start } , { end } \" except Exception : # run normal run if time window not provided pass yield from sling . replicate ( context = context , replication_config = config , dagster_sling_translator = CustomDagsterSlingTranslator (), ) for row in sling . stream_raw_logs (): context . log . info ( row ) return assets @staticmethod def _set_dev_schema ( replication_config : dict ) -> dict : \"\"\"Override the desination schema set in the yaml file when the environment is set to dev to point to a unique schema based on the developer. \"\"\" user = os . environ [ \"DESTINATION__SNOWFLAKE__CREDENTIALS__USERNAME\" ] . upper () if default_object := replication_config [ \"defaults\" ][ \"object\" ]: schema , table = default_object . split ( \".\" ) replication_config [ \"defaults\" ][ \"object\" ] = f \" { schema } __ { user } . { table } \" for stream , stream_config in list ( replication_config . get ( \"streams\" , {}) . items () ): stream_config = stream_config or {} if stream_object := stream_config . get ( \"object\" ): schema , table = stream_object . split ( \".\" ) replication_config [ \"streams\" ][ stream ][ \"object\" ] = ( f \" { schema } __ { user } . { table } \" ) return replication_config @staticmethod def _get_sling_deps ( replication_config : dict , kind : str | None ) -> list [ dg . AssetSpec ] | None : \"\"\"Create an external asset that is placed in the same prefix as the asset, and assigned the correct resource kind. \"\"\" kinds = { kind } if kind else None deps = [] for k in replication_config [ \"streams\" ]: schema , table = k . split ( \".\" ) dep = dg . AssetSpec ( key = [ schema , \"src\" , table ], group_name = schema , kinds = kinds ) deps . append ( dep ) return deps @staticmethod def _get_freshness_checks ( replication_config : dict , ) -> list [ dg . AssetChecksDefinition ]: \"\"\"Returns a list of AssetChecksDefinition for replication configs. Configs supplied on the stream will take priority, otherwise the default will be used. \"\"\" freshness_checks = [] default_freshness_check_config = ( get_nested ( replication_config , [ \"defaults\" , \"meta\" , \"dagster\" , \"freshness_check\" ] ) or {} ) default_partition = get_nested ( replication_config , [ \"defaults\" , \"meta\" , \"dagster\" , \"partition\" ] ) streams = replication_config . get ( \"streams\" , {}) for stream_name , steam_config in streams . items (): freshness_check_config = ( get_nested ( steam_config , [ \"meta\" , \"dagster\" , \"freshness_check\" ]) or {} ) partition = get_nested ( steam_config , [ \"meta\" , \"dagster\" , \"partition\" ]) freshness_check_config = ( freshness_check_config | default_freshness_check_config ) partition = partition or default_partition if freshness_check_config : if lower_bound_delta_seconds := freshness_check_config . pop ( \"lower_bound_delta_seconds\" , None ): lower_bound_delta = timedelta ( seconds = float ( lower_bound_delta_seconds ) ) freshness_check_config [ \"lower_bound_delta\" ] = lower_bound_delta schema , table_name = stream_name . split ( \".\" ) asset_key = [ schema , \"raw\" , table_name ] freshness_check_config [ \"assets\" ] = [ asset_key ] try : if partition in [ \"hourly\" , \"daily\" , \"weekly\" , \"monthly\" ]: freshness_check_config = sanitize_input_signature ( dg . build_time_partition_freshness_checks , freshness_check_config , ) time_partition_update_freshness_checks = ( dg . build_time_partition_freshness_checks ( ** freshness_check_config ) ) freshness_checks . extend ( time_partition_update_freshness_checks ) else : freshness_check_config = sanitize_input_signature ( dg . build_last_update_freshness_checks , freshness_check_config , ) last_update_freshness_checks = ( dg . build_last_update_freshness_checks ( ** freshness_check_config ) ) freshness_checks . extend ( last_update_freshness_checks ) except TypeError as e : raise TypeError ( \"Error creating freshness check, check your configuration for \" f \"' { asset_key } '. Supplied arguments: { freshness_check_config } \" ) from e return freshness_checks","title":"DagsterSlingFactory"},{"location":"reference/lib/sling/__init__/#data_platform.lib.sling.DagsterSlingFactory.build_definitions","text":"Returns a Definitions object for a path that contains Sling yaml configs. Source code in data_platform\\lib\\sling\\__init__.py 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 @cache @staticmethod def build_definitions ( config_dir : Path ) -> dg . Definitions : \"\"\"Returns a Definitions object for a path that contains Sling yaml configs.\"\"\" connections = [] assets = [] freshness_checks = [] kind_map = {} for config_path in os . listdir ( config_dir ): if config_path . endswith ( \".yaml\" ) or config_path . endswith ( \".yml\" ): config_path = config_dir . joinpath ( config_path ) . resolve () with open ( config_path ) as file : config = yaml . load ( file , Loader = yaml . FullLoader ) if not config : continue if connection_configs := config . get ( \"connections\" ): connections , kind_map = DagsterSlingFactory . _get_connections ( connection_configs , connections , kind_map ) if replication_configs := config . get ( \"replications\" ): assets , freshness_checks = DagsterSlingFactory . _get_replications ( replication_configs , freshness_checks , kind_map , assets ) return dg . Definitions ( resources = { \"sling\" : SlingResource ( connections = connections )}, assets = assets , asset_checks = freshness_checks , sensors = [ dg . build_sensor_for_freshness_checks ( freshness_checks = freshness_checks , name = \"sling_freshness_checks_sensor\" , ) ], )","title":"build_definitions"},{"location":"reference/lib/sling/translator/","text":"CustomDagsterSlingTranslator Bases: DagsterSlingTranslator Overrides methods of the standard translator. Holds a set of methods that derive Dagster asset definition metadata given a representation of Sling resource (connections, replications). Methods are overriden to customize the implementation. See parent class for details on the purpose of each override Source code in data_platform\\lib\\sling\\translator.py 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 class CustomDagsterSlingTranslator ( dg_sling . DagsterSlingTranslator ): \"\"\"Overrides methods of the standard translator. Holds a set of methods that derive Dagster asset definition metadata given a representation of Sling resource (connections, replications). Methods are overriden to customize the implementation. See parent class for details on the purpose of each override\"\"\" @override def get_asset_spec ( self , stream_definition : Mapping [ str , Any ]) -> dg . AssetSpec : return dg . AssetSpec ( automation_condition = self . get_automation_condition ( stream_definition ), partitions_def = self . get_partitions_def ( stream_definition ), key = self . _resolve_back_compat_method ( \"get_asset_key\" , self . _default_asset_key_fn , stream_definition ), deps = self . _resolve_back_compat_method ( \"get_deps_asset_key\" , self . _default_deps_fn , stream_definition ), description = self . _resolve_back_compat_method ( \"get_description\" , self . _default_description_fn , stream_definition ), metadata = self . _resolve_back_compat_method ( \"get_metadata\" , self . _default_metadata_fn , stream_definition ), tags = self . _resolve_back_compat_method ( \"get_tags\" , self . _default_tags_fn , stream_definition ), kinds = self . _resolve_back_compat_method ( \"get_kinds\" , self . _default_kinds_fn , stream_definition ), group_name = self . _resolve_back_compat_method ( \"get_group_name\" , self . _default_group_name_fn , stream_definition ), legacy_freshness_policy = self . _resolve_back_compat_method ( \"get_freshness_policy\" , self . _default_freshness_policy_fn , stream_definition , ), auto_materialize_policy = self . _resolve_back_compat_method ( \"get_auto_materialize_policy\" , self . _default_auto_materialize_policy_fn , stream_definition , ), ) @override def get_asset_key ( self , stream_definition : Mapping [ str , Any ]) -> dg . AssetKey : config = stream_definition . get ( \"config\" ) or {} meta = config . get ( \"meta\" ) or {} dagster = meta . get ( \"dagster\" ) or {} asset_key = dagster . get ( \"asset_key\" , None ) if asset_key : if self . sanitize_stream_name ( asset_key ) != asset_key : raise ValueError ( f \"Asset key { asset_key } for stream { stream_definition [ 'name' ] } \" \"is not sanitized. Please use only alphanumeric characters \" \"and underscores.\" ) return dg . AssetKey ( asset_key . split ( \".\" )) # You can override the Sling Replication default object with an object key stream_name = stream_definition [ \"name\" ] schema , table = self . sanitize_stream_name ( stream_name ) . split ( \".\" ) return dg . AssetKey ([ schema , \"raw\" , table ]) @override def get_deps_asset_key ( self , stream_definition : Mapping [ str , Any ] ) -> Iterable [ dg . AssetKey ]: config = stream_definition . get ( \"config\" , {}) or {} meta = config . get ( \"meta\" , {}) or {} deps = meta . get ( \"dagster\" , {}) . get ( \"deps\" ) deps_out = [] if deps and isinstance ( deps , str ): deps = [ deps ] if deps : assert isinstance ( deps , list ) for asset_key in deps : if self . sanitize_stream_name ( asset_key ) != asset_key : raise ValueError ( f \"Deps Asset key { asset_key } for stream \" f \" { stream_definition [ 'name' ] } is not sanitized. \" \"Please use only alphanumeric characters and underscores.\" ) deps_out . append ( dg . AssetKey ( asset_key . split ( \".\" ))) return deps_out stream_name = stream_definition [ \"name\" ] schema , table = self . sanitize_stream_name ( stream_name ) . split ( \".\" ) return [ dg . AssetKey ([ schema , \"src\" , table ])] @override def get_group_name ( self , stream_definition : Mapping [ str , Any ]) -> str : try : group = stream_definition [ \"config\" ][ \"meta\" ][ \"dagster\" ][ \"group\" ] if group : return group except Exception : ... stream_name = stream_definition [ \"name\" ] schema , _ = self . sanitize_stream_name ( stream_name ) . split ( \".\" ) return schema @override def get_tags ( self , stream_definition : Mapping [ str , Any ]) -> Mapping [ str , Any ]: try : tags = stream_definition [ \"config\" ][ \"meta\" ][ \"dagster\" ][ \"tags\" ] return { tag : \"\" for tag in tags if is_valid_tag_key ( tag )} except Exception : ... return {} def get_automation_condition ( self , stream_definition : Mapping [ str , Any ] ) -> None | dg . AutomationCondition : try : meta = stream_definition [ \"config\" ][ \"meta\" ][ \"dagster\" ] automation_condition = get_automation_condition_from_meta ( meta ) return automation_condition except Exception : ... return None def get_partitions_def ( self , stream_definition : Mapping [ str , Any ] ) -> None | dg . PartitionsDefinition : try : meta = stream_definition [ \"config\" ][ \"meta\" ][ \"dagster\" ] automation_condition = get_partitions_def_from_meta ( meta ) return automation_condition except Exception : ... return None","title":"Translator"},{"location":"reference/lib/sling/translator/#data_platform.lib.sling.translator.CustomDagsterSlingTranslator","text":"Bases: DagsterSlingTranslator Overrides methods of the standard translator. Holds a set of methods that derive Dagster asset definition metadata given a representation of Sling resource (connections, replications). Methods are overriden to customize the implementation. See parent class for details on the purpose of each override Source code in data_platform\\lib\\sling\\translator.py 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 class CustomDagsterSlingTranslator ( dg_sling . DagsterSlingTranslator ): \"\"\"Overrides methods of the standard translator. Holds a set of methods that derive Dagster asset definition metadata given a representation of Sling resource (connections, replications). Methods are overriden to customize the implementation. See parent class for details on the purpose of each override\"\"\" @override def get_asset_spec ( self , stream_definition : Mapping [ str , Any ]) -> dg . AssetSpec : return dg . AssetSpec ( automation_condition = self . get_automation_condition ( stream_definition ), partitions_def = self . get_partitions_def ( stream_definition ), key = self . _resolve_back_compat_method ( \"get_asset_key\" , self . _default_asset_key_fn , stream_definition ), deps = self . _resolve_back_compat_method ( \"get_deps_asset_key\" , self . _default_deps_fn , stream_definition ), description = self . _resolve_back_compat_method ( \"get_description\" , self . _default_description_fn , stream_definition ), metadata = self . _resolve_back_compat_method ( \"get_metadata\" , self . _default_metadata_fn , stream_definition ), tags = self . _resolve_back_compat_method ( \"get_tags\" , self . _default_tags_fn , stream_definition ), kinds = self . _resolve_back_compat_method ( \"get_kinds\" , self . _default_kinds_fn , stream_definition ), group_name = self . _resolve_back_compat_method ( \"get_group_name\" , self . _default_group_name_fn , stream_definition ), legacy_freshness_policy = self . _resolve_back_compat_method ( \"get_freshness_policy\" , self . _default_freshness_policy_fn , stream_definition , ), auto_materialize_policy = self . _resolve_back_compat_method ( \"get_auto_materialize_policy\" , self . _default_auto_materialize_policy_fn , stream_definition , ), ) @override def get_asset_key ( self , stream_definition : Mapping [ str , Any ]) -> dg . AssetKey : config = stream_definition . get ( \"config\" ) or {} meta = config . get ( \"meta\" ) or {} dagster = meta . get ( \"dagster\" ) or {} asset_key = dagster . get ( \"asset_key\" , None ) if asset_key : if self . sanitize_stream_name ( asset_key ) != asset_key : raise ValueError ( f \"Asset key { asset_key } for stream { stream_definition [ 'name' ] } \" \"is not sanitized. Please use only alphanumeric characters \" \"and underscores.\" ) return dg . AssetKey ( asset_key . split ( \".\" )) # You can override the Sling Replication default object with an object key stream_name = stream_definition [ \"name\" ] schema , table = self . sanitize_stream_name ( stream_name ) . split ( \".\" ) return dg . AssetKey ([ schema , \"raw\" , table ]) @override def get_deps_asset_key ( self , stream_definition : Mapping [ str , Any ] ) -> Iterable [ dg . AssetKey ]: config = stream_definition . get ( \"config\" , {}) or {} meta = config . get ( \"meta\" , {}) or {} deps = meta . get ( \"dagster\" , {}) . get ( \"deps\" ) deps_out = [] if deps and isinstance ( deps , str ): deps = [ deps ] if deps : assert isinstance ( deps , list ) for asset_key in deps : if self . sanitize_stream_name ( asset_key ) != asset_key : raise ValueError ( f \"Deps Asset key { asset_key } for stream \" f \" { stream_definition [ 'name' ] } is not sanitized. \" \"Please use only alphanumeric characters and underscores.\" ) deps_out . append ( dg . AssetKey ( asset_key . split ( \".\" ))) return deps_out stream_name = stream_definition [ \"name\" ] schema , table = self . sanitize_stream_name ( stream_name ) . split ( \".\" ) return [ dg . AssetKey ([ schema , \"src\" , table ])] @override def get_group_name ( self , stream_definition : Mapping [ str , Any ]) -> str : try : group = stream_definition [ \"config\" ][ \"meta\" ][ \"dagster\" ][ \"group\" ] if group : return group except Exception : ... stream_name = stream_definition [ \"name\" ] schema , _ = self . sanitize_stream_name ( stream_name ) . split ( \".\" ) return schema @override def get_tags ( self , stream_definition : Mapping [ str , Any ]) -> Mapping [ str , Any ]: try : tags = stream_definition [ \"config\" ][ \"meta\" ][ \"dagster\" ][ \"tags\" ] return { tag : \"\" for tag in tags if is_valid_tag_key ( tag )} except Exception : ... return {} def get_automation_condition ( self , stream_definition : Mapping [ str , Any ] ) -> None | dg . AutomationCondition : try : meta = stream_definition [ \"config\" ][ \"meta\" ][ \"dagster\" ] automation_condition = get_automation_condition_from_meta ( meta ) return automation_condition except Exception : ... return None def get_partitions_def ( self , stream_definition : Mapping [ str , Any ] ) -> None | dg . PartitionsDefinition : try : meta = stream_definition [ \"config\" ][ \"meta\" ][ \"dagster\" ] automation_condition = get_partitions_def_from_meta ( meta ) return automation_condition except Exception : ... return None","title":"CustomDagsterSlingTranslator"},{"location":"reference/utils/automation_conditions/","text":"CustomAutomationCondition Bases: AutomationCondition Source code in data_platform\\utils\\automation_conditions.py 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 class CustomAutomationCondition ( AutomationCondition ): @classmethod def get_automation_condition ( cls , automation_condition_name : str ) -> AutomationCondition | None : methods = AutomationCondition . __dict__ | cls . __dict__ return methods . get ( automation_condition_name , None ) @staticmethod def manual () -> None : \"\"\"Returns no AutomationCondition that will require a user to manually trigger. Used for overriding default automations for static assets. \"\"\" return None @staticmethod def missing_or_changed () -> AutomationCondition : \"\"\"Returns no AutomationCondition that will trigger only if the asset has never been materialized, or if its definition has changed. Common use for dbt seeds that only need to be reloaded when the underlying csv file changes. \"\"\" return ( AutomationCondition . in_latest_time_window () & ( AutomationCondition . code_version_changed () | AutomationCondition . newly_missing () ) . since_last_handled () & ~ AutomationCondition . in_progress () ) . with_label ( \"missing_or_changed\" ) @override @staticmethod def eager () -> AndAutomationCondition : \"\"\"Returns an AutomationCondition which will cause a target to be executed if any of its dependencies update, and will execute missing partitions if they become missing after this condition is applied to the target. This will not execute targets that have any missing or in progress dependencies, or are currently in progress. For time partitioned assets, only the latest time partition will be considered. Commonly used for assets that are far downstream and have users that directly interact with them, and do not have sensitivity to late arriving dimensions. \"\"\" return ( AutomationCondition . in_latest_time_window () & ( AutomationCondition . newly_missing () | AutomationCondition . any_deps_updated () ) . since_last_handled () & ~ AutomationCondition . any_deps_missing () & ~ AutomationCondition . any_deps_in_progress () & ~ AutomationCondition . in_progress () ) . with_label ( \"eager\" ) @staticmethod def eager_with_deps_checks () -> AutomationCondition : \"\"\"Returns an AutomationCondition which will cause a target to be executed if any of its dependencies update but only after, the dependencies blocking checks have passed, and will execute missing partitions if they become missing after this condition is applied to the target. This will not execute targets that have any missing or in progress dependencies, or are currently in progress. For time partitioned assets, only the latest time partition will be considered. Commonly used for assets that are far downstream and have users that directly interact with them, and do not have sensitivity to late arriving dimensions. \"\"\" return ( AutomationCondition . eager () & AutomationCondition . all_deps_blocking_checks_passed () ) @classmethod def lazy ( cls ) -> AutomationCondition : \"\"\"Returns an AutomationCondition which will cause a target to be executed if any downstream conditions are true or the partition is missing or changed. Commonly used for intermediate assets that are used for downstream materializations. \"\"\" return ( AutomationCondition . any_downstream_conditions () | cls . missing_or_changed () ) . with_label ( \"lazy\" ) @staticmethod def lazy_on_cron ( cron_schedule : str , cron_timezone : str = \"UTC\" , ignore_asset_keys : list [ list [ str ]] | None = None , ) -> AutomationCondition : \"\"\"Returns an AutomationCondition which will cause a target to be executed if any downstream conditions are true or the partition is missing or changed. Will limit to only one execution for the given cron_schedule. Commonly used for intermediate assets that are used for downstream materializations, that have high frequency upstream assets, but themselves do not need to be updated as frequently. \"\"\" ignore_asset_keys = ignore_asset_keys or [] return ( AutomationCondition . in_latest_time_window () & AutomationCondition . cron_tick_passed ( cron_schedule , cron_timezone ) . since_last_handled () & AutomationCondition . all_deps_updated_since_cron ( cron_schedule , cron_timezone ) . ignore ( AssetSelection . assets ( * ignore_asset_keys )) & ~ AutomationCondition . in_progress () ) . with_label ( f \"lazy_on_cron( { cron_schedule } , { cron_timezone } )\" ) @staticmethod @override def on_cron ( cron_schedule : str , cron_timezone : str = \"UTC\" , ignore_asset_keys : list [ list [ str ]] | None = None , ) -> AndAutomationCondition : \"\"\"Returns an AutomationCondition which will cause a target to be executed on a given cron schedule, after all of its dependencies have been updated since the latest tick of that cron schedule. For time partitioned assets, only the latest time partition will be considered. Commonly used for assets that are far downstream and have users that directly interact with them, and have sensitivity to late arriving dimensions. \"\"\" ignore_asset_keys = ignore_asset_keys or [] return AutomationCondition . on_cron ( cron_schedule , cron_timezone ) . ignore ( AssetSelection . assets ( * ignore_asset_keys ) ) @staticmethod def on_schedule ( cron_schedule : str , cron_timezone : str = \"utc\" ) -> AutomationCondition : \"\"\"Returns an AutomationCondition which will cause a target to be executed on a given cron schedule, regardless of the state of its dependencies For time partitioned assets, only the latest time partition will be considered. Commonly used for assets in the ingestion layer that should always run on a scheduled basis, and have no way of knowing when the source system has updates. \"\"\" return ( AutomationCondition . in_latest_time_window () & AutomationCondition . cron_tick_passed ( cron_schedule , cron_timezone ) . since_last_handled () ) . with_label ( f \"on_schedule( { cron_schedule } , { cron_timezone } )\" ) eager () staticmethod Returns an AutomationCondition which will cause a target to be executed if any of its dependencies update, and will execute missing partitions if they become missing after this condition is applied to the target. This will not execute targets that have any missing or in progress dependencies, or are currently in progress. For time partitioned assets, only the latest time partition will be considered. Commonly used for assets that are far downstream and have users that directly interact with them, and do not have sensitivity to late arriving dimensions. Source code in data_platform\\utils\\automation_conditions.py 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 @override @staticmethod def eager () -> AndAutomationCondition : \"\"\"Returns an AutomationCondition which will cause a target to be executed if any of its dependencies update, and will execute missing partitions if they become missing after this condition is applied to the target. This will not execute targets that have any missing or in progress dependencies, or are currently in progress. For time partitioned assets, only the latest time partition will be considered. Commonly used for assets that are far downstream and have users that directly interact with them, and do not have sensitivity to late arriving dimensions. \"\"\" return ( AutomationCondition . in_latest_time_window () & ( AutomationCondition . newly_missing () | AutomationCondition . any_deps_updated () ) . since_last_handled () & ~ AutomationCondition . any_deps_missing () & ~ AutomationCondition . any_deps_in_progress () & ~ AutomationCondition . in_progress () ) . with_label ( \"eager\" ) eager_with_deps_checks () staticmethod Returns an AutomationCondition which will cause a target to be executed if any of its dependencies update but only after, the dependencies blocking checks have passed, and will execute missing partitions if they become missing after this condition is applied to the target. This will not execute targets that have any missing or in progress dependencies, or are currently in progress. For time partitioned assets, only the latest time partition will be considered. Commonly used for assets that are far downstream and have users that directly interact with them, and do not have sensitivity to late arriving dimensions. Source code in data_platform\\utils\\automation_conditions.py 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 @staticmethod def eager_with_deps_checks () -> AutomationCondition : \"\"\"Returns an AutomationCondition which will cause a target to be executed if any of its dependencies update but only after, the dependencies blocking checks have passed, and will execute missing partitions if they become missing after this condition is applied to the target. This will not execute targets that have any missing or in progress dependencies, or are currently in progress. For time partitioned assets, only the latest time partition will be considered. Commonly used for assets that are far downstream and have users that directly interact with them, and do not have sensitivity to late arriving dimensions. \"\"\" return ( AutomationCondition . eager () & AutomationCondition . all_deps_blocking_checks_passed () ) lazy () classmethod Returns an AutomationCondition which will cause a target to be executed if any downstream conditions are true or the partition is missing or changed. Commonly used for intermediate assets that are used for downstream materializations. Source code in data_platform\\utils\\automation_conditions.py 84 85 86 87 88 89 90 91 92 93 94 @classmethod def lazy ( cls ) -> AutomationCondition : \"\"\"Returns an AutomationCondition which will cause a target to be executed if any downstream conditions are true or the partition is missing or changed. Commonly used for intermediate assets that are used for downstream materializations. \"\"\" return ( AutomationCondition . any_downstream_conditions () | cls . missing_or_changed () ) . with_label ( \"lazy\" ) lazy_on_cron ( cron_schedule , cron_timezone = 'UTC' , ignore_asset_keys = None ) staticmethod Returns an AutomationCondition which will cause a target to be executed if any downstream conditions are true or the partition is missing or changed. Will limit to only one execution for the given cron_schedule. Commonly used for intermediate assets that are used for downstream materializations, that have high frequency upstream assets, but themselves do not need to be updated as frequently. Source code in data_platform\\utils\\automation_conditions.py 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 @staticmethod def lazy_on_cron ( cron_schedule : str , cron_timezone : str = \"UTC\" , ignore_asset_keys : list [ list [ str ]] | None = None , ) -> AutomationCondition : \"\"\"Returns an AutomationCondition which will cause a target to be executed if any downstream conditions are true or the partition is missing or changed. Will limit to only one execution for the given cron_schedule. Commonly used for intermediate assets that are used for downstream materializations, that have high frequency upstream assets, but themselves do not need to be updated as frequently. \"\"\" ignore_asset_keys = ignore_asset_keys or [] return ( AutomationCondition . in_latest_time_window () & AutomationCondition . cron_tick_passed ( cron_schedule , cron_timezone ) . since_last_handled () & AutomationCondition . all_deps_updated_since_cron ( cron_schedule , cron_timezone ) . ignore ( AssetSelection . assets ( * ignore_asset_keys )) & ~ AutomationCondition . in_progress () ) . with_label ( f \"lazy_on_cron( { cron_schedule } , { cron_timezone } )\" ) manual () staticmethod Returns no AutomationCondition that will require a user to manually trigger. Used for overriding default automations for static assets. Source code in data_platform\\utils\\automation_conditions.py 17 18 19 20 21 22 @staticmethod def manual () -> None : \"\"\"Returns no AutomationCondition that will require a user to manually trigger. Used for overriding default automations for static assets. \"\"\" return None missing_or_changed () staticmethod Returns no AutomationCondition that will trigger only if the asset has never been materialized, or if its definition has changed. Common use for dbt seeds that only need to be reloaded when the underlying csv file changes. Source code in data_platform\\utils\\automation_conditions.py 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 @staticmethod def missing_or_changed () -> AutomationCondition : \"\"\"Returns no AutomationCondition that will trigger only if the asset has never been materialized, or if its definition has changed. Common use for dbt seeds that only need to be reloaded when the underlying csv file changes. \"\"\" return ( AutomationCondition . in_latest_time_window () & ( AutomationCondition . code_version_changed () | AutomationCondition . newly_missing () ) . since_last_handled () & ~ AutomationCondition . in_progress () ) . with_label ( \"missing_or_changed\" ) on_cron ( cron_schedule , cron_timezone = 'UTC' , ignore_asset_keys = None ) staticmethod Returns an AutomationCondition which will cause a target to be executed on a given cron schedule, after all of its dependencies have been updated since the latest tick of that cron schedule. For time partitioned assets, only the latest time partition will be considered. Commonly used for assets that are far downstream and have users that directly interact with them, and have sensitivity to late arriving dimensions. Source code in data_platform\\utils\\automation_conditions.py 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 @staticmethod @override def on_cron ( cron_schedule : str , cron_timezone : str = \"UTC\" , ignore_asset_keys : list [ list [ str ]] | None = None , ) -> AndAutomationCondition : \"\"\"Returns an AutomationCondition which will cause a target to be executed on a given cron schedule, after all of its dependencies have been updated since the latest tick of that cron schedule. For time partitioned assets, only the latest time partition will be considered. Commonly used for assets that are far downstream and have users that directly interact with them, and have sensitivity to late arriving dimensions. \"\"\" ignore_asset_keys = ignore_asset_keys or [] return AutomationCondition . on_cron ( cron_schedule , cron_timezone ) . ignore ( AssetSelection . assets ( * ignore_asset_keys ) ) on_schedule ( cron_schedule , cron_timezone = 'utc' ) staticmethod Returns an AutomationCondition which will cause a target to be executed on a given cron schedule, regardless of the state of its dependencies For time partitioned assets, only the latest time partition will be considered. Commonly used for assets in the ingestion layer that should always run on a scheduled basis, and have no way of knowing when the source system has updates. Source code in data_platform\\utils\\automation_conditions.py 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 @staticmethod def on_schedule ( cron_schedule : str , cron_timezone : str = \"utc\" ) -> AutomationCondition : \"\"\"Returns an AutomationCondition which will cause a target to be executed on a given cron schedule, regardless of the state of its dependencies For time partitioned assets, only the latest time partition will be considered. Commonly used for assets in the ingestion layer that should always run on a scheduled basis, and have no way of knowing when the source system has updates. \"\"\" return ( AutomationCondition . in_latest_time_window () & AutomationCondition . cron_tick_passed ( cron_schedule , cron_timezone ) . since_last_handled () ) . with_label ( f \"on_schedule( { cron_schedule } , { cron_timezone } )\" )","title":"Automation conditions"},{"location":"reference/utils/automation_conditions/#data_platform.utils.automation_conditions.CustomAutomationCondition","text":"Bases: AutomationCondition Source code in data_platform\\utils\\automation_conditions.py 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 class CustomAutomationCondition ( AutomationCondition ): @classmethod def get_automation_condition ( cls , automation_condition_name : str ) -> AutomationCondition | None : methods = AutomationCondition . __dict__ | cls . __dict__ return methods . get ( automation_condition_name , None ) @staticmethod def manual () -> None : \"\"\"Returns no AutomationCondition that will require a user to manually trigger. Used for overriding default automations for static assets. \"\"\" return None @staticmethod def missing_or_changed () -> AutomationCondition : \"\"\"Returns no AutomationCondition that will trigger only if the asset has never been materialized, or if its definition has changed. Common use for dbt seeds that only need to be reloaded when the underlying csv file changes. \"\"\" return ( AutomationCondition . in_latest_time_window () & ( AutomationCondition . code_version_changed () | AutomationCondition . newly_missing () ) . since_last_handled () & ~ AutomationCondition . in_progress () ) . with_label ( \"missing_or_changed\" ) @override @staticmethod def eager () -> AndAutomationCondition : \"\"\"Returns an AutomationCondition which will cause a target to be executed if any of its dependencies update, and will execute missing partitions if they become missing after this condition is applied to the target. This will not execute targets that have any missing or in progress dependencies, or are currently in progress. For time partitioned assets, only the latest time partition will be considered. Commonly used for assets that are far downstream and have users that directly interact with them, and do not have sensitivity to late arriving dimensions. \"\"\" return ( AutomationCondition . in_latest_time_window () & ( AutomationCondition . newly_missing () | AutomationCondition . any_deps_updated () ) . since_last_handled () & ~ AutomationCondition . any_deps_missing () & ~ AutomationCondition . any_deps_in_progress () & ~ AutomationCondition . in_progress () ) . with_label ( \"eager\" ) @staticmethod def eager_with_deps_checks () -> AutomationCondition : \"\"\"Returns an AutomationCondition which will cause a target to be executed if any of its dependencies update but only after, the dependencies blocking checks have passed, and will execute missing partitions if they become missing after this condition is applied to the target. This will not execute targets that have any missing or in progress dependencies, or are currently in progress. For time partitioned assets, only the latest time partition will be considered. Commonly used for assets that are far downstream and have users that directly interact with them, and do not have sensitivity to late arriving dimensions. \"\"\" return ( AutomationCondition . eager () & AutomationCondition . all_deps_blocking_checks_passed () ) @classmethod def lazy ( cls ) -> AutomationCondition : \"\"\"Returns an AutomationCondition which will cause a target to be executed if any downstream conditions are true or the partition is missing or changed. Commonly used for intermediate assets that are used for downstream materializations. \"\"\" return ( AutomationCondition . any_downstream_conditions () | cls . missing_or_changed () ) . with_label ( \"lazy\" ) @staticmethod def lazy_on_cron ( cron_schedule : str , cron_timezone : str = \"UTC\" , ignore_asset_keys : list [ list [ str ]] | None = None , ) -> AutomationCondition : \"\"\"Returns an AutomationCondition which will cause a target to be executed if any downstream conditions are true or the partition is missing or changed. Will limit to only one execution for the given cron_schedule. Commonly used for intermediate assets that are used for downstream materializations, that have high frequency upstream assets, but themselves do not need to be updated as frequently. \"\"\" ignore_asset_keys = ignore_asset_keys or [] return ( AutomationCondition . in_latest_time_window () & AutomationCondition . cron_tick_passed ( cron_schedule , cron_timezone ) . since_last_handled () & AutomationCondition . all_deps_updated_since_cron ( cron_schedule , cron_timezone ) . ignore ( AssetSelection . assets ( * ignore_asset_keys )) & ~ AutomationCondition . in_progress () ) . with_label ( f \"lazy_on_cron( { cron_schedule } , { cron_timezone } )\" ) @staticmethod @override def on_cron ( cron_schedule : str , cron_timezone : str = \"UTC\" , ignore_asset_keys : list [ list [ str ]] | None = None , ) -> AndAutomationCondition : \"\"\"Returns an AutomationCondition which will cause a target to be executed on a given cron schedule, after all of its dependencies have been updated since the latest tick of that cron schedule. For time partitioned assets, only the latest time partition will be considered. Commonly used for assets that are far downstream and have users that directly interact with them, and have sensitivity to late arriving dimensions. \"\"\" ignore_asset_keys = ignore_asset_keys or [] return AutomationCondition . on_cron ( cron_schedule , cron_timezone ) . ignore ( AssetSelection . assets ( * ignore_asset_keys ) ) @staticmethod def on_schedule ( cron_schedule : str , cron_timezone : str = \"utc\" ) -> AutomationCondition : \"\"\"Returns an AutomationCondition which will cause a target to be executed on a given cron schedule, regardless of the state of its dependencies For time partitioned assets, only the latest time partition will be considered. Commonly used for assets in the ingestion layer that should always run on a scheduled basis, and have no way of knowing when the source system has updates. \"\"\" return ( AutomationCondition . in_latest_time_window () & AutomationCondition . cron_tick_passed ( cron_schedule , cron_timezone ) . since_last_handled () ) . with_label ( f \"on_schedule( { cron_schedule } , { cron_timezone } )\" )","title":"CustomAutomationCondition"},{"location":"reference/utils/automation_conditions/#data_platform.utils.automation_conditions.CustomAutomationCondition.eager","text":"Returns an AutomationCondition which will cause a target to be executed if any of its dependencies update, and will execute missing partitions if they become missing after this condition is applied to the target. This will not execute targets that have any missing or in progress dependencies, or are currently in progress. For time partitioned assets, only the latest time partition will be considered. Commonly used for assets that are far downstream and have users that directly interact with them, and do not have sensitivity to late arriving dimensions. Source code in data_platform\\utils\\automation_conditions.py 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 @override @staticmethod def eager () -> AndAutomationCondition : \"\"\"Returns an AutomationCondition which will cause a target to be executed if any of its dependencies update, and will execute missing partitions if they become missing after this condition is applied to the target. This will not execute targets that have any missing or in progress dependencies, or are currently in progress. For time partitioned assets, only the latest time partition will be considered. Commonly used for assets that are far downstream and have users that directly interact with them, and do not have sensitivity to late arriving dimensions. \"\"\" return ( AutomationCondition . in_latest_time_window () & ( AutomationCondition . newly_missing () | AutomationCondition . any_deps_updated () ) . since_last_handled () & ~ AutomationCondition . any_deps_missing () & ~ AutomationCondition . any_deps_in_progress () & ~ AutomationCondition . in_progress () ) . with_label ( \"eager\" )","title":"eager"},{"location":"reference/utils/automation_conditions/#data_platform.utils.automation_conditions.CustomAutomationCondition.eager_with_deps_checks","text":"Returns an AutomationCondition which will cause a target to be executed if any of its dependencies update but only after, the dependencies blocking checks have passed, and will execute missing partitions if they become missing after this condition is applied to the target. This will not execute targets that have any missing or in progress dependencies, or are currently in progress. For time partitioned assets, only the latest time partition will be considered. Commonly used for assets that are far downstream and have users that directly interact with them, and do not have sensitivity to late arriving dimensions. Source code in data_platform\\utils\\automation_conditions.py 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 @staticmethod def eager_with_deps_checks () -> AutomationCondition : \"\"\"Returns an AutomationCondition which will cause a target to be executed if any of its dependencies update but only after, the dependencies blocking checks have passed, and will execute missing partitions if they become missing after this condition is applied to the target. This will not execute targets that have any missing or in progress dependencies, or are currently in progress. For time partitioned assets, only the latest time partition will be considered. Commonly used for assets that are far downstream and have users that directly interact with them, and do not have sensitivity to late arriving dimensions. \"\"\" return ( AutomationCondition . eager () & AutomationCondition . all_deps_blocking_checks_passed () )","title":"eager_with_deps_checks"},{"location":"reference/utils/automation_conditions/#data_platform.utils.automation_conditions.CustomAutomationCondition.lazy","text":"Returns an AutomationCondition which will cause a target to be executed if any downstream conditions are true or the partition is missing or changed. Commonly used for intermediate assets that are used for downstream materializations. Source code in data_platform\\utils\\automation_conditions.py 84 85 86 87 88 89 90 91 92 93 94 @classmethod def lazy ( cls ) -> AutomationCondition : \"\"\"Returns an AutomationCondition which will cause a target to be executed if any downstream conditions are true or the partition is missing or changed. Commonly used for intermediate assets that are used for downstream materializations. \"\"\" return ( AutomationCondition . any_downstream_conditions () | cls . missing_or_changed () ) . with_label ( \"lazy\" )","title":"lazy"},{"location":"reference/utils/automation_conditions/#data_platform.utils.automation_conditions.CustomAutomationCondition.lazy_on_cron","text":"Returns an AutomationCondition which will cause a target to be executed if any downstream conditions are true or the partition is missing or changed. Will limit to only one execution for the given cron_schedule. Commonly used for intermediate assets that are used for downstream materializations, that have high frequency upstream assets, but themselves do not need to be updated as frequently. Source code in data_platform\\utils\\automation_conditions.py 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 @staticmethod def lazy_on_cron ( cron_schedule : str , cron_timezone : str = \"UTC\" , ignore_asset_keys : list [ list [ str ]] | None = None , ) -> AutomationCondition : \"\"\"Returns an AutomationCondition which will cause a target to be executed if any downstream conditions are true or the partition is missing or changed. Will limit to only one execution for the given cron_schedule. Commonly used for intermediate assets that are used for downstream materializations, that have high frequency upstream assets, but themselves do not need to be updated as frequently. \"\"\" ignore_asset_keys = ignore_asset_keys or [] return ( AutomationCondition . in_latest_time_window () & AutomationCondition . cron_tick_passed ( cron_schedule , cron_timezone ) . since_last_handled () & AutomationCondition . all_deps_updated_since_cron ( cron_schedule , cron_timezone ) . ignore ( AssetSelection . assets ( * ignore_asset_keys )) & ~ AutomationCondition . in_progress () ) . with_label ( f \"lazy_on_cron( { cron_schedule } , { cron_timezone } )\" )","title":"lazy_on_cron"},{"location":"reference/utils/automation_conditions/#data_platform.utils.automation_conditions.CustomAutomationCondition.manual","text":"Returns no AutomationCondition that will require a user to manually trigger. Used for overriding default automations for static assets. Source code in data_platform\\utils\\automation_conditions.py 17 18 19 20 21 22 @staticmethod def manual () -> None : \"\"\"Returns no AutomationCondition that will require a user to manually trigger. Used for overriding default automations for static assets. \"\"\" return None","title":"manual"},{"location":"reference/utils/automation_conditions/#data_platform.utils.automation_conditions.CustomAutomationCondition.missing_or_changed","text":"Returns no AutomationCondition that will trigger only if the asset has never been materialized, or if its definition has changed. Common use for dbt seeds that only need to be reloaded when the underlying csv file changes. Source code in data_platform\\utils\\automation_conditions.py 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 @staticmethod def missing_or_changed () -> AutomationCondition : \"\"\"Returns no AutomationCondition that will trigger only if the asset has never been materialized, or if its definition has changed. Common use for dbt seeds that only need to be reloaded when the underlying csv file changes. \"\"\" return ( AutomationCondition . in_latest_time_window () & ( AutomationCondition . code_version_changed () | AutomationCondition . newly_missing () ) . since_last_handled () & ~ AutomationCondition . in_progress () ) . with_label ( \"missing_or_changed\" )","title":"missing_or_changed"},{"location":"reference/utils/automation_conditions/#data_platform.utils.automation_conditions.CustomAutomationCondition.on_cron","text":"Returns an AutomationCondition which will cause a target to be executed on a given cron schedule, after all of its dependencies have been updated since the latest tick of that cron schedule. For time partitioned assets, only the latest time partition will be considered. Commonly used for assets that are far downstream and have users that directly interact with them, and have sensitivity to late arriving dimensions. Source code in data_platform\\utils\\automation_conditions.py 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 @staticmethod @override def on_cron ( cron_schedule : str , cron_timezone : str = \"UTC\" , ignore_asset_keys : list [ list [ str ]] | None = None , ) -> AndAutomationCondition : \"\"\"Returns an AutomationCondition which will cause a target to be executed on a given cron schedule, after all of its dependencies have been updated since the latest tick of that cron schedule. For time partitioned assets, only the latest time partition will be considered. Commonly used for assets that are far downstream and have users that directly interact with them, and have sensitivity to late arriving dimensions. \"\"\" ignore_asset_keys = ignore_asset_keys or [] return AutomationCondition . on_cron ( cron_schedule , cron_timezone ) . ignore ( AssetSelection . assets ( * ignore_asset_keys ) )","title":"on_cron"},{"location":"reference/utils/automation_conditions/#data_platform.utils.automation_conditions.CustomAutomationCondition.on_schedule","text":"Returns an AutomationCondition which will cause a target to be executed on a given cron schedule, regardless of the state of its dependencies For time partitioned assets, only the latest time partition will be considered. Commonly used for assets in the ingestion layer that should always run on a scheduled basis, and have no way of knowing when the source system has updates. Source code in data_platform\\utils\\automation_conditions.py 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 @staticmethod def on_schedule ( cron_schedule : str , cron_timezone : str = \"utc\" ) -> AutomationCondition : \"\"\"Returns an AutomationCondition which will cause a target to be executed on a given cron schedule, regardless of the state of its dependencies For time partitioned assets, only the latest time partition will be considered. Commonly used for assets in the ingestion layer that should always run on a scheduled basis, and have no way of knowing when the source system has updates. \"\"\" return ( AutomationCondition . in_latest_time_window () & AutomationCondition . cron_tick_passed ( cron_schedule , cron_timezone ) . since_last_handled () ) . with_label ( f \"on_schedule( { cron_schedule } , { cron_timezone } )\" )","title":"on_schedule"},{"location":"reference/utils/helpers/","text":"get_automation_condition_from_meta ( meta ) Return an AutomationCondition if valid configuartion is provided in the meta. Meta should be of format dict in the following structure: .. code-block:: python \"meta\":{ \"dagster\":{ \"automation_condition\": condition, \"automation_condition_config\": {argument: value} } } Source code in data_platform\\utils\\helpers.py 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 def get_automation_condition_from_meta ( meta : dict [ str , Any ], ) -> dg . AutomationCondition | None : \"\"\"Return an AutomationCondition if valid configuartion is provided in the meta. Meta should be of format dict in the following structure: .. code-block:: python \"meta\":{ \"dagster\":{ \"automation_condition\": condition, \"automation_condition_config\": {argument: value} } } \"\"\" condition_name = meta . get ( \"automation_condition\" ) if not condition_name : return None condition = CustomAutomationCondition . get_automation_condition ( condition_name ) if not isinstance ( condition , Callable ): raise KeyError ( f \"Automation condition not found for key ' { condition_name } '\" ) condition_config = meta . get ( \"automation_condition_config\" , {}) or {} if not isinstance ( condition_config , dict ): raise ValueError ( f \"Invalid condition config: ' { condition_config } '\" ) condition_config = sanitize_input_signature ( condition , condition_config ) try : return condition ( ** condition_config ) except Exception as e : e . add_note ( \"'condition_config' is missing required keys\" f \"for condition ' { condition_name } '\" ) raise get_nested ( config , path ) Helper function to safely traverse a nested dictionary that may have null values for a set key that is expected to be a dict. helpful because stream definitions that use only the default configs behave this way. .. code-block:: yaml streams: source.table_one: source.table_two: Source code in data_platform\\utils\\helpers.py 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 def get_nested ( config : dict , path : list ) -> Any : \"\"\"Helper function to safely traverse a nested dictionary that may have null values for a set key that is expected to be a dict. helpful because stream definitions that use only the default configs behave this way. .. code-block:: yaml streams: source.table_one: source.table_two: \"\"\" try : for item in path : config = config [ item ] return config except Exception : ... return None get_partitions_def_from_meta ( meta ) Return an TimeWindowPartitionsDefinition if valid configuartion is provided in the meta. - partition accepts the values: hourly, daily, weekly, monthly. - partition_start_date should be a iso format date, or timestamp. Meta should be of format dict in the following structure: .. code-block:: python \"meta\":{ \"dagster\":{ \"partition\": \"daily\", \"partition_start_date\": \"2025-01-01\" } } Source code in data_platform\\utils\\helpers.py 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 def get_partitions_def_from_meta ( meta : dict [ str , Any ], ) -> dg . TimeWindowPartitionsDefinition | None : \"\"\"Return an TimeWindowPartitionsDefinition if valid configuartion is provided in the meta. - partition accepts the values: hourly, daily, weekly, monthly. - partition_start_date should be a iso format date, or timestamp. Meta should be of format dict in the following structure: .. code-block:: python \"meta\":{ \"dagster\":{ \"partition\": \"daily\", \"partition_start_date\": \"2025-01-01\" } } \"\"\" try : partition = meta . get ( \"partition\" ) partition_start_date = meta . get ( \"partition_start_date\" ) if partition and partition_start_date : start_date = datetime . fromisoformat ( partition_start_date ) if partition == \"hourly\" : return dg . HourlyPartitionsDefinition ( start_date = start_date . strftime ( \"%Y-%m- %d -%H:%M\" ) ) if partition == \"daily\" : return dg . DailyPartitionsDefinition ( start_date = start_date . strftime ( \"%Y-%m- %d \" ) ) if partition == \"weekly\" : return dg . WeeklyPartitionsDefinition ( start_date = start_date . strftime ( \"%Y-%m- %d \" ) ) if partition == \"monthly\" : return dg . MonthlyPartitionsDefinition ( start_date = start_date . strftime ( \"%Y-%m- %d \" ) ) except Exception : ... return None sanitize_input_signature ( func , kwargs ) Remove any arguments that are not expected by the recieving function. Source code in data_platform\\utils\\helpers.py 91 92 93 94 95 96 97 98 99 100 101 def sanitize_input_signature ( func : Callable , kwargs : dict ) -> dict : \"\"\"Remove any arguments that are not expected by the recieving function.\"\"\" sig = signature ( func ) key_words = list ( kwargs . keys ()) expected_arguments = { argument for argument , _ in sig . parameters . items ()} for argument in key_words : if argument not in expected_arguments : kwargs . pop ( argument ) return kwargs","title":"Helpers"},{"location":"reference/utils/helpers/#data_platform.utils.helpers.get_automation_condition_from_meta","text":"Return an AutomationCondition if valid configuartion is provided in the meta. Meta should be of format dict in the following structure: .. code-block:: python \"meta\":{ \"dagster\":{ \"automation_condition\": condition, \"automation_condition_config\": {argument: value} } } Source code in data_platform\\utils\\helpers.py 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 def get_automation_condition_from_meta ( meta : dict [ str , Any ], ) -> dg . AutomationCondition | None : \"\"\"Return an AutomationCondition if valid configuartion is provided in the meta. Meta should be of format dict in the following structure: .. code-block:: python \"meta\":{ \"dagster\":{ \"automation_condition\": condition, \"automation_condition_config\": {argument: value} } } \"\"\" condition_name = meta . get ( \"automation_condition\" ) if not condition_name : return None condition = CustomAutomationCondition . get_automation_condition ( condition_name ) if not isinstance ( condition , Callable ): raise KeyError ( f \"Automation condition not found for key ' { condition_name } '\" ) condition_config = meta . get ( \"automation_condition_config\" , {}) or {} if not isinstance ( condition_config , dict ): raise ValueError ( f \"Invalid condition config: ' { condition_config } '\" ) condition_config = sanitize_input_signature ( condition , condition_config ) try : return condition ( ** condition_config ) except Exception as e : e . add_note ( \"'condition_config' is missing required keys\" f \"for condition ' { condition_name } '\" ) raise","title":"get_automation_condition_from_meta"},{"location":"reference/utils/helpers/#data_platform.utils.helpers.get_nested","text":"Helper function to safely traverse a nested dictionary that may have null values for a set key that is expected to be a dict. helpful because stream definitions that use only the default configs behave this way. .. code-block:: yaml streams: source.table_one: source.table_two: Source code in data_platform\\utils\\helpers.py 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 def get_nested ( config : dict , path : list ) -> Any : \"\"\"Helper function to safely traverse a nested dictionary that may have null values for a set key that is expected to be a dict. helpful because stream definitions that use only the default configs behave this way. .. code-block:: yaml streams: source.table_one: source.table_two: \"\"\" try : for item in path : config = config [ item ] return config except Exception : ... return None","title":"get_nested"},{"location":"reference/utils/helpers/#data_platform.utils.helpers.get_partitions_def_from_meta","text":"Return an TimeWindowPartitionsDefinition if valid configuartion is provided in the meta. - partition accepts the values: hourly, daily, weekly, monthly. - partition_start_date should be a iso format date, or timestamp. Meta should be of format dict in the following structure: .. code-block:: python \"meta\":{ \"dagster\":{ \"partition\": \"daily\", \"partition_start_date\": \"2025-01-01\" } } Source code in data_platform\\utils\\helpers.py 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 def get_partitions_def_from_meta ( meta : dict [ str , Any ], ) -> dg . TimeWindowPartitionsDefinition | None : \"\"\"Return an TimeWindowPartitionsDefinition if valid configuartion is provided in the meta. - partition accepts the values: hourly, daily, weekly, monthly. - partition_start_date should be a iso format date, or timestamp. Meta should be of format dict in the following structure: .. code-block:: python \"meta\":{ \"dagster\":{ \"partition\": \"daily\", \"partition_start_date\": \"2025-01-01\" } } \"\"\" try : partition = meta . get ( \"partition\" ) partition_start_date = meta . get ( \"partition_start_date\" ) if partition and partition_start_date : start_date = datetime . fromisoformat ( partition_start_date ) if partition == \"hourly\" : return dg . HourlyPartitionsDefinition ( start_date = start_date . strftime ( \"%Y-%m- %d -%H:%M\" ) ) if partition == \"daily\" : return dg . DailyPartitionsDefinition ( start_date = start_date . strftime ( \"%Y-%m- %d \" ) ) if partition == \"weekly\" : return dg . WeeklyPartitionsDefinition ( start_date = start_date . strftime ( \"%Y-%m- %d \" ) ) if partition == \"monthly\" : return dg . MonthlyPartitionsDefinition ( start_date = start_date . strftime ( \"%Y-%m- %d \" ) ) except Exception : ... return None","title":"get_partitions_def_from_meta"},{"location":"reference/utils/helpers/#data_platform.utils.helpers.sanitize_input_signature","text":"Remove any arguments that are not expected by the recieving function. Source code in data_platform\\utils\\helpers.py 91 92 93 94 95 96 97 98 99 100 101 def sanitize_input_signature ( func : Callable , kwargs : dict ) -> dict : \"\"\"Remove any arguments that are not expected by the recieving function.\"\"\" sig = signature ( func ) key_words = list ( kwargs . keys ()) expected_arguments = { argument for argument , _ in sig . parameters . items ()} for argument in key_words : if argument not in expected_arguments : kwargs . pop ( argument ) return kwargs","title":"sanitize_input_signature"},{"location":"reference/utils/keyvault_stub/","text":"SecretClient A stub keyvault to simulate an integration with Azure Keyvault. This would be replaced by a keyvault library. Source code in data_platform\\utils\\keyvault_stub.py 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 class SecretClient : \"\"\"A stub keyvault to simulate an integration with Azure Keyvault. This would be replaced by a keyvault library. \"\"\" def get_secret ( self , secret_name : str ) -> str : \"\"\"returns a secret from the keyvault\"\"\" secrets = self . __secrets location , _ , attribute = secret_name . split ( \"__\" ) secret = secrets . get ( location , {}) . get ( attribute ) return secret or \"\" def __init__ ( self , vault_url : str | None = None , credential : str | None = None ) -> None : secrets = { \"SOURCE\" : {}, \"DESTINATION\" : {}} env_path = Path ( __file__ ) . joinpath ( * [ \"..\" ] * 3 , \".env\" ) . resolve () set_env = os . getenv ( \"TARGET\" , \"\" ) with open ( env_path ) as env : for line in env : line = line . strip () if line . startswith ( set_env . upper ()) or line . startswith ( \"ANY\" ): key , secret = line . split ( \"=\" ) env , location , attribute = key . split ( \"__\" ) secrets [ location ][ attribute ] = secret self . __secrets = secrets get_secret ( secret_name ) returns a secret from the keyvault Source code in data_platform\\utils\\keyvault_stub.py 10 11 12 13 14 15 16 def get_secret ( self , secret_name : str ) -> str : \"\"\"returns a secret from the keyvault\"\"\" secrets = self . __secrets location , _ , attribute = secret_name . split ( \"__\" ) secret = secrets . get ( location , {}) . get ( attribute ) return secret or \"\"","title":"Keyvault stub"},{"location":"reference/utils/keyvault_stub/#data_platform.utils.keyvault_stub.SecretClient","text":"A stub keyvault to simulate an integration with Azure Keyvault. This would be replaced by a keyvault library. Source code in data_platform\\utils\\keyvault_stub.py 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 class SecretClient : \"\"\"A stub keyvault to simulate an integration with Azure Keyvault. This would be replaced by a keyvault library. \"\"\" def get_secret ( self , secret_name : str ) -> str : \"\"\"returns a secret from the keyvault\"\"\" secrets = self . __secrets location , _ , attribute = secret_name . split ( \"__\" ) secret = secrets . get ( location , {}) . get ( attribute ) return secret or \"\" def __init__ ( self , vault_url : str | None = None , credential : str | None = None ) -> None : secrets = { \"SOURCE\" : {}, \"DESTINATION\" : {}} env_path = Path ( __file__ ) . joinpath ( * [ \"..\" ] * 3 , \".env\" ) . resolve () set_env = os . getenv ( \"TARGET\" , \"\" ) with open ( env_path ) as env : for line in env : line = line . strip () if line . startswith ( set_env . upper ()) or line . startswith ( \"ANY\" ): key , secret = line . split ( \"=\" ) env , location , attribute = key . split ( \"__\" ) secrets [ location ][ attribute ] = secret self . __secrets = secrets","title":"SecretClient"},{"location":"reference/utils/keyvault_stub/#data_platform.utils.keyvault_stub.SecretClient.get_secret","text":"returns a secret from the keyvault Source code in data_platform\\utils\\keyvault_stub.py 10 11 12 13 14 15 16 def get_secret ( self , secret_name : str ) -> str : \"\"\"returns a secret from the keyvault\"\"\" secrets = self . __secrets location , _ , attribute = secret_name . split ( \"__\" ) secret = secrets . get ( location , {}) . get ( attribute ) return secret or \"\"","title":"get_secret"},{"location":"reference/utils/secrets/","text":"get_secret ( env_var_name ) A wrapper for a keyvault to integrate with the Dagster EnvVar class. Returns a secret from the keyvault and set it to an environment variable that can be used securly with dagsters EnvVar class. Source code in data_platform\\utils\\secrets.py 13 14 15 16 17 18 19 20 21 22 23 24 25 26 def get_secret ( env_var_name : str ) -> dg . EnvVar : \"\"\"A wrapper for a keyvault to integrate with the Dagster EnvVar class. Returns a secret from the keyvault and set it to an environment variable that can be used securly with dagsters EnvVar class. \"\"\" if secret := keyvault . get_secret ( env_var_name ): os . environ [ env_var_name ] = secret return dg . EnvVar ( env_var_name ) raise ValueError ( f \"Secret for key ' { env_var_name } ' not found.\" \"Please check that this is the correct key.\" )","title":"Secrets"},{"location":"reference/utils/secrets/#data_platform.utils.secrets.get_secret","text":"A wrapper for a keyvault to integrate with the Dagster EnvVar class. Returns a secret from the keyvault and set it to an environment variable that can be used securly with dagsters EnvVar class. Source code in data_platform\\utils\\secrets.py 13 14 15 16 17 18 19 20 21 22 23 24 25 26 def get_secret ( env_var_name : str ) -> dg . EnvVar : \"\"\"A wrapper for a keyvault to integrate with the Dagster EnvVar class. Returns a secret from the keyvault and set it to an environment variable that can be used securly with dagsters EnvVar class. \"\"\" if secret := keyvault . get_secret ( env_var_name ): os . environ [ env_var_name ] = secret return dg . EnvVar ( env_var_name ) raise ValueError ( f \"Secret for key ' { env_var_name } ' not found.\" \"Please check that this is the correct key.\" )","title":"get_secret"}]}