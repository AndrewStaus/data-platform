{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Data Platform","text":"<p>Welcome to the data platform documentation page. This page provides documentation about the dagster implementation.</p>"},{"location":"#utils","title":"Utils","text":"<p>Utils contains the utility functions that are shared between different integrations.  Notably the definitions for Automation Condtions are availible here.</p>"},{"location":"#lib","title":"Lib","text":"<p>Lib contains the library code that translates configurations into dagster artifacts.</p>"},{"location":"#definitions","title":"Definitions","text":"<p>Definitions is how Dagster loads the artifacts.</p>"},{"location":"#dbt","title":"DBT","text":"<p>A static dbt documentation refrence can be viewed at the link below:</p> <p> Dbt Documentation</p>"},{"location":"reference/definitions/","title":"Definitions","text":""},{"location":"reference/defs/_azureml/definitions/","title":"Definitions","text":""},{"location":"reference/defs/_powerbi/definitions/","title":"Definitions","text":""},{"location":"reference/defs/dbt/definitions/","title":"Definitions","text":""},{"location":"reference/defs/dbt/definitions/#data_platform.defs.dbt.definitions.defs","title":"<code>defs()</code>","text":"<p>Returns set of definitions explicitly available and loadable by Dagster tools. Will be automatically dectectd and loaded by the load_defs function in the root definitions file.</p> <p>@definitions decorator will provides lazy loading so that the assets are only instantiated when needed.</p> Source code in <code>data_platform\\defs\\dbt\\definitions.py</code> <pre><code>@definitions\ndef defs() -&gt; Definitions:\n    \"\"\"Returns set of definitions explicitly available and loadable by Dagster tools.\n    Will be automatically dectectd and loaded by the load_defs function in the root\n    definitions file.\n\n    @definitions decorator will provides lazy loading so that the assets are only\n    instantiated when needed.\n    \"\"\"\n    project_dir = Path(__file__).joinpath(*[\"..\"] * 4, \"dbt/\").resolve()\n    state_path = \"state/\"\n\n    def dbt() -&gt; DbtProject:\n        project = DbtProject(\n            project_dir=project_dir,\n            target=os.getenv(\"TARGET\", \"prod\"),\n            state_path=state_path,\n            profile=\"dbt\",\n        )\n        if os.getenv(\"PREPARE_IF_DEV\") == \"1\":\n            project.prepare_if_dev()\n        return project\n\n    return DagsterDbtFactory.build_definitions(dbt)\n</code></pre>"},{"location":"reference/defs/dlthub/resources/","title":"Resources","text":""},{"location":"reference/defs/dlthub/resources/#data_platform.defs.dlthub.resources.defs","title":"<code>defs()</code>","text":"<p>Returns set of definitions explicitly available and loadable by Dagster tools. Will be automatically dectectd and loaded by the load_defs function in the root definitions file.</p> <p>Assets and asset checks for dltHub are defined in the dlthub subfolder in the definitions.py file for each resource.</p> <p>@definitions decorator will provides lazy loading so that the assets are only instantiated when needed.</p> Source code in <code>data_platform\\defs\\dlthub\\resources.py</code> <pre><code>@definitions\ndef defs() -&gt; Definitions:\n    \"\"\"Returns set of definitions explicitly available and loadable by Dagster tools.\n    Will be automatically dectectd and loaded by the load_defs function in the root\n    definitions file.\n\n    Assets and asset checks for dltHub are defined in the dlthub subfolder in the\n    definitions.py file for each resource.\n\n    @definitions decorator will provides lazy loading so that the assets are only\n    instantiated when needed.\n    \"\"\"\n    import os\n\n    import dagster as dg\n    from dagster_dlt import DagsterDltResource\n\n    from ...utils.keyvault_stub import SecretClient\n\n    kv = SecretClient(\n        vault_url=os.getenv(\"AZURE_KEYVAULT_URL\"),\n        credential=os.getenv(\"AZURE_KEYVAULT_CREDENTIAL\"),\n    )\n\n    os.environ[\"DESTINATION__SNOWFLAKE__CREDENTIALS__HOST\"] = kv.get_secret(\n        \"DESTINATION__SNOWFLAKE__HOST\"\n    )\n    os.environ[\"DESTINATION__SNOWFLAKE__CREDENTIALS__USERNAME\"] = kv.get_secret(\n        \"DESTINATION__SNOWFLAKE__USER\"\n    )\n    os.environ[\"DESTINATION__SNOWFLAKE__CREDENTIALS__PASSWORD\"] = kv.get_secret(\n        \"DESTINATION__SNOWFLAKE__PASSWORD\"\n    )\n    os.environ[\"DESTINATION__SNOWFLAKE__CREDENTIALS__DATABASE\"] = kv.get_secret(\n        \"DESTINATION__SNOWFLAKE__DATABASE\"\n    )\n    os.environ[\"DESTINATION__SNOWFLAKE__CREDENTIALS__ROLE\"] = kv.get_secret(\n        \"DESTINATION__SNOWFLAKE__ROLE\"\n    )\n    os.environ[\"DESTINATION__SNOWFLAKE__CREDENTIALS__WAREHOUSE\"] = kv.get_secret(\n        \"DESTINATION__SNOWFLAKE__WAREHOUSE\"\n    )\n\n    os.environ[\"ENABLE_DATASET_NAME_NORMALIZATION\"] = \"false\"\n\n    return dg.Definitions(resources={\"dlt\": DagsterDltResource()})\n</code></pre>"},{"location":"reference/defs/dlthub/dlthub/exchange_rate/data/","title":"Data","text":""},{"location":"reference/defs/dlthub/dlthub/exchange_rate/data/#data_platform.defs.dlthub.dlthub.exchange_rate.data.get_exchange_rate","title":"<code>get_exchange_rate(currency)</code>","text":"<p>Return a generator that will yield responses from an api with daily exchange rates for the selected currency</p> Source code in <code>data_platform\\defs\\dlthub\\dlthub\\exchange_rate\\data.py</code> <pre><code>def get_exchange_rate(currency: str) -&gt; Callable[[], Any]:\n    \"\"\"Return a generator that will yield responses from an api\n    with daily exchange rates for the selected currency\n    \"\"\"\n\n    uri = (\n        \"https://cdn.jsdelivr.net/npm/@fawazahmed0/currency-api\"\n        \"@latest\"\n        \"/v1/\"\n        f\"currencies/{currency}.json\"\n    )\n\n    def exchange_api() -&gt; Generator[Any, Any, None]:\n        response = requests.get(uri)\n        yield response.json()\n        while next_uri := response.json().get(\"next_page\"):\n            response = requests.get(next_uri)\n            yield response.json()\n\n    return exchange_api\n</code></pre>"},{"location":"reference/defs/dlthub/dlthub/exchange_rate/definitions/","title":"Definitions","text":""},{"location":"reference/defs/dlthub/dlthub/facebook_ads/data/","title":"Data","text":""},{"location":"reference/defs/dlthub/dlthub/facebook_ads/data/#data_platform.defs.dlthub.dlthub.facebook_ads.data.get_campaigns","title":"<code>get_campaigns()</code>","text":"<p>A generator that will yield responses from a stub representing an api to download data from facebook ads.</p> Source code in <code>data_platform\\defs\\dlthub\\dlthub\\facebook_ads\\data.py</code> <pre><code>def get_campaigns() -&gt; Generator[list[dict[str, Any]], Any, None]:\n    \"\"\"A generator that will yield responses from a stub representing an api to\n    download data from facebook ads.\n    \"\"\"\n\n    response = [\n        {\n            \"id\": 90009,\n            \"name\": \"summer_sale\",\n            \"start_date\": \"2024-06-01\",\n            \"updated\": \"2025-07-02 21:14:03\",\n        },\n        {\n            \"id\": 80008,\n            \"name\": \"winter_sale\",\n            \"start_date\": \"2024-01-01\",\n            \"updated\": \"2025-07-02 21:14:03\",\n        },\n        {\n            \"id\": 70008,\n            \"name\": \"LAPTOP\",\n            \"start_date\": \"2024-01-01\",\n            \"updated\": \"2025-07-03 21:14:03\",\n        },\n        {\n            \"id\": 60008,\n            \"name\": \"greenfrog\",\n            \"start_date\": \"2024-01-01\",\n            \"updated\": \"2025-07-04 21:14:03\",\n        },\n        {\n            \"id\": 50008,\n            \"name\": \"blowout\",\n            \"start_date\": \"2024-01-01\",\n            \"updated\": \"2025-07-05 21:14:03\",\n        },\n        {\n            \"id\": 40008,\n            \"name\": \"raindays\",\n            \"start_date\": \"2024-01-01\",\n            \"updated\": \"2025-07-06 21:14:03\",\n        },\n        {\n            \"id\": 30008,\n            \"name\": \"powersale\",\n            \"start_date\": \"2024-01-01\",\n            \"updated\": \"2025-07-07 21:14:03\",\n        },\n        {\n            \"id\": 20008,\n            \"name\": \"sale11111\",\n            \"start_date\": \"2024-01-01\",\n            \"updated\": \"2025-07-08 21:14:03\",\n        },\n        {\n            \"id\": 10008,\n            \"name\": \"sale11112\",\n            \"start_date\": \"2024-01-01\",\n            \"updated\": \"2025-07-09 21:14:03\",\n        },\n        {\n            \"id\": 11008,\n            \"name\": \"sale11113\",\n            \"start_date\": \"2024-01-01\",\n            \"updated\": \"2025-07-10 21:14:03\",\n        },\n        {\n            \"id\": 12008,\n            \"name\": \"sale11114\",\n            \"start_date\": \"2024-01-01\",\n            \"updated\": \"2025-07-11 21:14:03\",\n        },\n        {\n            \"id\": 13008,\n            \"name\": \"sale11115\",\n            \"start_date\": \"2024-01-01\",\n            \"updated\": \"2025-07-12 21:14:03\",\n        },\n        {\n            \"id\": 14008,\n            \"name\": \"sale11116\",\n            \"start_date\": \"2024-01-01\",\n            \"updated\": \"2025-07-13 21:14:03\",\n        },\n        {\n            \"id\": 15008,\n            \"name\": \"sale11117\",\n            \"start_date\": \"2024-01-01\",\n            \"updated\": \"2025-07-14 21:14:03\",\n        },\n        {\n            \"id\": 16008,\n            \"name\": \"sale11118\",\n            \"start_date\": \"2024-01-01\",\n            \"updated\": \"2025-07-15 21:14:03\",\n        },\n        {\n            \"id\": 17008,\n            \"name\": \"sale11119\",\n            \"start_date\": \"2024-01-01\",\n            \"updated\": \"2025-07-16 21:14:03\",\n        },\n        {\n            \"id\": 18008,\n            \"name\": \"sale11110\",\n            \"start_date\": \"2024-01-01\",\n            \"updated\": \"2025-07-17 21:14:03\",\n        },\n        {\n            \"id\": 19008,\n            \"name\": \"sale11121\",\n            \"start_date\": \"2024-01-01\",\n            \"updated\": \"2025-07-18 21:14:03\",\n        },\n        {\n            \"id\": 11001,\n            \"name\": \"sale11122\",\n            \"start_date\": \"2024-01-01\",\n            \"updated\": \"2025-07-19 21:14:03\",\n        },\n        {\n            \"id\": 11002,\n            \"name\": \"sale11123\",\n            \"start_date\": \"2024-01-01\",\n            \"updated\": \"2025-07-20 21:14:03\",\n        },\n        {\n            \"id\": 11003,\n            \"name\": \"sale11124\",\n            \"start_date\": \"2024-01-01\",\n            \"updated\": \"2025-07-21 21:14:03\",\n        },\n        {\n            \"id\": 11004,\n            \"name\": \"sale11125\",\n            \"start_date\": \"2024-01-01\",\n            \"updated\": \"2025-07-22 21:14:03\",\n        },\n        {\n            \"id\": 11005,\n            \"name\": \"sale11126\",\n            \"start_date\": \"2024-01-01\",\n            \"updated\": \"2025-07-23 21:14:03\",\n        },\n        {\n            \"id\": 11006,\n            \"name\": \"sale11127\",\n            \"start_date\": \"2024-01-01\",\n            \"updated\": \"2025-07-24 21:14:03\",\n        },\n        {\n            \"id\": 66007,\n            \"name\": \"sale11128\",\n            \"start_date\": \"2024-01-01\",\n            \"updated\": \"2025-07-25 21:14:03\",\n        },\n        {\n            \"id\": 43008,\n            \"name\": \"sale11129\",\n            \"start_date\": \"2024-01-01\",\n            \"updated\": \"2025-07-26 21:14:03\",\n        },\n        {\n            \"id\": 76009,\n            \"name\": \"sale11131\",\n            \"start_date\": \"2024-01-01\",\n            \"updated\": \"2025-07-27 21:14:03\",\n        },\n        {\n            \"id\": 65008,\n            \"name\": \"sale11132\",\n            \"start_date\": \"2024-01-01\",\n            \"updated\": \"2025-07-28 21:14:03\",\n        },\n        {\n            \"id\": 54007,\n            \"name\": \"sale11133\",\n            \"start_date\": \"2024-01-01\",\n            \"updated\": \"2025-07-29 21:14:03\",\n        },\n        {\n            \"id\": 43006,\n            \"name\": \"sale11134\",\n            \"start_date\": \"2024-01-01\",\n            \"updated\": \"2025-07-31 21:14:03\",\n        },\n        {\n            \"id\": 32005,\n            \"name\": \"sale11135\",\n            \"start_date\": \"2024-01-01\",\n            \"updated\": \"2025-08-01 21:14:03\",\n        },\n        {\n            \"id\": 21004,\n            \"name\": \"sale11136\",\n            \"start_date\": \"2024-01-01\",\n            \"updated\": \"2025-08-02 21:14:03\",\n        },\n        {\n            \"id\": 54003,\n            \"name\": \"sale11137\",\n            \"start_date\": \"2024-01-01\",\n            \"updated\": \"2025-08-03 21:14:03\",\n        },\n        {\n            \"id\": 66002,\n            \"name\": \"sale11138\",\n            \"start_date\": \"2024-01-01\",\n            \"updated\": \"2025-08-04 21:14:03\",\n        },\n        {\n            \"id\": 77001,\n            \"name\": \"sale11139\",\n            \"start_date\": \"2024-01-01\",\n            \"updated\": \"2025-08-05 21:14:03\",\n        },\n        {\n            \"id\": 88044,\n            \"name\": \"sale11141\",\n            \"start_date\": \"2024-01-01\",\n            \"updated\": \"2025-08-06 21:14:03\",\n        },\n        {\n            \"id\": 99033,\n            \"name\": \"sale11142\",\n            \"start_date\": \"2024-01-01\",\n            \"updated\": \"2025-08-07 21:14:03\",\n        },\n        {\n            \"id\": 33220,\n            \"name\": \"sale11143\",\n            \"start_date\": \"2024-01-01\",\n            \"updated\": \"2025-08-08 21:14:03\",\n        },\n    ]\n    yield response\n</code></pre>"},{"location":"reference/defs/dlthub/dlthub/facebook_ads/definitions/","title":"Definitions","text":""},{"location":"reference/defs/dlthub/dlthub/google_ads/data/","title":"Data","text":""},{"location":"reference/defs/dlthub/dlthub/google_ads/data/#data_platform.defs.dlthub.dlthub.google_ads.data.google_ads","title":"<code>google_ads(endpoint)</code>","text":"<p>Return a generator that will yield responses from a stub representing an api to download data from google ads.</p> Source code in <code>data_platform\\defs\\dlthub\\dlthub\\google_ads\\data.py</code> <pre><code>def google_ads(endpoint) -&gt; Callable[[], Generator[list[dict[str, Any]], Any, None]]:\n    \"\"\"Return a generator that will yield responses from a stub representing an api to\n    download data from google ads.\n    \"\"\"\n\n    if endpoint == \"get_campaigns\":\n\n        def get_campaigns() -&gt; Generator[list[dict[str, Any]], Any, None]:\n            response = [\n                {\n                    \"id\": 10001,\n                    \"name\": \"summer_sale\",\n                    \"start_date\": \"2024-06-01\",\n                    \"criteria\": [{\"id\": 1}, {\"id\": 2}],\n                },\n                {\n                    \"id\": 20002,\n                    \"name\": \"winter_sale\",\n                    \"start_date\": \"2024-01-01\",\n                    \"criteria\": [{\"id\": 2}],\n                },\n            ]\n            yield response\n\n        return get_campaigns\n\n    if endpoint == \"get_criterion\":\n\n        def get_criterion() -&gt; Generator[list[dict[str, Any]], Any, None]:\n            response = [\n                {\"id\": 1, \"type\": \"audience\", \"value\": \"summer_shoppers\"},\n                {\"id\": 2, \"type\": \"age\", \"value\": \"20-35\"},\n            ]\n            yield response\n\n        return get_criterion\n\n    else:\n        raise KeyError(f\"Endpoint '{endpoint}' is not implemented.\")\n</code></pre>"},{"location":"reference/defs/dlthub/dlthub/google_ads/definitions/","title":"Definitions","text":""},{"location":"reference/defs/sling/definitions/","title":"Definitions","text":""},{"location":"reference/defs/sling/definitions/#data_platform.defs.sling.definitions.defs","title":"<code>defs()</code>","text":"<p>Returns set of definitions explicitly available and loadable by Dagster tools. Will be automatically dectectd and loaded by the load_defs function in the root definitions file.</p> <p>@definitions decorator will provides lazy loading so that the assets are only instantiated when needed.</p> Source code in <code>data_platform\\defs\\sling\\definitions.py</code> <pre><code>@definitions\ndef defs() -&gt; Definitions:\n    \"\"\"Returns set of definitions explicitly available and loadable by Dagster tools.\n    Will be automatically dectectd and loaded by the load_defs function in the root\n    definitions file.\n\n    @definitions decorator will provides lazy loading so that the assets are only\n    instantiated when needed.\n    \"\"\"\n    from pathlib import Path\n\n    from ...lib.sling import DagsterSlingFactory\n\n    config_dir = Path(__file__).joinpath(*[\"..\"], \"sling\").resolve()\n\n    return DagsterSlingFactory.build_definitions(config_dir)\n</code></pre>"},{"location":"reference/lib/lib/","title":"Lib","text":""},{"location":"reference/lib/dbt/constants/","title":"Constants","text":"<p>Constant values that are useful in selecting dbt models.</p>"},{"location":"reference/lib/dbt/dbt/","title":"Dbt","text":""},{"location":"reference/lib/dbt/dbt/#data_platform.lib.dbt.DagsterDbtFactory","title":"<code>DagsterDbtFactory</code>","text":"<p>Factory to generate dagster definitions from a dbt project.</p> Source code in <code>data_platform\\lib\\dbt\\__init__.py</code> <pre><code>class DagsterDbtFactory:\n    \"\"\"Factory to generate dagster definitions from a dbt project.\"\"\"\n\n    @cache\n    @staticmethod\n    def build_definitions(dbt: Callable[[], DbtProject]) -&gt; dg.Definitions:\n        \"\"\"Returns a Definitions object from a dbt project.\"\"\"\n\n        assets = [\n            DagsterDbtFactory._get_assets(\n                \"dbt_partitioned_models\",\n                dbt=dbt,\n                select=TIME_PARTITION_SELECTOR,\n                partitioned=True,\n            ),\n            DagsterDbtFactory._get_assets(\n                \"dbt_non_partitioned_models\",\n                dbt=dbt,\n                exclude=TIME_PARTITION_SELECTOR,\n                partitioned=False,\n            ),\n        ]\n\n        freshness_checks = build_freshness_checks_from_dbt_assets(dbt_assets=assets)\n        freshness_sensor = dg.build_sensor_for_freshness_checks(\n            freshness_checks=freshness_checks, name=\"dbt_freshness_checks_sensor\"\n        )\n\n        return dg.Definitions(\n            resources={\n                \"dbt\": DbtCliResource(\n                    project_dir=dbt(),\n                )\n            },\n            assets=assets,\n            asset_checks=freshness_checks,\n            sensors=[freshness_sensor],\n        )\n\n    @cache\n    @staticmethod\n    def _get_assets(\n        name: str | None,\n        dbt: Callable[[], DbtProject],\n        partitioned: bool = False,\n        select: str = DBT_DEFAULT_SELECT,\n        exclude: str | None = None,\n    ) -&gt; dg.AssetsDefinition:\n        \"\"\"Returns a AssetsDefinition with different execution for partitioned\n        and non-partitioned models so that they can be ran on the same job.\n        \"\"\"\n        dbt_project = dbt()\n        assert dbt_project\n\n        @dbt_assets(\n            name=name,\n            manifest=dbt_project.manifest_path,\n            select=select,\n            exclude=exclude,\n            dagster_dbt_translator=CustomDagsterDbtTranslator(\n                settings=DagsterDbtTranslatorSettings(\n                    enable_duplicate_source_asset_keys=True,\n                )\n            ),\n            backfill_policy=dg.BackfillPolicy.single_run(),\n            project=dbt_project,\n            pool=\"dbt\",\n        )\n        def assets(\n            context: dg.AssetExecutionContext, dbt: DbtCliResource, config: DbtConfig\n        ) -&gt; Generator[DbtEventIterator, Any, Any]:\n            args = [\"build\"]\n\n            if config.full_refresh:\n                args.append(\"--full-refresh\")\n            if config.defer_to_prod:\n                args.extend(dbt.get_defer_args())\n                if config.favor_state:\n                    args.append(\"--favor-state\")\n\n            if partitioned:\n                time_window = context.partition_time_window\n                format = \"%Y-%m-%d %H:%M:%S\"\n                dbt_vars = {\n                    \"min_date\": time_window.start.strftime(format),\n                    \"max_date\": time_window.end.strftime(format),\n                }\n\n                args.extend((\"--vars\", json.dumps(dbt_vars)))\n\n                yield from dbt.cli(\n                    args, context=context\n                ).stream()  # .with_insights() # type: ignore\n            else:\n                yield from dbt.cli(\n                    args, context=context\n                ).stream()  # .with_insights() # type: ignore\n\n        return assets\n</code></pre>"},{"location":"reference/lib/dbt/dbt/#data_platform.lib.dbt.DagsterDbtFactory.build_definitions","title":"<code>build_definitions(dbt)</code>  <code>cached</code> <code>staticmethod</code>","text":"<p>Returns a Definitions object from a dbt project.</p> Source code in <code>data_platform\\lib\\dbt\\__init__.py</code> <pre><code>@cache\n@staticmethod\ndef build_definitions(dbt: Callable[[], DbtProject]) -&gt; dg.Definitions:\n    \"\"\"Returns a Definitions object from a dbt project.\"\"\"\n\n    assets = [\n        DagsterDbtFactory._get_assets(\n            \"dbt_partitioned_models\",\n            dbt=dbt,\n            select=TIME_PARTITION_SELECTOR,\n            partitioned=True,\n        ),\n        DagsterDbtFactory._get_assets(\n            \"dbt_non_partitioned_models\",\n            dbt=dbt,\n            exclude=TIME_PARTITION_SELECTOR,\n            partitioned=False,\n        ),\n    ]\n\n    freshness_checks = build_freshness_checks_from_dbt_assets(dbt_assets=assets)\n    freshness_sensor = dg.build_sensor_for_freshness_checks(\n        freshness_checks=freshness_checks, name=\"dbt_freshness_checks_sensor\"\n    )\n\n    return dg.Definitions(\n        resources={\n            \"dbt\": DbtCliResource(\n                project_dir=dbt(),\n            )\n        },\n        assets=assets,\n        asset_checks=freshness_checks,\n        sensors=[freshness_sensor],\n    )\n</code></pre>"},{"location":"reference/lib/dbt/dbt/#data_platform.lib.dbt.DbtConfig","title":"<code>DbtConfig</code>","text":"<p>               Bases: <code>Config</code></p> <p>Exposes configuration options to end users in the Dagster launchpad.</p> Source code in <code>data_platform\\lib\\dbt\\__init__.py</code> <pre><code>class DbtConfig(dg.Config):\n    \"\"\"Exposes configuration options to end users in the Dagster\n    launchpad.\n    \"\"\"\n\n    full_refresh: bool = False\n    defer_to_prod: bool = defer_to_prod\n    favor_state: bool = False\n</code></pre>"},{"location":"reference/lib/dbt/translator/","title":"Translator","text":""},{"location":"reference/lib/dbt/translator/#data_platform.lib.dbt.translator.CustomDagsterDbtTranslator","title":"<code>CustomDagsterDbtTranslator</code>","text":"<p>               Bases: <code>DagsterDbtTranslator</code></p> <p>Overrides methods of the standard translator.</p> <p>Holds a set of methods that derive Dagster asset definition metadata given a representation of a dbt resource (models, tests, sources, etc). Methods are overriden to customize the implementation.</p> <p>See parent class for details on the purpose of each override</p> Source code in <code>data_platform\\lib\\dbt\\translator.py</code> <pre><code>class CustomDagsterDbtTranslator(DagsterDbtTranslator):\n    \"\"\"Overrides methods of the standard translator.\n\n    Holds a set of methods that derive Dagster asset definition metadata given\n    a representation of a dbt resource (models, tests, sources, etc).\n    Methods are overriden to customize the implementation.\n\n    See parent class for details on the purpose of each override\"\"\"\n\n    @override\n    def get_asset_key(self, dbt_resource_props: Mapping[str, Any]) -&gt; dg.AssetKey:\n        meta = dbt_resource_props.get(\"config\", {}).get(\n            \"meta\", {}\n        ) or dbt_resource_props.get(\"meta\", {})\n        meta_dagster = meta.get(\"dagster\") or {}\n        asset_key_config = meta_dagster.get(\"asset_key\")\n        if asset_key_config:\n            return dg.AssetKey(asset_key_config)\n\n        prop_key = \"name\"\n        if dbt_resource_props.get(\"version\"):\n            prop_key = \"alias\"\n\n        if dbt_resource_props[\"resource_type\"] == \"source\":\n            schema = dbt_resource_props[\"source_name\"]\n            table = dbt_resource_props[\"name\"]\n            step = \"raw\"\n            return dg.AssetKey([schema, step, table])\n\n        parsed_name = re.search(\"(.*?)_(.*)__(.*)\", dbt_resource_props[prop_key])\n        if parsed_name:\n            schema = parsed_name.group(2)\n            table = parsed_name.group(3)\n            step = parsed_name.group(1)\n            return dg.AssetKey([schema, step, table])\n\n        return super().get_asset_key(dbt_resource_props)\n\n    @override\n    def get_group_name(self, dbt_resource_props: Mapping[str, Any]) -&gt; str | None:\n        prop_key = \"name\"\n        if dbt_resource_props.get(\"version\"):\n            prop_key = \"alias\"\n        parsed_name = re.search(\"(.*?)_(.*)__(.*)\", dbt_resource_props[prop_key])\n        if parsed_name:\n            schema = parsed_name.group(2)\n            return schema\n\n        return super().get_group_name(dbt_resource_props)\n\n    @override\n    def get_partitions_def(\n        self, dbt_resource_props: Mapping[str, Any]\n    ) -&gt; dg.PartitionsDefinition | None:\n        meta = dbt_resource_props.get(\"config\", {}).get(\"meta\", {}).get(\"dagster\", {})\n        return get_partitions_def_from_meta(meta)\n\n    @override\n    def get_automation_condition(\n        self, dbt_resource_props: Mapping[str, Any]\n    ) -&gt; dg.AutomationCondition | None:\n        meta = dbt_resource_props.get(\"config\", {}).get(\"meta\", {}).get(\"dagster\", {})\n        automation_condition = get_automation_condition_from_meta(meta)\n        if automation_condition:\n            return automation_condition\n\n        # default settings for resource types\n        resource_type = dbt_resource_props.get(\"resource_type\")\n        if resource_type == \"snapshot\":\n            return CustomAutomationCondition.eager_with_deps_checks()\n\n        if resource_type == \"seed\":\n            return CustomAutomationCondition.code_version_changed()\n\n        else:\n            return CustomAutomationCondition.lazy()\n\n    @override\n    def get_tags(self, dbt_resource_props: Mapping[str, Any]) -&gt; Mapping[str, str]:\n        tags = super().get_tags(dbt_resource_props)\n        return tags\n</code></pre>"},{"location":"reference/lib/dlthub/dlthub/","title":"Dlthub","text":""},{"location":"reference/lib/dlthub/dlthub/#data_platform.lib.dlthub.ConfigurableDltResource","title":"<code>ConfigurableDltResource</code>","text":"<p>               Bases: <code>DltResource</code></p> <p>Wrapper class to add aditional attributes to the DltResource class.  These attributes are used in the factory to add aditional configuration such as automation conditions, asset checks, tags, and upstream external assets.</p> Source code in <code>data_platform\\lib\\dlthub\\__init__.py</code> <pre><code>class ConfigurableDltResource(DltResource):\n    \"\"\"Wrapper class to add aditional attributes to the DltResource class.  These\n    attributes are used in the factory to add aditional configuration such as automation\n    conditions, asset checks, tags, and upstream external assets.\n    \"\"\"\n\n    meta: dict | None\n    tags: list | None\n    kinds: set | None\n\n    @staticmethod\n    def config(\n        resource: DltResource,\n        meta: dict | None = None,\n        tags: list[str] | None = None,\n        kinds: set[str] | None = None,\n    ) -&gt; \"ConfigurableDltResource\":\n        \"\"\"Returns a ConfigurableDltResource wrapped DltResource with aditional\n        attributes used by the factory to generate enhanced definitions.\n        \"\"\"\n        resource = ConfigurableDltResource._convert(resource, meta, tags, kinds)\n        return resource  # type: ignore\n\n    @staticmethod\n    def _convert(\n        dlt_resource: DltResource,\n        meta: dict | None,\n        tags: list[str] | None,\n        kinds: set[str] | None,\n    ) -&gt; \"ConfigurableDltResource\":\n        \"\"\"Helper method to wrap a DltResource\"\"\"\n        dlt_resource.tags = tags  # type: ignore\n        dlt_resource.meta = meta  # type: ignore\n        dlt_resource.kinds = kinds  # type: ignore\n        return dlt_resource  # type: ignore\n</code></pre>"},{"location":"reference/lib/dlthub/dlthub/#data_platform.lib.dlthub.ConfigurableDltResource.config","title":"<code>config(resource, meta=None, tags=None, kinds=None)</code>  <code>staticmethod</code>","text":"<p>Returns a ConfigurableDltResource wrapped DltResource with aditional attributes used by the factory to generate enhanced definitions.</p> Source code in <code>data_platform\\lib\\dlthub\\__init__.py</code> <pre><code>@staticmethod\ndef config(\n    resource: DltResource,\n    meta: dict | None = None,\n    tags: list[str] | None = None,\n    kinds: set[str] | None = None,\n) -&gt; \"ConfigurableDltResource\":\n    \"\"\"Returns a ConfigurableDltResource wrapped DltResource with aditional\n    attributes used by the factory to generate enhanced definitions.\n    \"\"\"\n    resource = ConfigurableDltResource._convert(resource, meta, tags, kinds)\n    return resource  # type: ignore\n</code></pre>"},{"location":"reference/lib/dlthub/translator/","title":"Translator","text":""},{"location":"reference/lib/dlthub/translator/#data_platform.lib.dlthub.translator.CustomDagsterDltTranslator","title":"<code>CustomDagsterDltTranslator</code>","text":"<p>               Bases: <code>DagsterDltTranslator</code></p> <p>Overrides methods of the standard translator.</p> <p>Holds a set of methods that derive Dagster asset definition metadata given a representation of dltHub resource (resources, pipes, etc).  Methods are overriden to customize the implementation.</p> <p>See parent class for details on the purpose of each override</p> Source code in <code>data_platform\\lib\\dlthub\\translator.py</code> <pre><code>class CustomDagsterDltTranslator(DagsterDltTranslator):\n    \"\"\"Overrides methods of the standard translator.\n\n    Holds a set of methods that derive Dagster asset definition metadata given a\n    representation of dltHub resource (resources, pipes, etc).  Methods are overriden to\n    customize the implementation.\n\n    See parent class for details on the purpose of each override\n    \"\"\"\n\n    @override\n    def get_asset_spec(self, data: DltResourceTranslatorData) -&gt; dg.AssetSpec:\n        return dg.AssetSpec(\n            key=self._resolve_back_compat_method(\n                \"get_asset_key\", self._default_asset_key_fn, data.resource\n            ),\n            automation_condition=self._resolve_back_compat_method(\n                \"get_automation_condition\",\n                self._default_automation_condition_fn,\n                data.resource,\n            ),\n            deps=self._resolve_back_compat_method(\n                \"get_deps_asset_keys\", self._default_deps_fn, data.resource\n            ),\n            description=self._resolve_back_compat_method(\n                \"get_description\", self._default_description_fn, data.resource\n            ),\n            group_name=self._resolve_back_compat_method(\n                \"get_group_name\", self._default_group_name_fn, data.resource\n            ),\n            metadata=self._resolve_back_compat_method(\n                \"get_metadata\", self._default_metadata_fn, data.resource\n            ),\n            owners=self._resolve_back_compat_method(\n                \"get_owners\", self._default_owners_fn, data.resource\n            ),\n            tags=self._resolve_back_compat_method(\n                \"get_tags\", self._default_tags_fn, data.resource\n            ),\n            kinds=self._resolve_back_compat_method(\n                \"get_kinds\", self._default_kinds_fn, data.resource, data.destination\n            ),\n            partitions_def=self.get_partitions_def(data.resource),\n        )\n\n    @override\n    def get_deps_asset_keys(self, resource: DltResource) -&gt; Iterable[dg.AssetKey]:\n        name: str | None = None\n        if resource.is_transformer:\n            pipe = resource._pipe\n            while pipe.has_parent:\n                pipe = pipe.parent\n                name = pipe.schema.name  # type: ignore\n        else:\n            name = resource.name\n        if name:\n            schema, table = name.split(\".\")\n            asset_key = [schema, \"src\", table]\n            return [dg.AssetKey(asset_key)]\n        return super().get_deps_asset_keys(resource)\n\n    @override\n    def get_asset_key(self, resource: DltResource) -&gt; dg.AssetKey:\n        schema, table = resource.name.split(\".\")\n        asset_key = [schema, \"raw\", table]\n        return dg.AssetKey(asset_key)\n\n    @override\n    def get_group_name(self, resource: DltResource) -&gt; str:\n        group = resource.name.split(\".\")[0]\n        return group\n\n    def get_partitions_def(\n        self, resource: DltResource\n    ) -&gt; dg.PartitionsDefinition | None:\n        try:\n            meta = resource.meta.get(\"dagster\")  # type: ignore\n            return get_partitions_def_from_meta(meta)\n        except Exception:\n            ...\n        return None\n\n    @override\n    def get_automation_condition(\n        self, resource: DltResource\n    ) -&gt; dg.AutomationCondition[Any] | None:\n        try:\n            meta = resource.meta.get(\"dagster\")  # type: ignore\n            automation_condition = get_automation_condition_from_meta(meta)\n            if automation_condition:\n                return automation_condition\n        except Exception:\n            ...\n        return super().get_automation_condition(resource)\n\n    @override\n    def get_tags(self, resource: DltResource) -&gt; Mapping[str, Any]:\n        try:\n            tags = resource.tags  # type: ignore\n            return {tag: \"\" for tag in tags if is_valid_tag_key(tag)}\n        except Exception:\n            ...\n        return {}\n</code></pre>"},{"location":"reference/lib/sling/sling/","title":"Sling","text":""},{"location":"reference/lib/sling/sling/#data_platform.lib.sling.DagsterSlingFactory","title":"<code>DagsterSlingFactory</code>","text":"<p>Factory to generate dagster definitions from Sling yaml config files.</p> Source code in <code>data_platform\\lib\\sling\\__init__.py</code> <pre><code>class DagsterSlingFactory:\n    \"\"\"Factory to generate dagster definitions from Sling yaml config files.\"\"\"\n\n    @cache\n    @staticmethod\n    def build_definitions(config_dir: Path) -&gt; dg.Definitions:\n        \"\"\"Returns a Definitions object for a path that contains Sling yaml configs.\"\"\"\n        connections = []\n        assets = []\n        freshness_checks = []\n        kind_map = {}\n\n        for config_path in os.listdir(config_dir):\n            if config_path.endswith(\".yaml\") or config_path.endswith(\".yml\"):\n                config_path = config_dir.joinpath(config_path).resolve()\n                with open(config_path) as file:\n                    config = yaml.load(file, Loader=yaml.FullLoader)\n                if not config:\n                    continue\n\n                if connection_configs := config.get(\"connections\"):\n                    connections, kind_map = DagsterSlingFactory._get_connections(\n                        connection_configs, connections, kind_map\n                    )\n\n                if replication_configs := config.get(\"replications\"):\n                    assets, freshness_checks = DagsterSlingFactory._get_replications(\n                        replication_configs, freshness_checks, kind_map, assets\n                    )\n\n        return dg.Definitions(\n            resources={\"sling\": SlingResource(connections=connections)},\n            assets=assets,\n            asset_checks=freshness_checks,\n            sensors=[\n                dg.build_sensor_for_freshness_checks(\n                    freshness_checks=freshness_checks,\n                    name=\"sling_freshness_checks_sensor\",\n                )\n            ],\n        )\n\n    @staticmethod\n    def _get_connections(\n        connection_configs, connections, kind_map\n    ) -&gt; tuple[list[SlingConnectionResource], dict[str, str]]:\n        \"\"\"Returns a list of SlingConnectionResource for connections in the Sling\n        yaml file.\n        \"\"\"\n        for connection_config in connection_configs:\n            if connection := DagsterSlingFactory._get_connection(connection_config):\n                source = connection_config.get(\"name\")\n                kind = connection_config.get(\"type\")\n                kind_map[source] = kind\n                connections.append(connection)\n\n        return connections, kind_map\n\n    @staticmethod\n    def _get_connection(connection_config: dict) -&gt; SlingConnectionResource | None:\n        \"\"\"Returns a SlingConnectionResource for a connection in the Sling yaml file.\"\"\"\n        for k, v in connection_config.items():\n            if isinstance(v, dict):\n                secret_name = list(v.keys())[0]\n                display_type = list(v.values())[0]\n\n                if display_type == \"show\":\n                    connection_config[k] = get_secret(secret_name).get_value()\n                else:\n                    connection_config[k] = get_secret(secret_name)\n\n        connection = SlingConnectionResource(**connection_config)\n        return connection\n\n    @staticmethod\n    def _get_replications(\n        replication_configs, freshness_checks, kind_map, assets\n    ) -&gt; tuple[list[dg.AssetsDefinition], list[dg.AssetChecksDefinition]]:\n        \"\"\"Returns a list of AssetsDefinitions for\n        replications in a Sling yaml file\n        \"\"\"\n        for replication_config in replication_configs:\n            if bool(os.getenv(\"DAGSTER_IS_DEV_CLI\")):\n                replication_config = DagsterSlingFactory._set_dev_schema(\n                    replication_config\n                )\n            assets_definition = DagsterSlingFactory._get_replication(replication_config)\n\n            kind = kind_map.get(replication_config.get(\"source\", None), None)\n            dep_asset_specs = DagsterSlingFactory._get_sling_deps(\n                replication_config, kind\n            )\n            asset_freshness_checks = DagsterSlingFactory._get_freshness_checks(\n                replication_config\n            )\n\n            if asset_freshness_checks:\n                freshness_checks.extend(asset_freshness_checks)\n            if assets_definition:\n                assets.append(assets_definition)\n            if dep_asset_specs:\n                assets.extend(dep_asset_specs)\n\n        return assets, freshness_checks\n\n    @staticmethod\n    def _get_replication(config: dict) -&gt; dg.AssetsDefinition:\n        \"\"\"Returns a AssetsDefinition for replication\n        in a Sling yaml file\n        \"\"\"\n\n        @sling_assets(\n            name=config[\"source\"] + \"_assets\",\n            replication_config=config,\n            backfill_policy=dg.BackfillPolicy.single_run(),\n            dagster_sling_translator=CustomDagsterSlingTranslator(),\n            pool=\"sling\",\n        )\n        def assets(\n            context: dg.AssetExecutionContext, sling: SlingResource\n        ) -&gt; Generator[SlingEventType, Any, None]:\n            if \"defaults\" not in config:\n                config[\"defaults\"] = {}\n\n            try:  # to inject start and end dates for partitioned runs\n                time_window = context.partition_time_window\n                if time_window:\n                    if \"source_options\" not in config[\"defaults\"]:\n                        config[\"defaults\"][\"source_options\"] = {}\n\n                    format = \"%Y-%m-%d %H:%M:%S\"\n                    start = time_window.start.strftime(format)\n                    end = time_window.end.strftime(format)\n                    config[\"defaults\"][\"source_options\"][\"range\"] = f\"{start},{end}\"\n            except Exception:  # run normal run if time window not provided\n                pass\n\n            yield from sling.replicate(\n                context=context,\n                replication_config=config,\n                dagster_sling_translator=CustomDagsterSlingTranslator(),\n            )\n            for row in sling.stream_raw_logs():\n                context.log.info(row)\n\n        return assets\n\n    @staticmethod\n    def _set_dev_schema(replication_config: dict) -&gt; dict:\n        \"\"\"Override the desination schema set in the yaml file when the environment\n        is set to dev to point to a unique schema based on the developer.\n        \"\"\"\n        user = os.environ[\"DESTINATION__SNOWFLAKE__CREDENTIALS__USERNAME\"].upper()\n        if default_object := replication_config[\"defaults\"][\"object\"]:\n            schema, table = default_object.split(\".\")\n            replication_config[\"defaults\"][\"object\"] = f\"{schema}__{user}.{table}\"\n\n        for stream, stream_config in list(\n            replication_config.get(\"streams\", {}).items()\n        ):\n            stream_config = stream_config or {}\n            if stream_object := stream_config.get(\"object\"):\n                schema, table = stream_object.split(\".\")\n                replication_config[\"streams\"][stream][\"object\"] = (\n                    f\"{schema}__{user}.{table}\"\n                )\n\n        return replication_config\n\n    @staticmethod\n    def _get_sling_deps(\n        replication_config: dict, kind: str | None\n    ) -&gt; list[dg.AssetSpec] | None:\n        \"\"\"Create an external asset that is placed in the same prefix\n        as the asset, and assigned the correct resource kind.\n        \"\"\"\n        kinds = {kind} if kind else None\n\n        deps = []\n        for k in replication_config[\"streams\"]:\n            schema, table = k.split(\".\")\n            dep = dg.AssetSpec(\n                key=[schema, \"src\", table], group_name=schema, kinds=kinds\n            )\n            deps.append(dep)\n        return deps\n\n    @staticmethod\n    def _get_freshness_checks(\n        replication_config: dict,\n    ) -&gt; list[dg.AssetChecksDefinition]:\n        \"\"\"Returns a list of AssetChecksDefinition for replication configs.\n        Configs supplied on the stream will take priority, otherwise the\n        default will be used.\n        \"\"\"\n        freshness_checks = []\n\n        default_freshness_check_config = (\n            get_nested(\n                replication_config, [\"defaults\", \"meta\", \"dagster\", \"freshness_check\"]\n            )\n            or {}\n        )\n        default_partition = get_nested(\n            replication_config, [\"defaults\", \"meta\", \"dagster\", \"partition\"]\n        )\n\n        streams = replication_config.get(\"streams\", {})\n        for stream_name, steam_config in streams.items():\n            freshness_check_config = (\n                get_nested(steam_config, [\"meta\", \"dagster\", \"freshness_check\"]) or {}\n            )\n            partition = get_nested(steam_config, [\"meta\", \"dagster\", \"partition\"])\n\n            freshness_check_config = (\n                freshness_check_config | default_freshness_check_config\n            )\n            partition = partition or default_partition\n\n            if freshness_check_config:\n                if lower_bound_delta_seconds := freshness_check_config.pop(\n                    \"lower_bound_delta_seconds\", None\n                ):\n                    lower_bound_delta = timedelta(\n                        seconds=float(lower_bound_delta_seconds)\n                    )\n                    freshness_check_config[\"lower_bound_delta\"] = lower_bound_delta\n\n                schema, table_name = stream_name.split(\".\")\n                asset_key = [schema, \"raw\", table_name]\n                freshness_check_config[\"assets\"] = [asset_key]\n\n                try:\n                    if partition in [\"hourly\", \"daily\", \"weekly\", \"monthly\"]:\n                        freshness_check_config = sanitize_input_signature(\n                            dg.build_time_partition_freshness_checks,\n                            freshness_check_config,\n                        )\n\n                        time_partition_update_freshness_checks = (\n                            dg.build_time_partition_freshness_checks(\n                                **freshness_check_config\n                            )\n                        )\n                        freshness_checks.extend(time_partition_update_freshness_checks)\n\n                    else:\n                        freshness_check_config = sanitize_input_signature(\n                            dg.build_last_update_freshness_checks,\n                            freshness_check_config,\n                        )\n\n                        last_update_freshness_checks = (\n                            dg.build_last_update_freshness_checks(\n                                **freshness_check_config\n                            )\n                        )\n                        freshness_checks.extend(last_update_freshness_checks)\n                except TypeError as e:\n                    raise TypeError(\n                        \"Error creating freshness check, check your configuration for \"\n                        f\"'{asset_key}'. Supplied arguments: {freshness_check_config}\"\n                    ) from e\n\n        return freshness_checks\n</code></pre>"},{"location":"reference/lib/sling/sling/#data_platform.lib.sling.DagsterSlingFactory.build_definitions","title":"<code>build_definitions(config_dir)</code>  <code>cached</code> <code>staticmethod</code>","text":"<p>Returns a Definitions object for a path that contains Sling yaml configs.</p> Source code in <code>data_platform\\lib\\sling\\__init__.py</code> <pre><code>@cache\n@staticmethod\ndef build_definitions(config_dir: Path) -&gt; dg.Definitions:\n    \"\"\"Returns a Definitions object for a path that contains Sling yaml configs.\"\"\"\n    connections = []\n    assets = []\n    freshness_checks = []\n    kind_map = {}\n\n    for config_path in os.listdir(config_dir):\n        if config_path.endswith(\".yaml\") or config_path.endswith(\".yml\"):\n            config_path = config_dir.joinpath(config_path).resolve()\n            with open(config_path) as file:\n                config = yaml.load(file, Loader=yaml.FullLoader)\n            if not config:\n                continue\n\n            if connection_configs := config.get(\"connections\"):\n                connections, kind_map = DagsterSlingFactory._get_connections(\n                    connection_configs, connections, kind_map\n                )\n\n            if replication_configs := config.get(\"replications\"):\n                assets, freshness_checks = DagsterSlingFactory._get_replications(\n                    replication_configs, freshness_checks, kind_map, assets\n                )\n\n    return dg.Definitions(\n        resources={\"sling\": SlingResource(connections=connections)},\n        assets=assets,\n        asset_checks=freshness_checks,\n        sensors=[\n            dg.build_sensor_for_freshness_checks(\n                freshness_checks=freshness_checks,\n                name=\"sling_freshness_checks_sensor\",\n            )\n        ],\n    )\n</code></pre>"},{"location":"reference/lib/sling/translator/","title":"Translator","text":""},{"location":"reference/lib/sling/translator/#data_platform.lib.sling.translator.CustomDagsterSlingTranslator","title":"<code>CustomDagsterSlingTranslator</code>","text":"<p>               Bases: <code>DagsterSlingTranslator</code></p> <p>Overrides methods of the standard translator.</p> <p>Holds a set of methods that derive Dagster asset definition metadata given a representation of Sling resource (connections, replications). Methods are overriden to customize the implementation.</p> <p>See parent class for details on the purpose of each override</p> Source code in <code>data_platform\\lib\\sling\\translator.py</code> <pre><code>class CustomDagsterSlingTranslator(dg_sling.DagsterSlingTranslator):\n    \"\"\"Overrides methods of the standard translator.\n\n    Holds a set of methods that derive Dagster asset definition metadata given a\n    representation of Sling resource (connections, replications). Methods are overriden\n    to customize the implementation.\n\n    See parent class for details on the purpose of each override\"\"\"\n\n    @override\n    def get_asset_spec(self, stream_definition: Mapping[str, Any]) -&gt; dg.AssetSpec:\n        return dg.AssetSpec(\n            automation_condition=self.get_automation_condition(stream_definition),\n            partitions_def=self.get_partitions_def(stream_definition),\n            key=self._resolve_back_compat_method(\n                \"get_asset_key\", self._default_asset_key_fn, stream_definition\n            ),\n            deps=self._resolve_back_compat_method(\n                \"get_deps_asset_key\", self._default_deps_fn, stream_definition\n            ),\n            description=self._resolve_back_compat_method(\n                \"get_description\", self._default_description_fn, stream_definition\n            ),\n            metadata=self._resolve_back_compat_method(\n                \"get_metadata\", self._default_metadata_fn, stream_definition\n            ),\n            tags=self._resolve_back_compat_method(\n                \"get_tags\", self._default_tags_fn, stream_definition\n            ),\n            kinds=self._resolve_back_compat_method(\n                \"get_kinds\", self._default_kinds_fn, stream_definition\n            ),\n            group_name=self._resolve_back_compat_method(\n                \"get_group_name\", self._default_group_name_fn, stream_definition\n            ),\n            legacy_freshness_policy=self._resolve_back_compat_method(\n                \"get_freshness_policy\",\n                self._default_freshness_policy_fn,\n                stream_definition,\n            ),\n            auto_materialize_policy=self._resolve_back_compat_method(\n                \"get_auto_materialize_policy\",\n                self._default_auto_materialize_policy_fn,\n                stream_definition,\n            ),\n        )\n\n    @override\n    def get_asset_key(self, stream_definition: Mapping[str, Any]) -&gt; dg.AssetKey:\n        config = stream_definition.get(\"config\") or {}\n        meta = config.get(\"meta\") or {}\n        dagster = meta.get(\"dagster\") or {}\n        asset_key = dagster.get(\"asset_key\", None)\n\n        if asset_key:\n            if self.sanitize_stream_name(asset_key) != asset_key:\n                raise ValueError(\n                    f\"Asset key {asset_key} for stream {stream_definition['name']} \"\n                    \"is not sanitized. Please use only alphanumeric characters \"\n                    \"and underscores.\"\n                )\n            return dg.AssetKey(asset_key.split(\".\"))\n\n        # You can override the Sling Replication default object with an object key\n        stream_name = stream_definition[\"name\"]\n        schema, table = self.sanitize_stream_name(stream_name).split(\".\")\n        return dg.AssetKey([schema, \"raw\", table])\n\n    @override\n    def get_deps_asset_key(\n        self, stream_definition: Mapping[str, Any]\n    ) -&gt; Iterable[dg.AssetKey]:\n        config = stream_definition.get(\"config\", {}) or {}\n        meta = config.get(\"meta\", {}) or {}\n        deps = meta.get(\"dagster\", {}).get(\"deps\")\n        deps_out = []\n        if deps and isinstance(deps, str):\n            deps = [deps]\n        if deps:\n            assert isinstance(deps, list)\n            for asset_key in deps:\n                if self.sanitize_stream_name(asset_key) != asset_key:\n                    raise ValueError(\n                        f\"Deps Asset key {asset_key} for stream  \"\n                        f\"{stream_definition['name']} is not sanitized. \"\n                        \"Please use only alphanumeric characters and underscores.\"\n                    )\n                deps_out.append(dg.AssetKey(asset_key.split(\".\")))\n            return deps_out\n\n        stream_name = stream_definition[\"name\"]\n        schema, table = self.sanitize_stream_name(stream_name).split(\".\")\n        return [dg.AssetKey([schema, \"src\", table])]\n\n    @override\n    def get_group_name(self, stream_definition: Mapping[str, Any]) -&gt; str:\n        try:\n            group = stream_definition[\"config\"][\"meta\"][\"dagster\"][\"group\"]\n            if group:\n                return group\n        except Exception:\n            ...\n\n        stream_name = stream_definition[\"name\"]\n        schema, _ = self.sanitize_stream_name(stream_name).split(\".\")\n        return schema\n\n    @override\n    def get_tags(self, stream_definition: Mapping[str, Any]) -&gt; Mapping[str, Any]:\n        try:\n            tags = stream_definition[\"config\"][\"meta\"][\"dagster\"][\"tags\"]\n            return {tag: \"\" for tag in tags if is_valid_tag_key(tag)}\n        except Exception:\n            ...\n        return {}\n\n    def get_automation_condition(\n        self, stream_definition: Mapping[str, Any]\n    ) -&gt; None | dg.AutomationCondition:\n        try:\n            meta = stream_definition[\"config\"][\"meta\"][\"dagster\"]\n            automation_condition = get_automation_condition_from_meta(meta)\n            return automation_condition\n        except Exception:\n            ...\n        return None\n\n    def get_partitions_def(\n        self, stream_definition: Mapping[str, Any]\n    ) -&gt; None | dg.PartitionsDefinition:\n        try:\n            meta = stream_definition[\"config\"][\"meta\"][\"dagster\"]\n            automation_condition = get_partitions_def_from_meta(meta)\n            return automation_condition\n        except Exception:\n            ...\n        return None\n</code></pre>"},{"location":"reference/utils/automation_conditions/","title":"Automation conditions","text":""},{"location":"reference/utils/automation_conditions/#data_platform.utils.automation_conditions.CustomAutomationCondition","title":"<code>CustomAutomationCondition</code>","text":"<p>               Bases: <code>AutomationCondition</code></p> Source code in <code>data_platform\\utils\\automation_conditions.py</code> <pre><code>class CustomAutomationCondition(AutomationCondition):\n    @classmethod\n    def get_automation_condition(\n        cls, automation_condition_name: str\n    ) -&gt; AutomationCondition | None:\n        methods = AutomationCondition.__dict__ | cls.__dict__\n        return methods.get(automation_condition_name, None)\n\n    @staticmethod\n    def manual() -&gt; None:\n        \"\"\"Returns no AutomationCondition that will require a user to manually trigger.\n        Used for overriding default automations for static assets.\n        \"\"\"\n        return None\n\n    @staticmethod\n    def missing_or_changed() -&gt; AutomationCondition:\n        \"\"\"Returns no AutomationCondition that will trigger only if the asset has never\n        been materialized, or if its definition has changed.\n\n        Common use for dbt seeds that only need to be reloaded when the underlying csv\n        file changes.\n        \"\"\"\n        return (\n            AutomationCondition.in_latest_time_window()\n            &amp; (\n                AutomationCondition.code_version_changed()\n                | AutomationCondition.newly_missing()\n            ).since_last_handled()\n            &amp; ~AutomationCondition.in_progress()\n        ).with_label(\"missing_or_changed\")\n\n    @override\n    @staticmethod\n    def eager() -&gt; AndAutomationCondition:\n        \"\"\"Returns an AutomationCondition which will cause a target to be executed if\n        any of its dependencies update, and will execute missing partitions if they\n        become missing after this condition is applied to the target. This will not\n        execute targets that have any missing or in progress dependencies, or are\n        currently in progress.\n\n        For time partitioned assets, only the latest time partition will be considered.\n        Commonly used for assets that are far downstream and have users that directly\n        interact with them, and do not have sensitivity to late arriving dimensions.\n        \"\"\"\n        return (\n            AutomationCondition.in_latest_time_window()\n            &amp; (\n                AutomationCondition.newly_missing()\n                | AutomationCondition.any_deps_updated()\n            ).since_last_handled()\n            &amp; ~AutomationCondition.any_deps_missing()\n            &amp; ~AutomationCondition.any_deps_in_progress()\n            &amp; ~AutomationCondition.in_progress()\n        ).with_label(\"eager\")\n\n    @staticmethod\n    def eager_with_deps_checks() -&gt; AutomationCondition:\n        \"\"\"Returns an AutomationCondition which will cause a target\n        to be executed if any of its dependencies update but only after,\n        the dependencies blocking checks have passed, and will\n        execute missing partitions if they become missing after this\n        condition is applied to the target. This will not execute targets\n        that have any missing or in progress dependencies,\n        or are currently in progress.\n\n        For time partitioned assets, only the latest time partition will be considered.\n        Commonly used for assets that are far downstream and have users that directly\n        interact with them, and do not have sensitivity to late arriving dimensions.\n        \"\"\"\n        return (\n            AutomationCondition.eager()\n            &amp; AutomationCondition.all_deps_blocking_checks_passed()\n        )\n\n    @classmethod\n    def lazy(cls) -&gt; AutomationCondition:\n        \"\"\"Returns an AutomationCondition which will cause a target to be executed if\n        any downstream conditions are true or the partition is missing or changed.\n\n        Commonly used for intermediate assets that are used for downstream\n        materializations.\n        \"\"\"\n        return (\n            AutomationCondition.any_downstream_conditions() | cls.missing_or_changed()\n        ).with_label(\"lazy\")\n\n    @staticmethod\n    def lazy_on_cron(\n        cron_schedule: str,\n        cron_timezone: str = \"UTC\",\n        ignore_asset_keys: list[list[str]] | None = None,\n    ) -&gt; AutomationCondition:\n        \"\"\"Returns an AutomationCondition which will cause a target to be\n        executed if any downstream conditions are true or the partition is missing or\n        changed. Will limit to only one execution for the given cron_schedule.\n\n        Commonly used for intermediate assets that are used for downstream\n        materializations, that have high frequency upstream assets, but themselves do\n        not need to be updated as frequently.\n        \"\"\"\n        ignore_asset_keys = ignore_asset_keys or []\n        return (\n            AutomationCondition.in_latest_time_window()\n            &amp; AutomationCondition.cron_tick_passed(\n                cron_schedule, cron_timezone\n            ).since_last_handled()\n            &amp; AutomationCondition.all_deps_updated_since_cron(\n                cron_schedule, cron_timezone\n            ).ignore(AssetSelection.assets(*ignore_asset_keys))\n            &amp; ~AutomationCondition.in_progress()\n        ).with_label(f\"lazy_on_cron({cron_schedule}, {cron_timezone})\")\n\n    @staticmethod\n    @override\n    def on_cron(\n        cron_schedule: str,\n        cron_timezone: str = \"UTC\",\n        ignore_asset_keys: list[list[str]] | None = None,\n    ) -&gt; AndAutomationCondition:\n        \"\"\"Returns an AutomationCondition which will cause a target to be executed on a\n        given cron schedule, after all of its dependencies have been updated since the\n        latest tick of that cron schedule.\n\n        For time partitioned assets, only the latest time partition will be considered.\n\n        Commonly used for assets that are far downstream and have users that directly\n        interact with them, and have sensitivity to late arriving dimensions.\n        \"\"\"\n        ignore_asset_keys = ignore_asset_keys or []\n        return AutomationCondition.on_cron(cron_schedule, cron_timezone).ignore(\n            AssetSelection.assets(*ignore_asset_keys)\n        )\n\n    @staticmethod\n    def on_schedule(\n        cron_schedule: str, cron_timezone: str = \"utc\"\n    ) -&gt; AutomationCondition:\n        \"\"\"Returns an AutomationCondition which will cause a target to be executed on a\n        given cron schedule, regardless of the state of its dependencies\n\n        For time partitioned assets, only the latest time partition will be considered.\n\n        Commonly used for assets in the ingestion layer that should always run on\n        a scheduled basis, and have no way of knowing when the source system has\n        updates.\n        \"\"\"\n        return (\n            AutomationCondition.in_latest_time_window()\n            &amp; AutomationCondition.cron_tick_passed(\n                cron_schedule, cron_timezone\n            ).since_last_handled()\n        ).with_label(f\"on_schedule({cron_schedule}, {cron_timezone})\")\n</code></pre>"},{"location":"reference/utils/automation_conditions/#data_platform.utils.automation_conditions.CustomAutomationCondition.eager","title":"<code>eager()</code>  <code>staticmethod</code>","text":"<p>Returns an AutomationCondition which will cause a target to be executed if any of its dependencies update, and will execute missing partitions if they become missing after this condition is applied to the target. This will not execute targets that have any missing or in progress dependencies, or are currently in progress.</p> <p>For time partitioned assets, only the latest time partition will be considered. Commonly used for assets that are far downstream and have users that directly interact with them, and do not have sensitivity to late arriving dimensions.</p> Source code in <code>data_platform\\utils\\automation_conditions.py</code> <pre><code>@override\n@staticmethod\ndef eager() -&gt; AndAutomationCondition:\n    \"\"\"Returns an AutomationCondition which will cause a target to be executed if\n    any of its dependencies update, and will execute missing partitions if they\n    become missing after this condition is applied to the target. This will not\n    execute targets that have any missing or in progress dependencies, or are\n    currently in progress.\n\n    For time partitioned assets, only the latest time partition will be considered.\n    Commonly used for assets that are far downstream and have users that directly\n    interact with them, and do not have sensitivity to late arriving dimensions.\n    \"\"\"\n    return (\n        AutomationCondition.in_latest_time_window()\n        &amp; (\n            AutomationCondition.newly_missing()\n            | AutomationCondition.any_deps_updated()\n        ).since_last_handled()\n        &amp; ~AutomationCondition.any_deps_missing()\n        &amp; ~AutomationCondition.any_deps_in_progress()\n        &amp; ~AutomationCondition.in_progress()\n    ).with_label(\"eager\")\n</code></pre>"},{"location":"reference/utils/automation_conditions/#data_platform.utils.automation_conditions.CustomAutomationCondition.eager_with_deps_checks","title":"<code>eager_with_deps_checks()</code>  <code>staticmethod</code>","text":"<p>Returns an AutomationCondition which will cause a target to be executed if any of its dependencies update but only after, the dependencies blocking checks have passed, and will execute missing partitions if they become missing after this condition is applied to the target. This will not execute targets that have any missing or in progress dependencies, or are currently in progress.</p> <p>For time partitioned assets, only the latest time partition will be considered. Commonly used for assets that are far downstream and have users that directly interact with them, and do not have sensitivity to late arriving dimensions.</p> Source code in <code>data_platform\\utils\\automation_conditions.py</code> <pre><code>@staticmethod\ndef eager_with_deps_checks() -&gt; AutomationCondition:\n    \"\"\"Returns an AutomationCondition which will cause a target\n    to be executed if any of its dependencies update but only after,\n    the dependencies blocking checks have passed, and will\n    execute missing partitions if they become missing after this\n    condition is applied to the target. This will not execute targets\n    that have any missing or in progress dependencies,\n    or are currently in progress.\n\n    For time partitioned assets, only the latest time partition will be considered.\n    Commonly used for assets that are far downstream and have users that directly\n    interact with them, and do not have sensitivity to late arriving dimensions.\n    \"\"\"\n    return (\n        AutomationCondition.eager()\n        &amp; AutomationCondition.all_deps_blocking_checks_passed()\n    )\n</code></pre>"},{"location":"reference/utils/automation_conditions/#data_platform.utils.automation_conditions.CustomAutomationCondition.lazy","title":"<code>lazy()</code>  <code>classmethod</code>","text":"<p>Returns an AutomationCondition which will cause a target to be executed if any downstream conditions are true or the partition is missing or changed.</p> <p>Commonly used for intermediate assets that are used for downstream materializations.</p> Source code in <code>data_platform\\utils\\automation_conditions.py</code> <pre><code>@classmethod\ndef lazy(cls) -&gt; AutomationCondition:\n    \"\"\"Returns an AutomationCondition which will cause a target to be executed if\n    any downstream conditions are true or the partition is missing or changed.\n\n    Commonly used for intermediate assets that are used for downstream\n    materializations.\n    \"\"\"\n    return (\n        AutomationCondition.any_downstream_conditions() | cls.missing_or_changed()\n    ).with_label(\"lazy\")\n</code></pre>"},{"location":"reference/utils/automation_conditions/#data_platform.utils.automation_conditions.CustomAutomationCondition.lazy_on_cron","title":"<code>lazy_on_cron(cron_schedule, cron_timezone='UTC', ignore_asset_keys=None)</code>  <code>staticmethod</code>","text":"<p>Returns an AutomationCondition which will cause a target to be executed if any downstream conditions are true or the partition is missing or changed. Will limit to only one execution for the given cron_schedule.</p> <p>Commonly used for intermediate assets that are used for downstream materializations, that have high frequency upstream assets, but themselves do not need to be updated as frequently.</p> Source code in <code>data_platform\\utils\\automation_conditions.py</code> <pre><code>@staticmethod\ndef lazy_on_cron(\n    cron_schedule: str,\n    cron_timezone: str = \"UTC\",\n    ignore_asset_keys: list[list[str]] | None = None,\n) -&gt; AutomationCondition:\n    \"\"\"Returns an AutomationCondition which will cause a target to be\n    executed if any downstream conditions are true or the partition is missing or\n    changed. Will limit to only one execution for the given cron_schedule.\n\n    Commonly used for intermediate assets that are used for downstream\n    materializations, that have high frequency upstream assets, but themselves do\n    not need to be updated as frequently.\n    \"\"\"\n    ignore_asset_keys = ignore_asset_keys or []\n    return (\n        AutomationCondition.in_latest_time_window()\n        &amp; AutomationCondition.cron_tick_passed(\n            cron_schedule, cron_timezone\n        ).since_last_handled()\n        &amp; AutomationCondition.all_deps_updated_since_cron(\n            cron_schedule, cron_timezone\n        ).ignore(AssetSelection.assets(*ignore_asset_keys))\n        &amp; ~AutomationCondition.in_progress()\n    ).with_label(f\"lazy_on_cron({cron_schedule}, {cron_timezone})\")\n</code></pre>"},{"location":"reference/utils/automation_conditions/#data_platform.utils.automation_conditions.CustomAutomationCondition.manual","title":"<code>manual()</code>  <code>staticmethod</code>","text":"<p>Returns no AutomationCondition that will require a user to manually trigger. Used for overriding default automations for static assets.</p> Source code in <code>data_platform\\utils\\automation_conditions.py</code> <pre><code>@staticmethod\ndef manual() -&gt; None:\n    \"\"\"Returns no AutomationCondition that will require a user to manually trigger.\n    Used for overriding default automations for static assets.\n    \"\"\"\n    return None\n</code></pre>"},{"location":"reference/utils/automation_conditions/#data_platform.utils.automation_conditions.CustomAutomationCondition.missing_or_changed","title":"<code>missing_or_changed()</code>  <code>staticmethod</code>","text":"<p>Returns no AutomationCondition that will trigger only if the asset has never been materialized, or if its definition has changed.</p> <p>Common use for dbt seeds that only need to be reloaded when the underlying csv file changes.</p> Source code in <code>data_platform\\utils\\automation_conditions.py</code> <pre><code>@staticmethod\ndef missing_or_changed() -&gt; AutomationCondition:\n    \"\"\"Returns no AutomationCondition that will trigger only if the asset has never\n    been materialized, or if its definition has changed.\n\n    Common use for dbt seeds that only need to be reloaded when the underlying csv\n    file changes.\n    \"\"\"\n    return (\n        AutomationCondition.in_latest_time_window()\n        &amp; (\n            AutomationCondition.code_version_changed()\n            | AutomationCondition.newly_missing()\n        ).since_last_handled()\n        &amp; ~AutomationCondition.in_progress()\n    ).with_label(\"missing_or_changed\")\n</code></pre>"},{"location":"reference/utils/automation_conditions/#data_platform.utils.automation_conditions.CustomAutomationCondition.on_cron","title":"<code>on_cron(cron_schedule, cron_timezone='UTC', ignore_asset_keys=None)</code>  <code>staticmethod</code>","text":"<p>Returns an AutomationCondition which will cause a target to be executed on a given cron schedule, after all of its dependencies have been updated since the latest tick of that cron schedule.</p> <p>For time partitioned assets, only the latest time partition will be considered.</p> <p>Commonly used for assets that are far downstream and have users that directly interact with them, and have sensitivity to late arriving dimensions.</p> Source code in <code>data_platform\\utils\\automation_conditions.py</code> <pre><code>@staticmethod\n@override\ndef on_cron(\n    cron_schedule: str,\n    cron_timezone: str = \"UTC\",\n    ignore_asset_keys: list[list[str]] | None = None,\n) -&gt; AndAutomationCondition:\n    \"\"\"Returns an AutomationCondition which will cause a target to be executed on a\n    given cron schedule, after all of its dependencies have been updated since the\n    latest tick of that cron schedule.\n\n    For time partitioned assets, only the latest time partition will be considered.\n\n    Commonly used for assets that are far downstream and have users that directly\n    interact with them, and have sensitivity to late arriving dimensions.\n    \"\"\"\n    ignore_asset_keys = ignore_asset_keys or []\n    return AutomationCondition.on_cron(cron_schedule, cron_timezone).ignore(\n        AssetSelection.assets(*ignore_asset_keys)\n    )\n</code></pre>"},{"location":"reference/utils/automation_conditions/#data_platform.utils.automation_conditions.CustomAutomationCondition.on_schedule","title":"<code>on_schedule(cron_schedule, cron_timezone='utc')</code>  <code>staticmethod</code>","text":"<p>Returns an AutomationCondition which will cause a target to be executed on a given cron schedule, regardless of the state of its dependencies</p> <p>For time partitioned assets, only the latest time partition will be considered.</p> <p>Commonly used for assets in the ingestion layer that should always run on a scheduled basis, and have no way of knowing when the source system has updates.</p> Source code in <code>data_platform\\utils\\automation_conditions.py</code> <pre><code>@staticmethod\ndef on_schedule(\n    cron_schedule: str, cron_timezone: str = \"utc\"\n) -&gt; AutomationCondition:\n    \"\"\"Returns an AutomationCondition which will cause a target to be executed on a\n    given cron schedule, regardless of the state of its dependencies\n\n    For time partitioned assets, only the latest time partition will be considered.\n\n    Commonly used for assets in the ingestion layer that should always run on\n    a scheduled basis, and have no way of knowing when the source system has\n    updates.\n    \"\"\"\n    return (\n        AutomationCondition.in_latest_time_window()\n        &amp; AutomationCondition.cron_tick_passed(\n            cron_schedule, cron_timezone\n        ).since_last_handled()\n    ).with_label(f\"on_schedule({cron_schedule}, {cron_timezone})\")\n</code></pre>"},{"location":"reference/utils/helpers/","title":"Helpers","text":""},{"location":"reference/utils/helpers/#data_platform.utils.helpers.get_automation_condition_from_meta","title":"<code>get_automation_condition_from_meta(meta)</code>","text":"<p>Return an AutomationCondition if valid configuartion is provided in the meta. Meta should be of format dict in the following structure: .. code-block:: python     \"meta\":{         \"dagster\":{             \"automation_condition\": condition,             \"automation_condition_config\": {argument: value}         }     }</p> Source code in <code>data_platform\\utils\\helpers.py</code> <pre><code>def get_automation_condition_from_meta(\n    meta: dict[str, Any],\n) -&gt; dg.AutomationCondition | None:\n    \"\"\"Return an AutomationCondition if valid configuartion is provided in the meta.\n    Meta should be of format dict in the following structure:\n    .. code-block:: python\n        \"meta\":{\n            \"dagster\":{\n                \"automation_condition\": condition,\n                \"automation_condition_config\": {argument: value}\n            }\n        }\n    \"\"\"\n    condition_name = meta.get(\"automation_condition\")\n    if not condition_name:\n        return None\n\n    condition = CustomAutomationCondition.get_automation_condition(condition_name)\n    if not isinstance(condition, Callable):\n        raise KeyError(f\"Automation condition not found for key '{condition_name}'\")\n\n    condition_config = meta.get(\"automation_condition_config\", {}) or {}\n    if not isinstance(condition_config, dict):\n        raise ValueError(f\"Invalid condition config: '{condition_config}'\")\n\n    condition_config = sanitize_input_signature(condition, condition_config)\n    try:\n        return condition(**condition_config)\n    except Exception as e:\n        e.add_note(\n            \"'condition_config' is missing required keys\"\n            f\"for condition '{condition_name}'\"\n        )\n        raise\n</code></pre>"},{"location":"reference/utils/helpers/#data_platform.utils.helpers.get_nested","title":"<code>get_nested(config, path)</code>","text":"<p>Helper function to safely traverse a nested dictionary that may have null values for a set key that is expected to be a dict. helpful because stream definitions that use only the default configs behave this way. .. code-block:: yaml streams:     source.table_one:     source.table_two:</p> Source code in <code>data_platform\\utils\\helpers.py</code> <pre><code>def get_nested(config: dict, path: list) -&gt; Any:\n    \"\"\"Helper function to safely traverse a nested dictionary that may have null values\n    for a set key that is expected to be a dict. helpful because stream definitions that\n    use only the default configs behave this way.\n    .. code-block:: yaml\n    streams:\n        source.table_one:\n        source.table_two:\n    \"\"\"\n    try:\n        for item in path:\n            config = config[item]\n        return config\n    except Exception:\n        ...\n    return None\n</code></pre>"},{"location":"reference/utils/helpers/#data_platform.utils.helpers.get_partitions_def_from_meta","title":"<code>get_partitions_def_from_meta(meta)</code>","text":"<p>Return an TimeWindowPartitionsDefinition if valid configuartion is provided in the meta. - partition accepts the values: hourly, daily, weekly, monthly. - partition_start_date should be a iso format date, or timestamp.</p> <p>Meta should be of format dict in the following structure: .. code-block:: python    \"meta\":{        \"dagster\":{            \"partition\": \"daily\",            \"partition_start_date\": \"2025-01-01\"        }    }</p> Source code in <code>data_platform\\utils\\helpers.py</code> <pre><code>def get_partitions_def_from_meta(\n    meta: dict[str, Any],\n) -&gt; dg.TimeWindowPartitionsDefinition | None:\n    \"\"\"Return an TimeWindowPartitionsDefinition if valid configuartion is provided in\n    the meta.\n    - partition accepts the values: hourly, daily, weekly, monthly.\n    - partition_start_date should be a iso format date, or timestamp.\n\n    Meta should be of format dict in the following structure:\n    .. code-block:: python\n       \"meta\":{\n           \"dagster\":{\n               \"partition\": \"daily\",\n               \"partition_start_date\": \"2025-01-01\"\n           }\n       }\n    \"\"\"\n    try:\n        partition = meta.get(\"partition\")\n        partition_start_date = meta.get(\"partition_start_date\")\n\n        if partition and partition_start_date:\n            start_date = datetime.fromisoformat(partition_start_date)\n            if partition == \"hourly\":\n                return dg.HourlyPartitionsDefinition(\n                    start_date=start_date.strftime(\"%Y-%m-%d-%H:%M\")\n                )\n            if partition == \"daily\":\n                return dg.DailyPartitionsDefinition(\n                    start_date=start_date.strftime(\"%Y-%m-%d\")\n                )\n            if partition == \"weekly\":\n                return dg.WeeklyPartitionsDefinition(\n                    start_date=start_date.strftime(\"%Y-%m-%d\")\n                )\n            if partition == \"monthly\":\n                return dg.MonthlyPartitionsDefinition(\n                    start_date=start_date.strftime(\"%Y-%m-%d\")\n                )\n    except Exception:\n        ...\n    return None\n</code></pre>"},{"location":"reference/utils/helpers/#data_platform.utils.helpers.sanitize_input_signature","title":"<code>sanitize_input_signature(func, kwargs)</code>","text":"<p>Remove any arguments that are not expected by the recieving function.</p> Source code in <code>data_platform\\utils\\helpers.py</code> <pre><code>def sanitize_input_signature(func: Callable, kwargs: dict) -&gt; dict:\n    \"\"\"Remove any arguments that are not expected by the recieving function.\"\"\"\n    sig = signature(func)\n    key_words = list(kwargs.keys())\n    expected_arguments = {argument for argument, _ in sig.parameters.items()}\n\n    for argument in key_words:\n        if argument not in expected_arguments:\n            kwargs.pop(argument)\n\n    return kwargs\n</code></pre>"},{"location":"reference/utils/keyvault_stub/","title":"Keyvault stub","text":""},{"location":"reference/utils/keyvault_stub/#data_platform.utils.keyvault_stub.SecretClient","title":"<code>SecretClient</code>","text":"<p>A stub keyvault to simulate an integration with Azure Keyvault. This would be replaced by a keyvault library.</p> Source code in <code>data_platform\\utils\\keyvault_stub.py</code> <pre><code>class SecretClient:\n    \"\"\"A stub keyvault to simulate an integration with Azure Keyvault. This would be\n    replaced by a keyvault library.\n    \"\"\"\n\n    def get_secret(self, secret_name: str) -&gt; str:\n        \"\"\"returns a secret from the keyvault\"\"\"\n        secrets = self.__secrets\n        location, _, attribute = secret_name.split(\"__\")\n        secret = secrets.get(location, {}).get(attribute)\n\n        return secret or \"\"\n\n    def __init__(\n        self, vault_url: str | None = None, credential: str | None = None\n    ) -&gt; None:\n        secrets = {\"SOURCE\": {}, \"DESTINATION\": {}}\n\n        env_path = Path(__file__).joinpath(*[\"..\"] * 3, \".env\").resolve()\n        set_env = os.getenv(\"TARGET\", \"\")\n        with open(env_path) as env:\n            for line in env:\n                line = line.strip()\n                if line.startswith(set_env.upper()) or line.startswith(\"ANY\"):\n                    key, secret = line.split(\"=\")\n                    env, location, attribute = key.split(\"__\")\n                    secrets[location][attribute] = secret\n\n        self.__secrets = secrets\n</code></pre>"},{"location":"reference/utils/keyvault_stub/#data_platform.utils.keyvault_stub.SecretClient.get_secret","title":"<code>get_secret(secret_name)</code>","text":"<p>returns a secret from the keyvault</p> Source code in <code>data_platform\\utils\\keyvault_stub.py</code> <pre><code>def get_secret(self, secret_name: str) -&gt; str:\n    \"\"\"returns a secret from the keyvault\"\"\"\n    secrets = self.__secrets\n    location, _, attribute = secret_name.split(\"__\")\n    secret = secrets.get(location, {}).get(attribute)\n\n    return secret or \"\"\n</code></pre>"},{"location":"reference/utils/secrets/","title":"Secrets","text":""},{"location":"reference/utils/secrets/#data_platform.utils.secrets.get_secret","title":"<code>get_secret(env_var_name)</code>","text":"<p>A wrapper for a keyvault to integrate with the Dagster EnvVar class.</p> <p>Returns a secret from the keyvault and set it to an environment variable that can be used securly with dagsters EnvVar class.</p> Source code in <code>data_platform\\utils\\secrets.py</code> <pre><code>def get_secret(env_var_name: str) -&gt; dg.EnvVar:\n    \"\"\"A wrapper for a keyvault to integrate with the Dagster EnvVar class.\n\n    Returns a secret from the keyvault and set it to an environment variable that can be\n    used securly with dagsters EnvVar class.\n    \"\"\"\n    if secret := keyvault.get_secret(env_var_name):\n        os.environ[env_var_name] = secret\n        return dg.EnvVar(env_var_name)\n\n    raise ValueError(\n        f\"Secret for key '{env_var_name}' not found.\"\n        \"Please check that this is the correct key.\"\n    )\n</code></pre>"}]}