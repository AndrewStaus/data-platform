{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":"<p>A Dagster project integrating dbt, Sling, dltHub, and Snowflake into a single data platform. Includes stubs for powerBi, and AzureML, as well as Azure Keyvault to demonstrate external integrations.</p> <pre><code>---\nconfig:\n  theme: neutral\n---\nflowchart LR\n  subgraph s1[\"sling dlt\"]\n        n1[\"src\"]\n        n2[\"raw\"]\n  end\n  subgraph s2[\"dbt\"]\n        n3[\"pii\"]\n        n4[\"stg\"]\n        n5[\"snp\"]\n        n6[\"int\"]\n        n7[\"mrt\"]\n  end\n  subgraph s3[\"powerbi\"]\n        n8[\"sem\"]\n        n9[\"exp\"]\n  end\n  subgraph s4[\"snowpark\"]\n        n10[\"ml\"]\n  end\n  n1 --&gt; n2\n  n2 --&gt; n3\n  n2 --&gt; n4\n  n2 --&gt; n5\n  n5 --&gt; n6\n  n2 --&gt; n6\n  n6 --&gt; n7\n  n7 --&gt; n8\n  n8 --&gt; n9\n  n7 --&gt; n10\n  n6 --&gt; n10\n\n</code></pre>"},{"location":"#getting-started","title":"Getting Started","text":""},{"location":"#note-on-security","title":"Note on Security","text":"<p>This is a simple configuration for demonstration.  Some security best practices have been skipped in order to make demonstration easier, however oath is supported for a real deployment to authenticate developers against the resources their accessing.</p>"},{"location":"#uv","title":"uv","text":"<p>This project is configured using the uv python package manager.  To initialize the project, install uv if you do not have it already, and enter the command:</p> <pre><code>uv sync\n</code></pre> <p>This will create a virtual environment and install all required dependacies.</p>"},{"location":"#resources","title":"Resources","text":""},{"location":"#source-database","title":"Source Database","text":"<p>This demo has been set up using a postgres server with a handful of tables in a single schema.</p>"},{"location":"#desination-warehouse","title":"Desination Warehouse","text":"<p>This demo is using a Snowflake warehouse to demonstrate. It assumes the existance of the following databases:</p>"},{"location":"#prod","title":"prod","text":""},{"location":"#analytics","title":"analytics","text":"<p>This is where the silver layer data will be stored: staged (stg), incremental (inc), dimensions (dim), facts (fct).  This is the main destination for dbt models.  </p>"},{"location":"#raw","title":"raw","text":"<p>The database that stores data ingested from the source systems.</p>"},{"location":"#snapshots","title":"snapshots","text":"<p>SCD Type 2 data, that is primaraly captured through dbt eagerly on raw data.</p>"},{"location":"#qa","title":"qa","text":"<p>The QA environment is not configured for this demonstration, however in a real deployment this would exist to perform slim CI on pull requests from feature branchs to develop, and pull requests from develop to main. </p>"},{"location":"#dev","title":"dev","text":"<p>The dev databases mirror the production, however each developer will generate shcmeas tagged with their user name so that they can develop new assets in isolation.</p>"},{"location":"#_dev_analytics","title":"_dev_analytics","text":""},{"location":"#_dev_raw","title":"_dev_raw","text":""},{"location":"#_dev_snapshots","title":"_dev_snapshots","text":""},{"location":"#env-file","title":".env File","text":"<p>The .env file will hold your environment variables. Create this file in your root project folder and populate it will the correct credentials for your database and warehouse.</p> <p>In a true deployment, this would be set up using more secure methods.</p> <pre><code># .env\nTARGET=dev\nDAGSTER_HOME=.\\\\.dagster_home\nPYTHONLEGACYWINDOWSSTDIO=1\nDBT_PROJECT_DIR=.\\\\dbt\\\\\nPREPARE_IF_DEV=1\n\nPROD__DESTINATION__DATABASE=raw\nPROD__DESTINATION__HOST=&lt;your_hostname&gt;\nPROD__DESTINATION__ROLE=&lt;role_with_create_grants_on_prod&gt;\nPROD__DESTINATION__USER=&lt;user_with_create_grants_on_prod&gt;\nPROD__DESTINATION__PASSWORD=&lt;user_password&gt;\nPROD__DESTINATION__WAREHOUSE=&lt;warehouse&gt;\n\nDEV__DESTINATION__DATABASE=raw\nDEV__DESTINATION__HOST=&lt;your_hostname&gt;\nDEV__DESTINATION__ROLE=&lt;role_with_create_grants_on_dev&gt;\nDEV__DESTINATION__USER=&lt;user_with_create_grants_on_dev&gt;\nDEV__DESTINATION__PASSWORD=&lt;user_password&gt;\nDEV__DESTINATION__WAREHOUSE=&lt;warehouse&gt;\n\nANY__SOURCE__DATABASE=&lt;demo_database&gt;\nANY__SOURCE__HOST=&lt;demo_host&gt;\nANY__SOURCE__PORT=&lt;demo_port&gt;\nANY__SOURCE__USER=&lt;demo_user&gt;\nANY__SOURCE__PASSWORD=&lt;demo_password&gt;\n</code></pre> <p>PROD configuration is used on the production deployment, and the credentials should have appropriate grants for the prod databases on the warehouse.</p> <p>DEV configuration is used in the local development environment, and should have select grants to the analytics databases, and create schema grants on the development databases. </p> <p>ANY configuration is shared between dev and prod, and will be used to connect to the demo database.</p>"},{"location":"#deployment","title":"Deployment","text":""},{"location":"#local-development","title":"Local Development","text":"<p>Once the above setup steps are complete, you are ready to launch you local development server.  A task has been defined in vs code to make running the server easy.</p> <ol> <li>Open the command pallet: <code>ctrl+shift+p</code></li> <li>Type <code>&gt;Tasks: Run Task</code> and hit enter</li> <li>Type <code>dagster dev</code> and hit enter</li> </ol> <p>You should now have a dagster development server running on your local machine.</p>"},{"location":"#production-deployment","title":"Production Deployment","text":"<p>To simulate a production deployment a <code>build and deploy</code> task has also been created. This task assumes that you have docker deskop installed and running, with a 'kind' kubernettes cluster availible.</p> <p>If so you can run this command to build the 'user code deployment' docker image and deploy it to the cluster.  Once deployed you can run the <code>k8s port forward</code> to make the web server availible at <code>http://127.0.0.1:63446/</code></p>"},{"location":"#useful-vs-code-extensions","title":"Useful VS Code Extensions","text":""},{"location":"#python","title":"Python","text":"<p>Core python extension from microsoft.  This will provide syntax highlighting for .py files.</p>"},{"location":"#ruff","title":"Ruff","text":"<p>A python linter that will help conform to style guides for the project.  The styles are enforced through settings in the pyproject file so that all contributers write high quality, standardized code.</p>"},{"location":"#power-user-for-dbt","title":"Power User for dbt","text":"<p>Allows for advanced functionality of dbt assets while the official dbt extension becomes availibe in general release.</p>"},{"location":"#even-better-toml","title":"Even Better TOML","text":"<p>Provides syntax highlighting for TOML files which are used for sling configurations.</p>"},{"location":"#cron-explained","title":"Cron Explained","text":"<p>When hovering over cron expressions, provides a plain english explanation of what the expression means.</p>"},{"location":"#snowflake","title":"Snowflake","text":"<p>Allows for viewing snowflake resources, and performing SQL, DML, and DDL against the warehouse.</p>"},{"location":"dlthub/","title":"dltHub","text":"<p>https://dlthub.com/ docs</p> <p>dlt is the most popular production-ready Python library for moving data. It loads data from various and often messy data sources into well-structured, live datasets.</p> <p>Unlike other non-Python solutions, with dlt, there's no need to use any backends or containers. We do not replace your data platform, deployments, or security models. Simply import dlt in your favorite AI code editor, or add it to your Jupyter Notebook. You can load data from any source that produces Python data structures, including APIs, files, databases, and more.</p>"},{"location":"dlthub/#structure","title":"Structure","text":"<pre><code>---\nconfig:\n  theme: neutral\n---\nflowchart LR\n subgraph s1[\"definitions\"]\n        n4[\"assets_definition\"]\n        n5[\"resource\"]\n  end\n    n1[\"dlt_source.py\"] --&gt; n3[\"factory\"]\n    n2[\"translator\"] --&gt; n3\n    n3 --&gt; n4\n    n5 --&gt; n7[\"run\"]\n    n4 --&gt; n7\n    n6[\"context\"] --&gt; n7\n    n4@{ shape: doc}\n    n5@{ shape: proc}\n    n1@{ shape: docs}\n    n3@{ shape: procs}\n    n2@{ shape: rect}\n    n7@{ shape: proc}\n    n6@{ shape: proc}\n</code></pre>"},{"location":"dlthub/#factory","title":"Factory","text":"<p>The factory will parse user defined python scripts into dagster resources and assets.</p>"},{"location":"dlthub/#translator","title":"Translator","text":"<p>The translator will tell dagster how to translate dltHub concepts into dagster concepts, such as how a asset key is defined, or a automation condition.</p>"},{"location":"dlthub/#resources","title":"Resources","text":"<p>The resources will pass all the translated assets to the dagster runtime.</p>"},{"location":"dlthub/#artifacts","title":"Artifacts","text":""},{"location":"dlthub/#definitionspy","title":"definitions.py","text":"<p>The reources file is what is ingested through the factory to create the dltHub assets in dagster.  The user will provide a lists of resources, typically one for each endpoint that will materialize as an asset in dagster.</p> <pre><code># definitions.py\nfrom .data import api_generator\n...\nConfigurableDltResource.config(\n    dlt.resource(\n        # the generator you defined in the data.py file\n        api_generator,\n        # the schema and table to materialize on the \n        name=\"schema.table\", warehouse \n        table_name=\"table\",\n        primary_key=\"id\", # the primary key column\n        write_disposition=\"merge\", # how to incrementally load\n    ),\n    kinds={\"api\"},\n    # aditional dagster configuration for orchestration and checks\n    meta={\n        \"dagster\": { \n            \"automation_condition\": \"on_schedule\", \n            \"automation_condition_config\": {\n                \"cron_schedule\": \"@daily\",\n                \"cron_timezone\": \"utc\",\n            },\n            \"freshness_lower_bound_delta_seconds\": 108000\n        }\n    },\n)\n...\n</code></pre>"},{"location":"dlthub/#datapy","title":"data.py","text":"<p>The code to generate data, this will be imported into the definitions.py module.  dltHub can accept any arbitrary code as long as it yields a python data object.  Suported  formats include avro data frames, json in the form of python dictonaries</p> <pre><code># data.py\nfrom collections.abc import Callable, Generator\nfrom typing import Any\n\nimport requests\n\ndef api_generator() -&gt; Generator[Any, Any, None]:\n    uri = \"https://www.api.com/endpoint\"\n    response = requests.get(uri)\n    yield response.json()\n    while next_uri := response.json().get(\"next_page\"):\n        response = requests.get(next_uri)\n        yield response.json()\n</code></pre> <p>A common design pattern for api's with multiple endpoints is to use a factory function that will return a different generator for different enpoints.</p> <pre><code># data.py\nfrom collections.abc import Callable, Generator\nfrom typing import Any\n\nimport requests\n\ndef get_api_generator(endpoint: str) -&gt; Callable[[], Any]:\n    base_uri = \"https://www.api.com/\"\n\n    def api_generator() -&gt; Generator[Any, Any, None]:\n        response = requests.get(base_uri+endpoint)\n        yield response.json()\n        while next_uri := response.json().get(\"next_page\"):\n            response = requests.get(next_uri)\n            yield response.json()\n\n    return api_generator\n</code></pre> <p>This can then be reused in the resources.py module</p> <pre><code># definitions.py\nfrom .data import get_api_generator\n...\nConfigurableDltResource.config(\n    dlt.resource(\n        get_api_generator(\"endpoint_one\"),\n        name=\"schema.table_one\",\n        table_name=\"table_one\",\n        primary_key=\"id\",\n        write_disposition=\"merge\",\n    ),\n    kinds={\"api\"}\n),\nConfigurableDltResource.config(\n    dlt.resource(\n        get_api_generator(\"endpoint_two\"),\n        name=\"schema.table_two\",\n        table_name=\"table_two\",\n        primary_key=\"id\",\n        write_disposition=\"merge\",\n    ),\n    kinds={\"api\"}\n),\n...\n</code></pre>"},{"location":"dlthub/#other-dlthub-concepts","title":"Other dltHub concepts","text":"<p>On its own dltHub has other concepts that you may see in their documentation such as pipelines, desinations, state, schema, however these have been abstracted away in the data platform, so all a developer needs to focus on is creating a generator, and defining it as a dagster asset in the definitions.py file.</p>"},{"location":"sling/","title":"Sling","text":"<p>https://slingdata.io/ https://github.com/slingdata-io/sling-cli</p> <p>Sling is a Powerful Data Integration tool enabling seamless ELT operations as well as quality checks across files, databases, and storage systems.</p>"},{"location":"sling/#structure","title":"Structure","text":"<pre><code>---\nconfig:\n  theme: neutral\n---\nflowchart LR\n subgraph s1[\"definitions\"]\n        n4[\"assets_definition\"]\n        n5[\"resource\"]\n  end\n    n1[\"replication_config.yaml\"] --&gt; n3[\"factory\"]\n    n2[\"translator\"] --&gt; n3\n    n3 --&gt; n4\n    n5 --&gt; n7[\"run\"]\n    n4 --&gt; n7\n    n6[\"context\"] --&gt; n7\n    n4@{ shape: doc}\n    n5@{ shape: proc}\n    n1@{ shape: docs}\n    n3@{ shape: procs}\n    n2@{ shape: rect}\n    n7@{ shape: proc}\n    n6@{ shape: proc}\n</code></pre>"},{"location":"sling/#factory","title":"Factory","text":"<p>The factory will parse user defined yaml files representing connections and streams into dagster resources and assets.</p>"},{"location":"sling/#translator","title":"Translator","text":"<p>The translator will tell dagster how to translate sling concepts into dagster concepts, such as how a asset key is defined, or a automation condition.</p>"},{"location":"sling/#resources","title":"Resources","text":"<p>The resources will pass all the translated assets to the dagster runtime.</p>"},{"location":"sling/#configs","title":"Configs","text":""},{"location":"sling/#connection-config","title":"Connection Config","text":"<p>https://docs.slingdata.io/sling-cli/environment#sling-env-file-env.yaml</p> <p>The connection config defines the database, or file system connection details so that it can be used as a source or desintation on replications.  It follows the similar patterns as the sling env file, however additional functionality iis applied for secret management through the keyvault.</p> <p>For values which are stored in the key vault, you can specify the key name, and if you wish for the key to be shown in plain text on the dagster front end, or if it should be securly masked.</p> <pre><code>connections:\n  -name: database_name\n   type: oracle\n   database: {SOURCE__ON_PREM_OLTP__CREDENTIALS__DATABASE: show}\n   host: {SOURCE__ON_PREM_OLTP__CREDENTIALS__HOST: show}\n   port: {SOURCE__ON_PREM_OLTP__CREDENTIALS__PORT: mask}\n   user: {SOURCE__ON_PREM_OLTP__CREDENTIALS__USER: mask}\n   password: {SOURCE__ON_PREM_OLTP__CREDENTIALS__PASSWORD: mask}\n</code></pre>"},{"location":"sling/#replication-config","title":"Replication Config","text":"<p>https://docs.slingdata.io/concepts/replication Replications are how extract and loads are defined.  Source name, and target names reference connections you have defined in the connections section.  The connection does not need to be defined in the yaml file you are referencing it in.  Typically there will be a single yaml file for each connection, with replications showing egress from that connection to another system.</p> <pre><code>replications:\n  - name: source_name-&gt;desitnation_name\n    env: {SLING_LOADED_AT_COLUMN: timestamp}\n    source: source_name\n    target: target_name\n    defaults:\n      mode: incremental\n      object: '{stream_schema_upper}.{stream_table_upper}'\n      primary_key: [id]\n      update_key: updated_at\n      target_options:\n        column_casing: snake\n        add_new_columns: true\n        adjust_column_type: true\n      meta:\n        dagster:\n          automation_condition: \"on_cron_no_deps\"\n          automation_condition_config: {\"cron_schedule\":\"@daily\", \"cron_timezone\":\"utc\"}\n          freshness_lower_bound_delta: 1800\n    streams:\n      source_schema.table_one:\n        tags: ['contains_pii']\n      source_schema.table_two:\n        primary_key: [pk_column]\n\n</code></pre> <p>Streams are the tables, or files to transfer, settings from the default section are applied to all streams, unless specific configuartion is applied to that stream in which case the stream config takes precedence.</p>"},{"location":"dbt/dbt_docs_site/","title":"dbt Docs Site","text":"click here if you are not redirected."},{"location":"dbt/getting_started/","title":"Getting Started","text":""},{"location":"dbt/getting_started/#what-is-dbt","title":"What is dbt","text":"<p>dbt (data build tool) is an open-source tool that empowers data analysts and engineers to transform data within their data warehouses by writing and executing SQL code. It focuses on the \"transform\" part of the ETL (Extract, Load, Transform) process, allowing users to define, test, and deploy data transformations using SQL. Essentially, dbt enables teams to build and manage their data models in a collaborative, version-controlled, and testable way, ensuring data quality and reliability.</p>"},{"location":"dbt/getting_started/#dbt-learning-material","title":"dbt Learning Material","text":"<p>dbt Fundamentals dbt Certified Developer</p>"},{"location":"dbt/getting_started/#contributing","title":"Contributing","text":"<p>Create a feature branch based off the <code>develop</code> branch. <code>feat/feature_name</code> Your branch name should be all lowercase, terse, and descriptive: <code>feat/adding_model_dim_product</code></p> <p>Once your changes are complete, submit a pull request to merge the changes into the develop branch, where it will be reviewed by your peers, and feedback will be provided.  Once approved it will be merged into the develop branch, and will be put into production on the next deployment.</p>"},{"location":"dbt/getting_started/#project-structure","title":"Project Structure","text":""},{"location":"dbt/getting_started/#naming-conventions","title":"Naming Conventions","text":"<p><code>type</code>_<code>schema</code>__<code>model_name</code>.sql</p> <p>Proper naming of models is important, as the file names are used to determine the schema and table names that are used to materialize the model on the warehouse.  It is also used to understand upstream and downstream dependencies within the orchestration tool.</p>"},{"location":"dbt/getting_started/#types","title":"Types","text":"<ul> <li>stg: Staged tables, that have had light transformations applied to be standardized in the warehouse, but generally are very close to the raw data.</li> <li>int:  Intermediate models that are used in service to apply common transformations that will be used by dims and facts.</li> <li>dim: Dimension models.</li> <li>fct: Fact models which provide measures.</li> </ul>"},{"location":"dbt/getting_started/#models","title":"Models","text":"<p>Models are sql or python code used to define a table to be materialized on the warehouse.  By defining a model using a select statement, dbt will handel the dml and ddl needed to create, and refresh the model.</p> <p>Models are organized into three folders: <code>staging</code>, <code>intermediate</code>, <code>marts</code>, as well as  two special kinds of materializations called <code>snapshots</code> and <code>seeds</code>.</p> <pre><code>---\nconfig:\n  theme: neutral\n---\ngraph LR\n  n1[\"raw\"]--&gt;n2[\"pii\"]\n  n1--&gt;n3[\"stg\"]\n  n1--&gt;snp\n  n3--&gt;n4[\"int\"]\n  n4--&gt;n5[\"mrt\"]\n</code></pre>"},{"location":"dbt/getting_started/#staging","title":"staging","text":"<p>Staging is a silver layer where raw data is made ready for general use.</p> <p>Models are organized by the source system they originate from.  Standardization happens at this layer: - All timestamp columns are converted to utc - Columns are renamed to be more explicit in what they represent, for example, a column named 'id' would be renamed to 'product_id' so it is easy to understand what it represents downstream. - data cleaning is applied, such as deduplication, or filtering of invalid records, for example, bot traffic is removed from website click data. - conversions are applied to standardize, for example if a source uses integers to represent monetary values, it should be converted to a decimal type. - PII data is anonymized / pseudo-anonymized and split off from non-pii data.  Vertical partitioning. - Light normalization and denormalization where it makes sense for example pre-joining tables with one to one cardinality, or converting a business key to a more usable key.</p> <p>Raw tables are also defined in the staging zone through a <code>__sources.yml</code> file, however they will be materialized upstream of dbt.</p>"},{"location":"dbt/getting_started/#intermediate","title":"intermediate","text":"<p>Intermediate is the layer between staging and marts, where common data transformations are applied so that code can be reused by multiple downstream tables.</p> <p>Models are organized by the primary mart that they intend to supply data to.  Models with general use cases across marts are put into a common grouping.</p>"},{"location":"dbt/getting_started/#marts","title":"marts","text":"<p>Marts are the gold layer where models are groups by business unit.  These models are primarily in service of a semantic layer, and general BI.  Data modeling best practices are followed (dimensional, cube, data vault, kimball, etc.) so that they can be queried easily and efficiently by business users.</p> <p>Models are organized by the business domain in which they serve.</p>"},{"location":"dbt/getting_started/#snapshots","title":"snapshots","text":"<p>Snapshots are used to capture the state of a table at a point in time.  Each time a snapshot is run, any records that have changed will have a new record inserted, the old version of the record will be marked as inactive, and will have a time period of when it was active from, and to.  This is commonly referred to as Slowly Changing Dimension Type Two (SCD2).</p>"},{"location":"dbt/getting_started/#seeds","title":"seeds","text":"<p>Seeds are CSV files that will be materialized as tables on the warehouse.  These are used for small datasets that do not contain sensitive information, and do not change often.  For example, a reference table maintained by the business that would not warrant a heavy pipeline to maintain.</p>"},{"location":"dbt/getting_started/#macros","title":"Macros","text":"<p>DBT utilizes jinja macros to programmatically inject values into your sql at run time.  By using macros you can apply powerful programmatic patterns that are not otherwise possible in normal sql.</p>"},{"location":"dbt/getting_started/#dagster-specific-config","title":"Dagster Specific Config","text":"<p>A meta key has been introduced to extend the functionality of dbt in dagster.  It is used to define automation's, checks, and partitions for finer control on how and when assets are materialized.</p> <p>Example config block: </p> <pre><code>{{-\n  config(\n    materialized = \"incremental\",\n    incremental_strategy = \"delete+insert\",\n    unique_key = \"id\",\n    meta = {\n      \"dagster\":{\n        \"partition\": \"daily\",\n        \"partition_start_date\": \"2025-07-01\",\n        \"automation_condition\": \"on_cron_no_deps\",\n        \"automation_condition_config\": {\n          \"cron_schedule\":\"@daily\",\n          \"cron_timezone\":\"utc\"\n        },\n        \"freshness_check\": {\n          \"lower_bound_delta_seconds\": 18000,\n          \"deadline_cron\": \"0 0 * * *\",\n          \"severity\": \"WARN\"\n        }\n      }\n    }\n  )\n-}}\n</code></pre>"},{"location":"dbt/getting_started/#partition-definitions","title":"Partition Definitions","text":"<p>When specified dagster will automatically create a partition for a given time span, allowing for incremental loading and backfills.  Typically an incremental materialization strategy is used, with a special jinja incremental macro that will accept time ranges from dagster.  See jinja below.</p>"},{"location":"dbt/getting_started/#partition","title":"partition","text":"<p>When specified dagster will automatically create a partition for a given time span, allowing for incremental loading and backfills.</p> <p>Accepted values: <code>hourly</code> <code>daily</code> <code>weekly</code> <code>monthly</code></p>"},{"location":"dbt/getting_started/#partition_start_date","title":"partition_start_date","text":"<p>When a partition is specified, you must also define the earliest time available in the upstream dependency.  This is given in ISO format:</p> <p>Accepted values: <code>YYYY-mm-dd</code> <code>YYYY-mm-dd HH:mm:ss</code></p>"},{"location":"dbt/getting_started/#jinja","title":"Jinja","text":"<p>When a partition is defined, a jinja macro is used to allow dagster to insert the time range of the backfill at runtime.  This is similar to the typical is_incremental pattern, but uses <code>var(\"min_date)</code> and <code>var(\"max_date\")</code>.</p> <pre><code>{% if is_incremental() -%}\n  where 1=1 \n    and hit_at &gt;= '{{ var(\"min_date\") }}'\n    and hit_at &lt;= '{{ var(\"max_date\") }}'\n{%- endif %}\n</code></pre>"},{"location":"dbt/getting_started/#asset-checks","title":"Asset Checks","text":""},{"location":"dbt/getting_started/#freshness_check","title":"freshness_check","text":""},{"location":"dbt/getting_started/#cron_deadline","title":"cron_deadline","text":"<p>Provide a deadline for when it is expected the asset to be available.  Required for partitioned modeles.</p> <p>Accepted values: <code>str</code> cron expression.</p>"},{"location":"dbt/getting_started/#lower_bound_delta_seconds","title":"lower_bound_delta_seconds","text":"<p>Provide a delta for when it is expected the asset to be refreshed since last materialization, or cron deadline.</p> <p>Accepted values: <code>float</code> seconds since last successful materialization, or cron deadline</p>"},{"location":"dbt/getting_started/#severity","title":"severity","text":"<p>Define the severity of the check in the case of failure.</p> <p>Accepted values: <code>WARN</code> <code>ERROR</code></p>"},{"location":"dbt/getting_started/#declarative-automation-definitions","title":"Declarative Automation Definitions","text":"<p>Instead of using fixed schedules, declarative automation is used.  With this methodology, the model will be aware of upstream assets, and will materialize as soon as it is able given the condition set.</p>"},{"location":"dbt/getting_started/#automation_condition","title":"automation_condition","text":"<p>Set under what conditions the asset should be materialized.  When the condition requires additional configuration, automation_condition_config is used.</p> <p>accepted_values: <code>manual</code> <code>on_cron_no_deps</code> <code>eager</code> <code>missing_or_changed</code> <code>lazy</code> <code>lazy_on_cron</code> <code>eager_with_deps_checks</code> <code>...</code></p> <p>Reference: elt_core/defs/automation_conditions.py</p>"},{"location":"dbt/getting_started/#automation_condition_config","title":"automation_condition_config","text":"<p>Some automation conditions require configuration, when this is the case, the configuration will be passed through this argument.</p> <p>accepted_values: <code>list</code> <code>dict</code></p> <p>Reference: elt_core/defs/automation_conditions.py</p>"},{"location":"dbt/getting_started/#available-automation-conditions","title":"Available Automation Conditions:","text":"<ul> <li>eager: Will materialize every time an upstream dependency is materialized.</li> <li>eager_with_deps_checks: Will materialize when any upstream dependency refreshes, but only after the upstream asset passes its quality checks.</li> <li>lazy: Will materialize only when upstream assets require it.</li> <li>lazy_on_cron: Will materialize only when upstream assets require it, but only once per cron schedule.</li> <li>missing_or_changed: Will materialize when the code has changed since the last run.</li> <li>manual: Asset will only materialize from a manual run in dagster.</li> <li>on_cron_no_deps: Will materialize on the cron schedule regardless of the state of upstream assets.</li> </ul> <p>additional conditions will be added over time, for the most up to date list, reference: elt_core/defs/automation_conditions.py </p>"}]}