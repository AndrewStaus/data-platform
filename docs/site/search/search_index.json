{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":"<p>A Dagster project integrating dbt, Sling, dltHub, and Snowflake into a single data platform. Includes stubs for powerBi, and AzureML, as well as Azure Keyvault to demonstrate external integrations.</p> <pre><code>---\nconfig:\n  theme: neutral\n---\nflowchart LR\n  subgraph s1[\"sling dlt\"]\n        n1[\"src\"]\n        n2[\"raw\"]\n  end\n  subgraph s2[\"dbt\"]\n        n3[\"pii\"]\n        n4[\"stg\"]\n        n5[\"snp\"]\n        n6[\"int\"]\n        n7[\"mrt\"]\n  end\n  subgraph s3[\"powerbi\"]\n        n8[\"sem\"]\n        n9[\"exp\"]\n  end\n  subgraph s4[\"snowpark\"]\n        n10[\"ml\"]\n  end\n  n1 --&gt; n2\n  n2 --&gt; n3\n  n2 --&gt; n4\n  n2 --&gt; n5\n  n5 --&gt; n6\n  n2 --&gt; n6\n  n6 --&gt; n7\n  n7 --&gt; n8\n  n8 --&gt; n9\n  n7 --&gt; n10\n  n6 --&gt; n10\n\n</code></pre>"},{"location":"#getting-started","title":"Getting Started","text":""},{"location":"#note-on-security","title":"Note on Security","text":"<p>This is a simple configuration for demonstration.  Some security best practices have been skipped in order to make demonstration easier, however oath is supported for a real deployment to authenticate developers against the resources their accessing.</p>"},{"location":"#uv","title":"uv","text":"<p>This project is configured using the uv python package manager.  To initialize the project, install uv if you do not have it already, and enter the command:</p> <pre><code>uv sync\n</code></pre> <p>This will create a virtual environment and install all required dependacies.</p>"},{"location":"#resources","title":"Resources","text":""},{"location":"#source-database","title":"Source Database","text":"<p>This demo has been set up using a postgres server with a handful of tables in a single schema.</p>"},{"location":"#desination-warehouse","title":"Desination Warehouse","text":"<p>This demo is using a Snowflake warehouse to demonstrate. It assumes the existance of the following databases:</p>"},{"location":"#prod","title":"prod","text":""},{"location":"#analytics","title":"analytics","text":"<p>This is where the silver layer data will be stored: staged (stg), incremental (inc), dimensions (dim), facts (fct).  This is the main destination for dbt models.  </p>"},{"location":"#raw","title":"raw","text":"<p>The database that stores data ingested from the source systems.</p>"},{"location":"#snapshots","title":"snapshots","text":"<p>SCD Type 2 data, that is primaraly captured through dbt eagerly on raw data.</p>"},{"location":"#qa","title":"qa","text":"<p>The QA environment is not configured for this demonstration, however in a real deployment this would exist to perform slim CI on pull requests from feature branchs to develop, and pull requests from develop to main. </p>"},{"location":"#dev","title":"dev","text":"<p>The dev databases mirror the production, however each developer will generate shcmeas tagged with their user name so that they can develop new assets in isolation.</p>"},{"location":"#_dev_analytics","title":"_dev_analytics","text":""},{"location":"#_dev_raw","title":"_dev_raw","text":""},{"location":"#_dev_snapshots","title":"_dev_snapshots","text":""},{"location":"#env-file","title":".env File","text":"<p>The .env file will hold your environment variables. Create this file in your root project folder and populate it will the correct credentials for your database and warehouse.</p> <p>In a true deployment, this would be set up using more secure methods.</p> <pre><code># .env\nTARGET=dev\nDAGSTER_HOME=.\\\\.dagster_home\nPYTHONLEGACYWINDOWSSTDIO=1\nDBT_PROJECT_DIR=.\\\\dbt\\\\\nPREPARE_IF_DEV=1\n\nPROD__DESTINATION__DATABASE=raw\nPROD__DESTINATION__HOST=&lt;your_hostname&gt;\nPROD__DESTINATION__ROLE=&lt;role_with_create_grants_on_prod&gt;\nPROD__DESTINATION__USER=&lt;user_with_create_grants_on_prod&gt;\nPROD__DESTINATION__PASSWORD=&lt;user_password&gt;\nPROD__DESTINATION__WAREHOUSE=&lt;warehouse&gt;\n\nDEV__DESTINATION__DATABASE=raw\nDEV__DESTINATION__HOST=&lt;your_hostname&gt;\nDEV__DESTINATION__ROLE=&lt;role_with_create_grants_on_dev&gt;\nDEV__DESTINATION__USER=&lt;user_with_create_grants_on_dev&gt;\nDEV__DESTINATION__PASSWORD=&lt;user_password&gt;\nDEV__DESTINATION__WAREHOUSE=&lt;warehouse&gt;\n\nANY__SOURCE__DATABASE=&lt;demo_database&gt;\nANY__SOURCE__HOST=&lt;demo_host&gt;\nANY__SOURCE__PORT=&lt;demo_port&gt;\nANY__SOURCE__USER=&lt;demo_user&gt;\nANY__SOURCE__PASSWORD=&lt;demo_password&gt;\n</code></pre> <p>PROD configuration is used on the production deployment, and the credentials should have appropriate grants for the prod databases on the warehouse.</p> <p>DEV configuration is used in the local development environment, and should have select grants to the analytics databases, and create schema grants on the development databases. </p> <p>ANY configuration is shared between dev and prod, and will be used to connect to the demo database.</p>"},{"location":"#deployment","title":"Deployment","text":""},{"location":"#local-development","title":"Local Development","text":"<p>Once the above setup steps are complete, you are ready to launch you local development server.  A task has been defined in vs code to make running the server easy.</p> <ol> <li>Open the command pallet: <code>ctrl+shift+p</code></li> <li>Type <code>&gt;Tasks: Run Task</code> and hit enter</li> <li>Type <code>dagster dev</code> and hit enter</li> </ol> <p>You should now have a dagster development server running on your local machine.</p>"},{"location":"#production-deployment","title":"Production Deployment","text":"<p>To simulate a production deployment a <code>build and deploy</code> task has also been created. This task assumes that you have docker deskop installed and running, with a 'kind' kubernettes cluster availible.</p> <p>If so you can run this command to build the 'user code deployment' docker image and deploy it to the cluster.  Once deployed you can run the <code>k8s port forward</code> to make the web server availible at <code>http://127.0.0.1:63446/</code></p>"},{"location":"#useful-vs-code-extensions","title":"Useful VS Code Extensions","text":""},{"location":"#python","title":"Python","text":"<p>Core python extension from microsoft.  This will provide syntax highlighting for .py files.</p>"},{"location":"#ruff","title":"Ruff","text":"<p>A python linter that will help conform to style guides for the project.  The styles are enforced through settings in the pyproject file so that all contributers write high quality, standardized code.</p>"},{"location":"#power-user-for-dbt","title":"Power User for dbt","text":"<p>Allows for advanced functionality of dbt assets while the official dbt extension becomes availibe in general release.</p>"},{"location":"#even-better-toml","title":"Even Better TOML","text":"<p>Provides syntax highlighting for TOML files which are used for sling configurations.</p>"},{"location":"#cron-explained","title":"Cron Explained","text":"<p>When hovering over cron expressions, provides a plain english explanation of what the expression means.</p>"},{"location":"#snowflake","title":"Snowflake","text":"<p>Allows for viewing snowflake resources, and performing SQL, DML, and DDL against the warehouse.</p>"},{"location":"dlthub/","title":"dltHub","text":"<p>https://dlthub.com/ docs</p> <p>dlt is the most popular production-ready Python library for moving data. It loads data from various and often messy data sources into well-structured, live datasets.</p> <p>Unlike other non-Python solutions, with dlt, there's no need to use any backends or containers. We do not replace your data platform, deployments, or security models. Simply import dlt in your favorite AI code editor, or add it to your Jupyter Notebook. You can load data from any source that produces Python data structures, including APIs, files, databases, and more.</p>"},{"location":"dlthub/#structure","title":"Structure","text":"<pre><code>---\nconfig:\n  theme: neutral\n---\nflowchart LR\n subgraph s1[\"definitions\"]\n        n4[\"assets_definition\"]\n        n5[\"resource\"]\n  end\n    n1[\"dlt_source.py\"] --&gt; n3[\"factory\"]\n    n2[\"translator\"] --&gt; n3\n    n3 --&gt; n4\n    n5 --&gt; n7[\"run\"]\n    n4 --&gt; n7\n    n6[\"context\"] --&gt; n7\n    n4@{ shape: doc}\n    n5@{ shape: proc}\n    n1@{ shape: docs}\n    n3@{ shape: procs}\n    n2@{ shape: rect}\n    n7@{ shape: proc}\n    n6@{ shape: proc}\n</code></pre>"},{"location":"dlthub/#factory","title":"Factory","text":"<p>The factory will parse user defined python scripts into dagster resources and assets.</p>"},{"location":"dlthub/#translator","title":"Translator","text":"<p>The translator will tell dagster how to translate dltHub concepts into dagster concepts, such as how a asset key is defined, or a automation condition.</p>"},{"location":"dlthub/#resources","title":"Resources","text":"<p>The resources will pass all the translated assets to the dagster runtime.</p>"},{"location":"dlthub/#artifacts","title":"Artifacts","text":""},{"location":"dlthub/#definitionspy","title":"definitions.py","text":"<p>The reources file is what is ingested through the factory to create the dltHub assets in dagster.  The user will provide a lists of resources, typically one for each endpoint that will materialize as an asset in dagster.</p> <pre><code># definitions.py\nfrom .data import api_generator\n...\nConfigurableDltResource.config(\n    dlt.resource(\n        # the generator you defined in the data.py file\n        api_generator,\n        # the schema and table to materialize on the \n        name=\"schema.table\", warehouse \n        table_name=\"table\",\n        primary_key=\"id\", # the primary key column\n        write_disposition=\"merge\", # how to incrementally load\n    ),\n    kinds={\"api\"},\n    # aditional dagster configuration for orchestration and checks\n    meta={\n        \"dagster\": { \n            \"automation_condition\": \"on_schedule\", \n            \"automation_condition_config\": {\n                \"cron_schedule\": \"@daily\",\n                \"cron_timezone\": \"utc\",\n            },\n            \"freshness_lower_bound_delta_seconds\": 108000\n        }\n    },\n)\n...\n</code></pre>"},{"location":"dlthub/#datapy","title":"data.py","text":"<p>The code to generate data, this will be imported into the definitions.py module.  dltHub can accept any arbitrary code as long as it yields a python data object.  Suported  formats include avro data frames, json in the form of python dictonaries</p> <pre><code># data.py\nfrom collections.abc import Callable, Generator\nfrom typing import Any\n\nimport requests\n\ndef api_generator() -&gt; Generator[Any, Any, None]:\n    uri = \"https://www.api.com/endpoint\"\n    response = requests.get(uri)\n    yield response.json()\n    while next_uri := response.json().get(\"next_page\"):\n        response = requests.get(next_uri)\n        yield response.json()\n</code></pre> <p>A common design pattern for api's with multiple endpoints is to use a factory function that will return a different generator for different enpoints.</p> <pre><code># data.py\nfrom collections.abc import Callable, Generator\nfrom typing import Any\n\nimport requests\n\ndef get_api_generator(endpoint: str) -&gt; Callable[[], Any]:\n    base_uri = \"https://www.api.com/\"\n\n    def api_generator() -&gt; Generator[Any, Any, None]:\n        response = requests.get(base_uri+endpoint)\n        yield response.json()\n        while next_uri := response.json().get(\"next_page\"):\n            response = requests.get(next_uri)\n            yield response.json()\n\n    return api_generator\n</code></pre> <p>This can then be reused in the resources.py module</p> <pre><code># definitions.py\nfrom .data import get_api_generator\n...\nConfigurableDltResource.config(\n    dlt.resource(\n        get_api_generator(\"endpoint_one\"),\n        name=\"schema.table_one\",\n        table_name=\"table_one\",\n        primary_key=\"id\",\n        write_disposition=\"merge\",\n    ),\n    kinds={\"api\"}\n),\nConfigurableDltResource.config(\n    dlt.resource(\n        get_api_generator(\"endpoint_two\"),\n        name=\"schema.table_two\",\n        table_name=\"table_two\",\n        primary_key=\"id\",\n        write_disposition=\"merge\",\n    ),\n    kinds={\"api\"}\n),\n...\n</code></pre>"},{"location":"dlthub/#other-dlthub-concepts","title":"Other dltHub concepts","text":"<p>On its own dltHub has other concepts that you may see in their documentation such as pipelines, desinations, state, schema, however these have been abstracted away in the data platform, so all a developer needs to focus on is creating a generator, and defining it as a dagster asset in the definitions.py file.</p>"},{"location":"sling/","title":"Sling","text":"<p>https://slingdata.io/ https://github.com/slingdata-io/sling-cli</p> <p>Sling is a Powerful Data Integration tool enabling seamless ELT operations as well as quality checks across files, databases, and storage systems.</p>"},{"location":"sling/#structure","title":"Structure","text":"<pre><code>---\nconfig:\n  theme: neutral\n---\nflowchart LR\n subgraph s1[\"definitions\"]\n        n4[\"assets_definition\"]\n        n5[\"resource\"]\n  end\n    n1[\"replication_config.yaml\"] --&gt; n3[\"factory\"]\n    n2[\"translator\"] --&gt; n3\n    n3 --&gt; n4\n    n5 --&gt; n7[\"run\"]\n    n4 --&gt; n7\n    n6[\"context\"] --&gt; n7\n    n4@{ shape: doc}\n    n5@{ shape: proc}\n    n1@{ shape: docs}\n    n3@{ shape: procs}\n    n2@{ shape: rect}\n    n7@{ shape: proc}\n    n6@{ shape: proc}\n</code></pre>"},{"location":"sling/#factory","title":"Factory","text":"<p>The factory will parse user defined yaml files representing connections and streams into dagster resources and assets.</p>"},{"location":"sling/#translator","title":"Translator","text":"<p>The translator will tell dagster how to translate sling concepts into dagster concepts, such as how a asset key is defined, or a automation condition.</p>"},{"location":"sling/#resources","title":"Resources","text":"<p>The resources will pass all the translated assets to the dagster runtime.</p>"},{"location":"sling/#configs","title":"Configs","text":""},{"location":"sling/#connection-config","title":"Connection Config","text":"<p>https://docs.slingdata.io/sling-cli/environment#sling-env-file-env.yaml</p> <p>The connection config defines the database, or file system connection details so that it can be used as a source or desintation on replications.  It follows the similar patterns as the sling env file, however additional functionality iis applied for secret management through the keyvault.</p> <p>For values which are stored in the key vault, you can specify the key name, and if you wish for the key to be shown in plain text on the dagster front end, or if it should be securly masked.</p> <pre><code>connections:\n  -name: database_name\n   type: oracle\n   database: {SOURCE__ON_PREM_OLTP__CREDENTIALS__DATABASE: show}\n   host: {SOURCE__ON_PREM_OLTP__CREDENTIALS__HOST: show}\n   port: {SOURCE__ON_PREM_OLTP__CREDENTIALS__PORT: mask}\n   user: {SOURCE__ON_PREM_OLTP__CREDENTIALS__USER: mask}\n   password: {SOURCE__ON_PREM_OLTP__CREDENTIALS__PASSWORD: mask}\n</code></pre>"},{"location":"sling/#replication-config","title":"Replication Config","text":"<p>https://docs.slingdata.io/concepts/replication Replications are how extract and loads are defined.  Source name, and target names reference connections you have defined in the connections section.  The connection does not need to be defined in the yaml file you are referencing it in.  Typically there will be a single yaml file for each connection, with replications showing egress from that connection to another system.</p> <pre><code>replications:\n  - name: source_name-&gt;desitnation_name\n    env: {SLING_LOADED_AT_COLUMN: timestamp}\n    source: source_name\n    target: target_name\n    defaults:\n      mode: incremental\n      object: '{stream_schema_upper}.{stream_table_upper}'\n      primary_key: [id]\n      update_key: updated_at\n      target_options:\n        column_casing: snake\n        add_new_columns: true\n        adjust_column_type: true\n      meta:\n        dagster:\n          automation_condition: \"on_cron_no_deps\"\n          automation_condition_config: {\"cron_schedule\":\"@daily\", \"cron_timezone\":\"utc\"}\n          freshness_lower_bound_delta: 1800\n    streams:\n      source_schema.table_one:\n        tags: ['contains_pii']\n      source_schema.table_two:\n        primary_key: [pk_column]\n\n</code></pre> <p>Streams are the tables, or files to transfer, settings from the default section are applied to all streams, unless specific configuartion is applied to that stream in which case the stream config takes precedence.</p>"},{"location":"dagster/definitions/","title":"Definitions","text":""},{"location":"dagster/defs/_azureml/definitions/","title":"Definitions","text":""},{"location":"dagster/defs/_powerbi/definitions/","title":"Definitions","text":""},{"location":"dagster/defs/dbt/definitions/","title":"Definitions","text":""},{"location":"dagster/defs/dbt/definitions/#data_platform.defs.dbt.definitions.defs","title":"<code>defs()</code>","text":"<p>Returns set of definitions explicitly available and loadable by Dagster tools. Will be automatically dectectd and loaded by the load_defs function in the root definitions file.</p> <p>@definitions decorator will provides lazy loading so that the assets are only instantiated when needed.</p> Source code in <code>data_platform\\defs\\dbt\\definitions.py</code> <pre><code>@definitions\ndef defs() -&gt; Definitions:\n    \"\"\"Returns set of definitions explicitly available and loadable by Dagster tools.\n    Will be automatically dectectd and loaded by the load_defs function in the root\n    definitions file.\n\n    @definitions decorator will provides lazy loading so that the assets are only\n    instantiated when needed.\n    \"\"\"\n    project_dir = Path(__file__).joinpath(*[\"..\"] * 4, \"dbt/\").resolve()\n    state_path = \"state/\"\n\n    def dbt() -&gt; DbtProject:\n        project = DbtProject(\n            project_dir=project_dir,\n            target=os.getenv(\"TARGET\", \"prod\"),\n            state_path=state_path,\n            profile=\"dbt\",\n        )\n        if os.getenv(\"PREPARE_IF_DEV\") == \"1\":\n            project.prepare_if_dev()\n        return project\n\n    return DagsterDbtFactory.build_definitions(dbt)\n</code></pre>"},{"location":"dagster/defs/dlthub/resources/","title":"Resources","text":""},{"location":"dagster/defs/dlthub/resources/#data_platform.defs.dlthub.resources.defs","title":"<code>defs()</code>","text":"<p>Returns set of definitions explicitly available and loadable by Dagster tools. Will be automatically dectectd and loaded by the load_defs function in the root definitions file.</p> <p>Assets and asset checks for dltHub are defined in the dlthub subfolder in the definitions.py file for each resource.</p> <p>@definitions decorator will provides lazy loading so that the assets are only instantiated when needed.</p> Source code in <code>data_platform\\defs\\dlthub\\resources.py</code> <pre><code>@definitions\ndef defs() -&gt; Definitions:\n    \"\"\"Returns set of definitions explicitly available and loadable by Dagster tools.\n    Will be automatically dectectd and loaded by the load_defs function in the root\n    definitions file.\n\n    Assets and asset checks for dltHub are defined in the dlthub subfolder in the\n    definitions.py file for each resource.\n\n    @definitions decorator will provides lazy loading so that the assets are only\n    instantiated when needed.\n    \"\"\"\n    import os\n\n    import dagster as dg\n    from dagster_dlt import DagsterDltResource\n\n    from ...utils.keyvault_stub import SecretClient\n\n    kv = SecretClient(\n        vault_url=os.getenv(\"AZURE_KEYVAULT_URL\"),\n        credential=os.getenv(\"AZURE_KEYVAULT_CREDENTIAL\"),\n    )\n\n    os.environ[\"DESTINATION__SNOWFLAKE__CREDENTIALS__HOST\"] = kv.get_secret(\n        \"DESTINATION__SNOWFLAKE__HOST\"\n    )\n    os.environ[\"DESTINATION__SNOWFLAKE__CREDENTIALS__USERNAME\"] = kv.get_secret(\n        \"DESTINATION__SNOWFLAKE__USER\"\n    )\n    os.environ[\"DESTINATION__SNOWFLAKE__CREDENTIALS__PASSWORD\"] = kv.get_secret(\n        \"DESTINATION__SNOWFLAKE__PASSWORD\"\n    )\n    os.environ[\"DESTINATION__SNOWFLAKE__CREDENTIALS__DATABASE\"] = kv.get_secret(\n        \"DESTINATION__SNOWFLAKE__DATABASE\"\n    )\n    os.environ[\"DESTINATION__SNOWFLAKE__CREDENTIALS__ROLE\"] = kv.get_secret(\n        \"DESTINATION__SNOWFLAKE__ROLE\"\n    )\n    os.environ[\"DESTINATION__SNOWFLAKE__CREDENTIALS__WAREHOUSE\"] = kv.get_secret(\n        \"DESTINATION__SNOWFLAKE__WAREHOUSE\"\n    )\n\n    os.environ[\"ENABLE_DATASET_NAME_NORMALIZATION\"] = \"false\"\n\n    return dg.Definitions(resources={\"dlt\": DagsterDltResource()})\n</code></pre>"},{"location":"dagster/defs/dlthub/dlthub/exchange_rate/data/","title":"Data","text":""},{"location":"dagster/defs/dlthub/dlthub/exchange_rate/data/#data_platform.defs.dlthub.dlthub.exchange_rate.data.get_exchange_rate","title":"<code>get_exchange_rate(currency)</code>","text":"<p>Return a generator that will yield responses from an api with daily exchange rates for the selected currency</p> Source code in <code>data_platform\\defs\\dlthub\\dlthub\\exchange_rate\\data.py</code> <pre><code>def get_exchange_rate(currency: str) -&gt; Callable[[], Any]:\n    \"\"\"Return a generator that will yield responses from an api\n    with daily exchange rates for the selected currency\n    \"\"\"\n\n    uri = (\n        \"https://cdn.jsdelivr.net/npm/@fawazahmed0/currency-api\"\n        \"@latest\"\n        \"/v1/\"\n        f\"currencies/{currency}.json\"\n    )\n\n    def exchange_api() -&gt; Generator[Any, Any, None]:\n        response = requests.get(uri)\n        yield response.json()\n        while next_uri := response.json().get(\"next_page\"):\n            response = requests.get(next_uri)\n            yield response.json()\n\n    return exchange_api\n</code></pre>"},{"location":"dagster/defs/dlthub/dlthub/exchange_rate/definitions/","title":"Definitions","text":""},{"location":"dagster/defs/dlthub/dlthub/facebook_ads/data/","title":"Data","text":""},{"location":"dagster/defs/dlthub/dlthub/facebook_ads/data/#data_platform.defs.dlthub.dlthub.facebook_ads.data.get_campaigns","title":"<code>get_campaigns()</code>","text":"<p>A generator that will yield responses from a stub representing an api to download data from facebook ads.</p> Source code in <code>data_platform\\defs\\dlthub\\dlthub\\facebook_ads\\data.py</code> <pre><code>def get_campaigns() -&gt; Generator[list[dict[str, Any]], Any, None]:\n    \"\"\"A generator that will yield responses from a stub representing an api to\n    download data from facebook ads.\n    \"\"\"\n\n    response = [\n        {\n            \"id\": 90009,\n            \"name\": \"summer_sale\",\n            \"start_date\": \"2024-06-01\",\n            \"updated\": \"2025-07-02 21:14:03\",\n        },\n        {\n            \"id\": 80008,\n            \"name\": \"winter_sale\",\n            \"start_date\": \"2024-01-01\",\n            \"updated\": \"2025-07-02 21:14:03\",\n        },\n        {\n            \"id\": 70008,\n            \"name\": \"LAPTOP\",\n            \"start_date\": \"2024-01-01\",\n            \"updated\": \"2025-07-03 21:14:03\",\n        },\n        {\n            \"id\": 60008,\n            \"name\": \"greenfrog\",\n            \"start_date\": \"2024-01-01\",\n            \"updated\": \"2025-07-04 21:14:03\",\n        },\n        {\n            \"id\": 50008,\n            \"name\": \"blowout\",\n            \"start_date\": \"2024-01-01\",\n            \"updated\": \"2025-07-05 21:14:03\",\n        },\n        {\n            \"id\": 40008,\n            \"name\": \"raindays\",\n            \"start_date\": \"2024-01-01\",\n            \"updated\": \"2025-07-06 21:14:03\",\n        },\n        {\n            \"id\": 30008,\n            \"name\": \"powersale\",\n            \"start_date\": \"2024-01-01\",\n            \"updated\": \"2025-07-07 21:14:03\",\n        },\n        {\n            \"id\": 20008,\n            \"name\": \"sale11111\",\n            \"start_date\": \"2024-01-01\",\n            \"updated\": \"2025-07-08 21:14:03\",\n        },\n        {\n            \"id\": 10008,\n            \"name\": \"sale11112\",\n            \"start_date\": \"2024-01-01\",\n            \"updated\": \"2025-07-09 21:14:03\",\n        },\n        {\n            \"id\": 11008,\n            \"name\": \"sale11113\",\n            \"start_date\": \"2024-01-01\",\n            \"updated\": \"2025-07-10 21:14:03\",\n        },\n        {\n            \"id\": 12008,\n            \"name\": \"sale11114\",\n            \"start_date\": \"2024-01-01\",\n            \"updated\": \"2025-07-11 21:14:03\",\n        },\n        {\n            \"id\": 13008,\n            \"name\": \"sale11115\",\n            \"start_date\": \"2024-01-01\",\n            \"updated\": \"2025-07-12 21:14:03\",\n        },\n        {\n            \"id\": 14008,\n            \"name\": \"sale11116\",\n            \"start_date\": \"2024-01-01\",\n            \"updated\": \"2025-07-13 21:14:03\",\n        },\n        {\n            \"id\": 15008,\n            \"name\": \"sale11117\",\n            \"start_date\": \"2024-01-01\",\n            \"updated\": \"2025-07-14 21:14:03\",\n        },\n        {\n            \"id\": 16008,\n            \"name\": \"sale11118\",\n            \"start_date\": \"2024-01-01\",\n            \"updated\": \"2025-07-15 21:14:03\",\n        },\n        {\n            \"id\": 17008,\n            \"name\": \"sale11119\",\n            \"start_date\": \"2024-01-01\",\n            \"updated\": \"2025-07-16 21:14:03\",\n        },\n        {\n            \"id\": 18008,\n            \"name\": \"sale11110\",\n            \"start_date\": \"2024-01-01\",\n            \"updated\": \"2025-07-17 21:14:03\",\n        },\n        {\n            \"id\": 19008,\n            \"name\": \"sale11121\",\n            \"start_date\": \"2024-01-01\",\n            \"updated\": \"2025-07-18 21:14:03\",\n        },\n        {\n            \"id\": 11001,\n            \"name\": \"sale11122\",\n            \"start_date\": \"2024-01-01\",\n            \"updated\": \"2025-07-19 21:14:03\",\n        },\n        {\n            \"id\": 11002,\n            \"name\": \"sale11123\",\n            \"start_date\": \"2024-01-01\",\n            \"updated\": \"2025-07-20 21:14:03\",\n        },\n        {\n            \"id\": 11003,\n            \"name\": \"sale11124\",\n            \"start_date\": \"2024-01-01\",\n            \"updated\": \"2025-07-21 21:14:03\",\n        },\n        {\n            \"id\": 11004,\n            \"name\": \"sale11125\",\n            \"start_date\": \"2024-01-01\",\n            \"updated\": \"2025-07-22 21:14:03\",\n        },\n        {\n            \"id\": 11005,\n            \"name\": \"sale11126\",\n            \"start_date\": \"2024-01-01\",\n            \"updated\": \"2025-07-23 21:14:03\",\n        },\n        {\n            \"id\": 11006,\n            \"name\": \"sale11127\",\n            \"start_date\": \"2024-01-01\",\n            \"updated\": \"2025-07-24 21:14:03\",\n        },\n        {\n            \"id\": 66007,\n            \"name\": \"sale11128\",\n            \"start_date\": \"2024-01-01\",\n            \"updated\": \"2025-07-25 21:14:03\",\n        },\n        {\n            \"id\": 43008,\n            \"name\": \"sale11129\",\n            \"start_date\": \"2024-01-01\",\n            \"updated\": \"2025-07-26 21:14:03\",\n        },\n        {\n            \"id\": 76009,\n            \"name\": \"sale11131\",\n            \"start_date\": \"2024-01-01\",\n            \"updated\": \"2025-07-27 21:14:03\",\n        },\n        {\n            \"id\": 65008,\n            \"name\": \"sale11132\",\n            \"start_date\": \"2024-01-01\",\n            \"updated\": \"2025-07-28 21:14:03\",\n        },\n        {\n            \"id\": 54007,\n            \"name\": \"sale11133\",\n            \"start_date\": \"2024-01-01\",\n            \"updated\": \"2025-07-29 21:14:03\",\n        },\n        {\n            \"id\": 43006,\n            \"name\": \"sale11134\",\n            \"start_date\": \"2024-01-01\",\n            \"updated\": \"2025-07-31 21:14:03\",\n        },\n        {\n            \"id\": 32005,\n            \"name\": \"sale11135\",\n            \"start_date\": \"2024-01-01\",\n            \"updated\": \"2025-08-01 21:14:03\",\n        },\n        {\n            \"id\": 21004,\n            \"name\": \"sale11136\",\n            \"start_date\": \"2024-01-01\",\n            \"updated\": \"2025-08-02 21:14:03\",\n        },\n        {\n            \"id\": 54003,\n            \"name\": \"sale11137\",\n            \"start_date\": \"2024-01-01\",\n            \"updated\": \"2025-08-03 21:14:03\",\n        },\n        {\n            \"id\": 66002,\n            \"name\": \"sale11138\",\n            \"start_date\": \"2024-01-01\",\n            \"updated\": \"2025-08-04 21:14:03\",\n        },\n        {\n            \"id\": 77001,\n            \"name\": \"sale11139\",\n            \"start_date\": \"2024-01-01\",\n            \"updated\": \"2025-08-05 21:14:03\",\n        },\n        {\n            \"id\": 88044,\n            \"name\": \"sale11141\",\n            \"start_date\": \"2024-01-01\",\n            \"updated\": \"2025-08-06 21:14:03\",\n        },\n        {\n            \"id\": 99033,\n            \"name\": \"sale11142\",\n            \"start_date\": \"2024-01-01\",\n            \"updated\": \"2025-08-07 21:14:03\",\n        },\n        {\n            \"id\": 33220,\n            \"name\": \"sale11143\",\n            \"start_date\": \"2024-01-01\",\n            \"updated\": \"2025-08-08 21:14:03\",\n        },\n    ]\n    yield response\n</code></pre>"},{"location":"dagster/defs/dlthub/dlthub/facebook_ads/definitions/","title":"Definitions","text":""},{"location":"dagster/defs/dlthub/dlthub/google_ads/data/","title":"Data","text":""},{"location":"dagster/defs/dlthub/dlthub/google_ads/data/#data_platform.defs.dlthub.dlthub.google_ads.data.google_ads","title":"<code>google_ads(endpoint)</code>","text":"<p>Return a generator that will yield responses from a stub representing an api to download data from google ads.</p> Source code in <code>data_platform\\defs\\dlthub\\dlthub\\google_ads\\data.py</code> <pre><code>def google_ads(endpoint) -&gt; Callable[[], Generator[list[dict[str, Any]], Any, None]]:\n    \"\"\"Return a generator that will yield responses from a stub representing an api to\n    download data from google ads.\n    \"\"\"\n\n    if endpoint == \"get_campaigns\":\n\n        def get_campaigns() -&gt; Generator[list[dict[str, Any]], Any, None]:\n            response = [\n                {\n                    \"id\": 10001,\n                    \"name\": \"summer_sale\",\n                    \"start_date\": \"2024-06-01\",\n                    \"criteria\": [{\"id\": 1}, {\"id\": 2}],\n                },\n                {\n                    \"id\": 20002,\n                    \"name\": \"winter_sale\",\n                    \"start_date\": \"2024-01-01\",\n                    \"criteria\": [{\"id\": 2}],\n                },\n            ]\n            yield response\n\n        return get_campaigns\n\n    if endpoint == \"get_criterion\":\n\n        def get_criterion() -&gt; Generator[list[dict[str, Any]], Any, None]:\n            response = [\n                {\"id\": 1, \"type\": \"audience\", \"value\": \"summer_shoppers\"},\n                {\"id\": 2, \"type\": \"age\", \"value\": \"20-35\"},\n            ]\n            yield response\n\n        return get_criterion\n\n    else:\n        raise KeyError(f\"Endpoint '{endpoint}' is not implemented.\")\n</code></pre>"},{"location":"dagster/defs/dlthub/dlthub/google_ads/definitions/","title":"Definitions","text":""},{"location":"dagster/defs/sling/definitions/","title":"Definitions","text":""},{"location":"dagster/defs/sling/definitions/#data_platform.defs.sling.definitions.defs","title":"<code>defs()</code>","text":"<p>Returns set of definitions explicitly available and loadable by Dagster tools. Will be automatically dectectd and loaded by the load_defs function in the root definitions file.</p> <p>@definitions decorator will provides lazy loading so that the assets are only instantiated when needed.</p> Source code in <code>data_platform\\defs\\sling\\definitions.py</code> <pre><code>@definitions\ndef defs() -&gt; Definitions:\n    \"\"\"Returns set of definitions explicitly available and loadable by Dagster tools.\n    Will be automatically dectectd and loaded by the load_defs function in the root\n    definitions file.\n\n    @definitions decorator will provides lazy loading so that the assets are only\n    instantiated when needed.\n    \"\"\"\n    from pathlib import Path\n\n    from ...lib.sling import DagsterSlingFactory\n\n    config_dir = Path(__file__).joinpath(*[\"..\"], \"sling\").resolve()\n\n    return DagsterSlingFactory.build_definitions(config_dir)\n</code></pre>"},{"location":"dagster/lib/lib/","title":"Lib","text":""},{"location":"dagster/lib/dbt/constants/","title":"Constants","text":"<p>Constant values that are useful in selecting dbt models.</p>"},{"location":"dagster/lib/dbt/dbt/","title":"Dbt","text":""},{"location":"dagster/lib/dbt/dbt/#data_platform.lib.dbt.DagsterDbtFactory","title":"<code>DagsterDbtFactory</code>","text":"<p>Factory to generate dagster definitions from a dbt project.</p> Source code in <code>data_platform\\lib\\dbt\\__init__.py</code> <pre><code>class DagsterDbtFactory:\n    \"\"\"Factory to generate dagster definitions from a dbt project.\"\"\"\n\n    @cache\n    @staticmethod\n    def build_definitions(dbt: Callable[[], DbtProject]) -&gt; dg.Definitions:\n        \"\"\"Returns a Definitions object from a dbt project.\"\"\"\n\n        assets = [\n            DagsterDbtFactory._get_assets(\n                \"dbt_partitioned_models\",\n                dbt=dbt,\n                select=TIME_PARTITION_SELECTOR,\n                partitioned=True,\n            ),\n            DagsterDbtFactory._get_assets(\n                \"dbt_non_partitioned_models\",\n                dbt=dbt,\n                exclude=TIME_PARTITION_SELECTOR,\n                partitioned=False,\n            ),\n        ]\n\n        freshness_checks = build_freshness_checks_from_dbt_assets(dbt_assets=assets)\n        freshness_sensor = dg.build_sensor_for_freshness_checks(\n            freshness_checks=freshness_checks, name=\"dbt_freshness_checks_sensor\"\n        )\n\n        return dg.Definitions(\n            resources={\n                \"dbt\": DbtCliResource(\n                    project_dir=dbt(),\n                )\n            },\n            assets=assets,\n            asset_checks=freshness_checks,\n            sensors=[freshness_sensor],\n        )\n\n    @cache\n    @staticmethod\n    def _get_assets(\n        name: str | None,\n        dbt: Callable[[], DbtProject],\n        partitioned: bool = False,\n        select: str = DBT_DEFAULT_SELECT,\n        exclude: str | None = None,\n    ) -&gt; dg.AssetsDefinition:\n        \"\"\"Returns a AssetsDefinition with different execution for partitioned\n        and non-partitioned models so that they can be ran on the same job.\n        \"\"\"\n        dbt_project = dbt()\n        assert dbt_project\n\n        @dbt_assets(\n            name=name,\n            manifest=dbt_project.manifest_path,\n            select=select,\n            exclude=exclude,\n            dagster_dbt_translator=CustomDagsterDbtTranslator(\n                settings=DagsterDbtTranslatorSettings(\n                    enable_duplicate_source_asset_keys=True,\n                )\n            ),\n            backfill_policy=dg.BackfillPolicy.single_run(),\n            project=dbt_project,\n            pool=\"dbt\",\n        )\n        def assets(\n            context: dg.AssetExecutionContext, dbt: DbtCliResource, config: DbtConfig\n        ) -&gt; Generator[DbtEventIterator, Any, Any]:\n            args = [\"build\"]\n\n            if config.full_refresh:\n                args.append(\"--full-refresh\")\n            if config.defer_to_prod:\n                args.extend(dbt.get_defer_args())\n                if config.favor_state:\n                    args.append(\"--favor-state\")\n\n            if partitioned:\n                time_window = context.partition_time_window\n                format = \"%Y-%m-%d %H:%M:%S\"\n                dbt_vars = {\n                    \"min_date\": time_window.start.strftime(format),\n                    \"max_date\": time_window.end.strftime(format),\n                }\n\n                args.extend((\"--vars\", json.dumps(dbt_vars)))\n\n                yield from dbt.cli(\n                    args, context=context\n                ).stream()  # .with_insights() # type: ignore\n            else:\n                yield from dbt.cli(\n                    args, context=context\n                ).stream()  # .with_insights() # type: ignore\n\n        return assets\n</code></pre>"},{"location":"dagster/lib/dbt/dbt/#data_platform.lib.dbt.DagsterDbtFactory.build_definitions","title":"<code>build_definitions(dbt)</code>  <code>cached</code> <code>staticmethod</code>","text":"<p>Returns a Definitions object from a dbt project.</p> Source code in <code>data_platform\\lib\\dbt\\__init__.py</code> <pre><code>@cache\n@staticmethod\ndef build_definitions(dbt: Callable[[], DbtProject]) -&gt; dg.Definitions:\n    \"\"\"Returns a Definitions object from a dbt project.\"\"\"\n\n    assets = [\n        DagsterDbtFactory._get_assets(\n            \"dbt_partitioned_models\",\n            dbt=dbt,\n            select=TIME_PARTITION_SELECTOR,\n            partitioned=True,\n        ),\n        DagsterDbtFactory._get_assets(\n            \"dbt_non_partitioned_models\",\n            dbt=dbt,\n            exclude=TIME_PARTITION_SELECTOR,\n            partitioned=False,\n        ),\n    ]\n\n    freshness_checks = build_freshness_checks_from_dbt_assets(dbt_assets=assets)\n    freshness_sensor = dg.build_sensor_for_freshness_checks(\n        freshness_checks=freshness_checks, name=\"dbt_freshness_checks_sensor\"\n    )\n\n    return dg.Definitions(\n        resources={\n            \"dbt\": DbtCliResource(\n                project_dir=dbt(),\n            )\n        },\n        assets=assets,\n        asset_checks=freshness_checks,\n        sensors=[freshness_sensor],\n    )\n</code></pre>"},{"location":"dagster/lib/dbt/dbt/#data_platform.lib.dbt.DbtConfig","title":"<code>DbtConfig</code>","text":"<p>               Bases: <code>Config</code></p> <p>Exposes configuration options to end users in the Dagster launchpad.</p> Source code in <code>data_platform\\lib\\dbt\\__init__.py</code> <pre><code>class DbtConfig(dg.Config):\n    \"\"\"Exposes configuration options to end users in the Dagster\n    launchpad.\n    \"\"\"\n\n    full_refresh: bool = False\n    defer_to_prod: bool = defer_to_prod\n    favor_state: bool = False\n</code></pre>"},{"location":"dagster/lib/dbt/translator/","title":"Translator","text":""},{"location":"dagster/lib/dbt/translator/#data_platform.lib.dbt.translator.CustomDagsterDbtTranslator","title":"<code>CustomDagsterDbtTranslator</code>","text":"<p>               Bases: <code>DagsterDbtTranslator</code></p> <p>Overrides methods of the standard translator.</p> <p>Holds a set of methods that derive Dagster asset definition metadata given a representation of a dbt resource (models, tests, sources, etc). Methods are overriden to customize the implementation.</p> <p>See parent class for details on the purpose of each override</p> Source code in <code>data_platform\\lib\\dbt\\translator.py</code> <pre><code>class CustomDagsterDbtTranslator(DagsterDbtTranslator):\n    \"\"\"Overrides methods of the standard translator.\n\n    Holds a set of methods that derive Dagster asset definition metadata given\n    a representation of a dbt resource (models, tests, sources, etc).\n    Methods are overriden to customize the implementation.\n\n    See parent class for details on the purpose of each override\"\"\"\n\n    @override\n    def get_asset_key(self, dbt_resource_props: Mapping[str, Any]) -&gt; dg.AssetKey:\n        meta = dbt_resource_props.get(\"config\", {}).get(\n            \"meta\", {}\n        ) or dbt_resource_props.get(\"meta\", {})\n        meta_dagster = meta.get(\"dagster\") or {}\n        asset_key_config = meta_dagster.get(\"asset_key\")\n        if asset_key_config:\n            return dg.AssetKey(asset_key_config)\n\n        prop_key = \"name\"\n        if dbt_resource_props.get(\"version\"):\n            prop_key = \"alias\"\n\n        if dbt_resource_props[\"resource_type\"] == \"source\":\n            schema = dbt_resource_props[\"source_name\"]\n            table = dbt_resource_props[\"name\"]\n            step = \"raw\"\n            return dg.AssetKey([schema, step, table])\n\n        parsed_name = re.search(\"(.*?)_(.*)__(.*)\", dbt_resource_props[prop_key])\n        if parsed_name:\n            schema = parsed_name.group(2)\n            table = parsed_name.group(3)\n            step = parsed_name.group(1)\n            return dg.AssetKey([schema, step, table])\n\n        return super().get_asset_key(dbt_resource_props)\n\n    @override\n    def get_group_name(self, dbt_resource_props: Mapping[str, Any]) -&gt; str | None:\n        prop_key = \"name\"\n        if dbt_resource_props.get(\"version\"):\n            prop_key = \"alias\"\n        parsed_name = re.search(\"(.*?)_(.*)__(.*)\", dbt_resource_props[prop_key])\n        if parsed_name:\n            schema = parsed_name.group(2)\n            return schema\n\n        return super().get_group_name(dbt_resource_props)\n\n    @override\n    def get_partitions_def(\n        self, dbt_resource_props: Mapping[str, Any]\n    ) -&gt; dg.PartitionsDefinition | None:\n        meta = dbt_resource_props.get(\"config\", {}).get(\"meta\", {}).get(\"dagster\", {})\n        return get_partitions_def_from_meta(meta)\n\n    @override\n    def get_automation_condition(\n        self, dbt_resource_props: Mapping[str, Any]\n    ) -&gt; dg.AutomationCondition | None:\n        meta = dbt_resource_props.get(\"config\", {}).get(\"meta\", {}).get(\"dagster\", {})\n        automation_condition = get_automation_condition_from_meta(meta)\n        if automation_condition:\n            return automation_condition\n\n        # default settings for resource types\n        resource_type = dbt_resource_props.get(\"resource_type\")\n        if resource_type == \"snapshot\":\n            return CustomAutomationCondition.eager_with_deps_checks()\n\n        if resource_type == \"seed\":\n            return CustomAutomationCondition.code_version_changed()\n\n        else:\n            return CustomAutomationCondition.lazy()\n\n    @override\n    def get_tags(self, dbt_resource_props: Mapping[str, Any]) -&gt; Mapping[str, str]:\n        tags = super().get_tags(dbt_resource_props)\n        return tags\n</code></pre>"},{"location":"dagster/lib/dlthub/dlthub/","title":"Dlthub","text":""},{"location":"dagster/lib/dlthub/dlthub/#data_platform.lib.dlthub.ConfigurableDltResource","title":"<code>ConfigurableDltResource</code>","text":"<p>               Bases: <code>DltResource</code></p> <p>Wrapper class to add aditional attributes to the DltResource class.  These attributes are used in the factory to add aditional configuration such as automation conditions, asset checks, tags, and upstream external assets.</p> Source code in <code>data_platform\\lib\\dlthub\\__init__.py</code> <pre><code>class ConfigurableDltResource(DltResource):\n    \"\"\"Wrapper class to add aditional attributes to the DltResource class.  These\n    attributes are used in the factory to add aditional configuration such as automation\n    conditions, asset checks, tags, and upstream external assets.\n    \"\"\"\n\n    meta: dict | None\n    tags: list | None\n    kinds: set | None\n\n    @staticmethod\n    def config(\n        resource: DltResource,\n        meta: dict | None = None,\n        tags: list[str] | None = None,\n        kinds: set[str] | None = None,\n    ) -&gt; \"ConfigurableDltResource\":\n        \"\"\"Returns a ConfigurableDltResource wrapped DltResource with aditional\n        attributes used by the factory to generate enhanced definitions.\n        \"\"\"\n        resource = ConfigurableDltResource._convert(resource, meta, tags, kinds)\n        return resource  # type: ignore\n\n    @staticmethod\n    def _convert(\n        dlt_resource: DltResource,\n        meta: dict | None,\n        tags: list[str] | None,\n        kinds: set[str] | None,\n    ) -&gt; \"ConfigurableDltResource\":\n        \"\"\"Helper method to wrap a DltResource\"\"\"\n        dlt_resource.tags = tags  # type: ignore\n        dlt_resource.meta = meta  # type: ignore\n        dlt_resource.kinds = kinds  # type: ignore\n        return dlt_resource  # type: ignore\n</code></pre>"},{"location":"dagster/lib/dlthub/dlthub/#data_platform.lib.dlthub.ConfigurableDltResource.config","title":"<code>config(resource, meta=None, tags=None, kinds=None)</code>  <code>staticmethod</code>","text":"<p>Returns a ConfigurableDltResource wrapped DltResource with aditional attributes used by the factory to generate enhanced definitions.</p> Source code in <code>data_platform\\lib\\dlthub\\__init__.py</code> <pre><code>@staticmethod\ndef config(\n    resource: DltResource,\n    meta: dict | None = None,\n    tags: list[str] | None = None,\n    kinds: set[str] | None = None,\n) -&gt; \"ConfigurableDltResource\":\n    \"\"\"Returns a ConfigurableDltResource wrapped DltResource with aditional\n    attributes used by the factory to generate enhanced definitions.\n    \"\"\"\n    resource = ConfigurableDltResource._convert(resource, meta, tags, kinds)\n    return resource  # type: ignore\n</code></pre>"},{"location":"dagster/lib/dlthub/translator/","title":"Translator","text":""},{"location":"dagster/lib/dlthub/translator/#data_platform.lib.dlthub.translator.CustomDagsterDltTranslator","title":"<code>CustomDagsterDltTranslator</code>","text":"<p>               Bases: <code>DagsterDltTranslator</code></p> <p>Overrides methods of the standard translator.</p> <p>Holds a set of methods that derive Dagster asset definition metadata given a representation of dltHub resource (resources, pipes, etc).  Methods are overriden to customize the implementation.</p> <p>See parent class for details on the purpose of each override</p> Source code in <code>data_platform\\lib\\dlthub\\translator.py</code> <pre><code>class CustomDagsterDltTranslator(DagsterDltTranslator):\n    \"\"\"Overrides methods of the standard translator.\n\n    Holds a set of methods that derive Dagster asset definition metadata given a\n    representation of dltHub resource (resources, pipes, etc).  Methods are overriden to\n    customize the implementation.\n\n    See parent class for details on the purpose of each override\n    \"\"\"\n\n    @override\n    def get_asset_spec(self, data: DltResourceTranslatorData) -&gt; dg.AssetSpec:\n        return dg.AssetSpec(\n            key=self._resolve_back_compat_method(\n                \"get_asset_key\", self._default_asset_key_fn, data.resource\n            ),\n            automation_condition=self._resolve_back_compat_method(\n                \"get_automation_condition\",\n                self._default_automation_condition_fn,\n                data.resource,\n            ),\n            deps=self._resolve_back_compat_method(\n                \"get_deps_asset_keys\", self._default_deps_fn, data.resource\n            ),\n            description=self._resolve_back_compat_method(\n                \"get_description\", self._default_description_fn, data.resource\n            ),\n            group_name=self._resolve_back_compat_method(\n                \"get_group_name\", self._default_group_name_fn, data.resource\n            ),\n            metadata=self._resolve_back_compat_method(\n                \"get_metadata\", self._default_metadata_fn, data.resource\n            ),\n            owners=self._resolve_back_compat_method(\n                \"get_owners\", self._default_owners_fn, data.resource\n            ),\n            tags=self._resolve_back_compat_method(\n                \"get_tags\", self._default_tags_fn, data.resource\n            ),\n            kinds=self._resolve_back_compat_method(\n                \"get_kinds\", self._default_kinds_fn, data.resource, data.destination\n            ),\n            partitions_def=self.get_partitions_def(data.resource),\n        )\n\n    @override\n    def get_deps_asset_keys(self, resource: DltResource) -&gt; Iterable[dg.AssetKey]:\n        name: str | None = None\n        if resource.is_transformer:\n            pipe = resource._pipe\n            while pipe.has_parent:\n                pipe = pipe.parent\n                name = pipe.schema.name  # type: ignore\n        else:\n            name = resource.name\n        if name:\n            schema, table = name.split(\".\")\n            asset_key = [schema, \"src\", table]\n            return [dg.AssetKey(asset_key)]\n        return super().get_deps_asset_keys(resource)\n\n    @override\n    def get_asset_key(self, resource: DltResource) -&gt; dg.AssetKey:\n        schema, table = resource.name.split(\".\")\n        asset_key = [schema, \"raw\", table]\n        return dg.AssetKey(asset_key)\n\n    @override\n    def get_group_name(self, resource: DltResource) -&gt; str:\n        group = resource.name.split(\".\")[0]\n        return group\n\n    def get_partitions_def(\n        self, resource: DltResource\n    ) -&gt; dg.PartitionsDefinition | None:\n        try:\n            meta = resource.meta.get(\"dagster\")  # type: ignore\n            return get_partitions_def_from_meta(meta)\n        except Exception:\n            ...\n        return None\n\n    @override\n    def get_automation_condition(\n        self, resource: DltResource\n    ) -&gt; dg.AutomationCondition[Any] | None:\n        try:\n            meta = resource.meta.get(\"dagster\")  # type: ignore\n            automation_condition = get_automation_condition_from_meta(meta)\n            if automation_condition:\n                return automation_condition\n        except Exception:\n            ...\n        return super().get_automation_condition(resource)\n\n    @override\n    def get_tags(self, resource: DltResource) -&gt; Mapping[str, Any]:\n        try:\n            tags = resource.tags  # type: ignore\n            return {tag: \"\" for tag in tags if is_valid_tag_key(tag)}\n        except Exception:\n            ...\n        return {}\n</code></pre>"},{"location":"dagster/lib/sling/sling/","title":"Sling","text":""},{"location":"dagster/lib/sling/sling/#data_platform.lib.sling.DagsterSlingFactory","title":"<code>DagsterSlingFactory</code>","text":"<p>Factory to generate dagster definitions from Sling yaml config files.</p> Source code in <code>data_platform\\lib\\sling\\__init__.py</code> <pre><code>class DagsterSlingFactory:\n    \"\"\"Factory to generate dagster definitions from Sling yaml config files.\"\"\"\n\n    @cache\n    @staticmethod\n    def build_definitions(config_dir: Path) -&gt; dg.Definitions:\n        \"\"\"Returns a Definitions object for a path that contains Sling yaml configs.\"\"\"\n        connections = []\n        assets = []\n        freshness_checks = []\n        kind_map = {}\n\n        for config_path in os.listdir(config_dir):\n            if config_path.endswith(\".yaml\") or config_path.endswith(\".yml\"):\n                config_path = config_dir.joinpath(config_path).resolve()\n                with open(config_path) as file:\n                    config = yaml.load(file, Loader=yaml.FullLoader)\n                if not config:\n                    continue\n\n                if connection_configs := config.get(\"connections\"):\n                    connections, kind_map = DagsterSlingFactory._get_connections(\n                        connection_configs, connections, kind_map\n                    )\n\n                if replication_configs := config.get(\"replications\"):\n                    assets, freshness_checks = DagsterSlingFactory._get_replications(\n                        replication_configs, freshness_checks, kind_map, assets\n                    )\n\n        return dg.Definitions(\n            resources={\"sling\": SlingResource(connections=connections)},\n            assets=assets,\n            asset_checks=freshness_checks,\n            sensors=[\n                dg.build_sensor_for_freshness_checks(\n                    freshness_checks=freshness_checks,\n                    name=\"sling_freshness_checks_sensor\",\n                )\n            ],\n        )\n\n    @staticmethod\n    def _get_connections(\n        connection_configs, connections, kind_map\n    ) -&gt; tuple[list[SlingConnectionResource], dict[str, str]]:\n        \"\"\"Returns a list of SlingConnectionResource for connections in the Sling\n        yaml file.\n        \"\"\"\n        for connection_config in connection_configs:\n            if connection := DagsterSlingFactory._get_connection(connection_config):\n                source = connection_config.get(\"name\")\n                kind = connection_config.get(\"type\")\n                kind_map[source] = kind\n                connections.append(connection)\n\n        return connections, kind_map\n\n    @staticmethod\n    def _get_connection(connection_config: dict) -&gt; SlingConnectionResource | None:\n        \"\"\"Returns a SlingConnectionResource for a connection in the Sling yaml file.\"\"\"\n        for k, v in connection_config.items():\n            if isinstance(v, dict):\n                secret_name = list(v.keys())[0]\n                display_type = list(v.values())[0]\n\n                if display_type == \"show\":\n                    connection_config[k] = get_secret(secret_name).get_value()\n                else:\n                    connection_config[k] = get_secret(secret_name)\n\n        connection = SlingConnectionResource(**connection_config)\n        return connection\n\n    @staticmethod\n    def _get_replications(\n        replication_configs, freshness_checks, kind_map, assets\n    ) -&gt; tuple[list[dg.AssetsDefinition], list[dg.AssetChecksDefinition]]:\n        \"\"\"Returns a list of AssetsDefinitions for\n        replications in a Sling yaml file\n        \"\"\"\n        for replication_config in replication_configs:\n            if bool(os.getenv(\"DAGSTER_IS_DEV_CLI\")):\n                replication_config = DagsterSlingFactory._set_dev_schema(\n                    replication_config\n                )\n            assets_definition = DagsterSlingFactory._get_replication(replication_config)\n\n            kind = kind_map.get(replication_config.get(\"source\", None), None)\n            dep_asset_specs = DagsterSlingFactory._get_sling_deps(\n                replication_config, kind\n            )\n            asset_freshness_checks = DagsterSlingFactory._get_freshness_checks(\n                replication_config\n            )\n\n            if asset_freshness_checks:\n                freshness_checks.extend(asset_freshness_checks)\n            if assets_definition:\n                assets.append(assets_definition)\n            if dep_asset_specs:\n                assets.extend(dep_asset_specs)\n\n        return assets, freshness_checks\n\n    @staticmethod\n    def _get_replication(config: dict) -&gt; dg.AssetsDefinition:\n        \"\"\"Returns a AssetsDefinition for replication\n        in a Sling yaml file\n        \"\"\"\n\n        @sling_assets(\n            name=config[\"source\"] + \"_assets\",\n            replication_config=config,\n            backfill_policy=dg.BackfillPolicy.single_run(),\n            dagster_sling_translator=CustomDagsterSlingTranslator(),\n            pool=\"sling\",\n        )\n        def assets(\n            context: dg.AssetExecutionContext, sling: SlingResource\n        ) -&gt; Generator[SlingEventType, Any, None]:\n            if \"defaults\" not in config:\n                config[\"defaults\"] = {}\n\n            try:  # to inject start and end dates for partitioned runs\n                time_window = context.partition_time_window\n                if time_window:\n                    if \"source_options\" not in config[\"defaults\"]:\n                        config[\"defaults\"][\"source_options\"] = {}\n\n                    format = \"%Y-%m-%d %H:%M:%S\"\n                    start = time_window.start.strftime(format)\n                    end = time_window.end.strftime(format)\n                    config[\"defaults\"][\"source_options\"][\"range\"] = f\"{start},{end}\"\n            except Exception:  # run normal run if time window not provided\n                pass\n\n            yield from sling.replicate(\n                context=context,\n                replication_config=config,\n                dagster_sling_translator=CustomDagsterSlingTranslator(),\n            )\n            for row in sling.stream_raw_logs():\n                context.log.info(row)\n\n        return assets\n\n    @staticmethod\n    def _set_dev_schema(replication_config: dict) -&gt; dict:\n        \"\"\"Override the desination schema set in the yaml file when the environment\n        is set to dev to point to a unique schema based on the developer.\n        \"\"\"\n        user = os.environ[\"DESTINATION__SNOWFLAKE__CREDENTIALS__USERNAME\"].upper()\n        if default_object := replication_config[\"defaults\"][\"object\"]:\n            schema, table = default_object.split(\".\")\n            replication_config[\"defaults\"][\"object\"] = f\"{schema}__{user}.{table}\"\n\n        for stream, stream_config in list(\n            replication_config.get(\"streams\", {}).items()\n        ):\n            stream_config = stream_config or {}\n            if stream_object := stream_config.get(\"object\"):\n                schema, table = stream_object.split(\".\")\n                replication_config[\"streams\"][stream][\"object\"] = (\n                    f\"{schema}__{user}.{table}\"\n                )\n\n        return replication_config\n\n    @staticmethod\n    def _get_sling_deps(\n        replication_config: dict, kind: str | None\n    ) -&gt; list[dg.AssetSpec] | None:\n        \"\"\"Create an external asset that is placed in the same prefix\n        as the asset, and assigned the correct resource kind.\n        \"\"\"\n        kinds = {kind} if kind else None\n\n        deps = []\n        for k in replication_config[\"streams\"]:\n            schema, table = k.split(\".\")\n            dep = dg.AssetSpec(\n                key=[schema, \"src\", table], group_name=schema, kinds=kinds\n            )\n            deps.append(dep)\n        return deps\n\n    @staticmethod\n    def _get_freshness_checks(\n        replication_config: dict,\n    ) -&gt; list[dg.AssetChecksDefinition]:\n        \"\"\"Returns a list of AssetChecksDefinition for replication configs.\n        Configs supplied on the stream will take priority, otherwise the\n        default will be used.\n        \"\"\"\n        freshness_checks = []\n\n        default_freshness_check_config = (\n            get_nested(\n                replication_config, [\"defaults\", \"meta\", \"dagster\", \"freshness_check\"]\n            )\n            or {}\n        )\n        default_partition = get_nested(\n            replication_config, [\"defaults\", \"meta\", \"dagster\", \"partition\"]\n        )\n\n        streams = replication_config.get(\"streams\", {})\n        for stream_name, steam_config in streams.items():\n            freshness_check_config = (\n                get_nested(steam_config, [\"meta\", \"dagster\", \"freshness_check\"]) or {}\n            )\n            partition = get_nested(steam_config, [\"meta\", \"dagster\", \"partition\"])\n\n            freshness_check_config = (\n                freshness_check_config | default_freshness_check_config\n            )\n            partition = partition or default_partition\n\n            if freshness_check_config:\n                if lower_bound_delta_seconds := freshness_check_config.pop(\n                    \"lower_bound_delta_seconds\", None\n                ):\n                    lower_bound_delta = timedelta(\n                        seconds=float(lower_bound_delta_seconds)\n                    )\n                    freshness_check_config[\"lower_bound_delta\"] = lower_bound_delta\n\n                schema, table_name = stream_name.split(\".\")\n                asset_key = [schema, \"raw\", table_name]\n                freshness_check_config[\"assets\"] = [asset_key]\n\n                try:\n                    if partition in [\"hourly\", \"daily\", \"weekly\", \"monthly\"]:\n                        freshness_check_config = sanitize_input_signature(\n                            dg.build_time_partition_freshness_checks,\n                            freshness_check_config,\n                        )\n\n                        time_partition_update_freshness_checks = (\n                            dg.build_time_partition_freshness_checks(\n                                **freshness_check_config\n                            )\n                        )\n                        freshness_checks.extend(time_partition_update_freshness_checks)\n\n                    else:\n                        freshness_check_config = sanitize_input_signature(\n                            dg.build_last_update_freshness_checks,\n                            freshness_check_config,\n                        )\n\n                        last_update_freshness_checks = (\n                            dg.build_last_update_freshness_checks(\n                                **freshness_check_config\n                            )\n                        )\n                        freshness_checks.extend(last_update_freshness_checks)\n                except TypeError as e:\n                    raise TypeError(\n                        \"Error creating freshness check, check your configuration for \"\n                        f\"'{asset_key}'. Supplied arguments: {freshness_check_config}\"\n                    ) from e\n\n        return freshness_checks\n</code></pre>"},{"location":"dagster/lib/sling/sling/#data_platform.lib.sling.DagsterSlingFactory.build_definitions","title":"<code>build_definitions(config_dir)</code>  <code>cached</code> <code>staticmethod</code>","text":"<p>Returns a Definitions object for a path that contains Sling yaml configs.</p> Source code in <code>data_platform\\lib\\sling\\__init__.py</code> <pre><code>@cache\n@staticmethod\ndef build_definitions(config_dir: Path) -&gt; dg.Definitions:\n    \"\"\"Returns a Definitions object for a path that contains Sling yaml configs.\"\"\"\n    connections = []\n    assets = []\n    freshness_checks = []\n    kind_map = {}\n\n    for config_path in os.listdir(config_dir):\n        if config_path.endswith(\".yaml\") or config_path.endswith(\".yml\"):\n            config_path = config_dir.joinpath(config_path).resolve()\n            with open(config_path) as file:\n                config = yaml.load(file, Loader=yaml.FullLoader)\n            if not config:\n                continue\n\n            if connection_configs := config.get(\"connections\"):\n                connections, kind_map = DagsterSlingFactory._get_connections(\n                    connection_configs, connections, kind_map\n                )\n\n            if replication_configs := config.get(\"replications\"):\n                assets, freshness_checks = DagsterSlingFactory._get_replications(\n                    replication_configs, freshness_checks, kind_map, assets\n                )\n\n    return dg.Definitions(\n        resources={\"sling\": SlingResource(connections=connections)},\n        assets=assets,\n        asset_checks=freshness_checks,\n        sensors=[\n            dg.build_sensor_for_freshness_checks(\n                freshness_checks=freshness_checks,\n                name=\"sling_freshness_checks_sensor\",\n            )\n        ],\n    )\n</code></pre>"},{"location":"dagster/lib/sling/translator/","title":"Translator","text":""},{"location":"dagster/lib/sling/translator/#data_platform.lib.sling.translator.CustomDagsterSlingTranslator","title":"<code>CustomDagsterSlingTranslator</code>","text":"<p>               Bases: <code>DagsterSlingTranslator</code></p> <p>Overrides methods of the standard translator.</p> <p>Holds a set of methods that derive Dagster asset definition metadata given a representation of Sling resource (connections, replications). Methods are overriden to customize the implementation.</p> <p>See parent class for details on the purpose of each override</p> Source code in <code>data_platform\\lib\\sling\\translator.py</code> <pre><code>class CustomDagsterSlingTranslator(dg_sling.DagsterSlingTranslator):\n    \"\"\"Overrides methods of the standard translator.\n\n    Holds a set of methods that derive Dagster asset definition metadata given a\n    representation of Sling resource (connections, replications). Methods are overriden\n    to customize the implementation.\n\n    See parent class for details on the purpose of each override\"\"\"\n\n    @override\n    def get_asset_spec(self, stream_definition: Mapping[str, Any]) -&gt; dg.AssetSpec:\n        return dg.AssetSpec(\n            automation_condition=self.get_automation_condition(stream_definition),\n            partitions_def=self.get_partitions_def(stream_definition),\n            key=self._resolve_back_compat_method(\n                \"get_asset_key\", self._default_asset_key_fn, stream_definition\n            ),\n            deps=self._resolve_back_compat_method(\n                \"get_deps_asset_key\", self._default_deps_fn, stream_definition\n            ),\n            description=self._resolve_back_compat_method(\n                \"get_description\", self._default_description_fn, stream_definition\n            ),\n            metadata=self._resolve_back_compat_method(\n                \"get_metadata\", self._default_metadata_fn, stream_definition\n            ),\n            tags=self._resolve_back_compat_method(\n                \"get_tags\", self._default_tags_fn, stream_definition\n            ),\n            kinds=self._resolve_back_compat_method(\n                \"get_kinds\", self._default_kinds_fn, stream_definition\n            ),\n            group_name=self._resolve_back_compat_method(\n                \"get_group_name\", self._default_group_name_fn, stream_definition\n            ),\n            legacy_freshness_policy=self._resolve_back_compat_method(\n                \"get_freshness_policy\",\n                self._default_freshness_policy_fn,\n                stream_definition,\n            ),\n            auto_materialize_policy=self._resolve_back_compat_method(\n                \"get_auto_materialize_policy\",\n                self._default_auto_materialize_policy_fn,\n                stream_definition,\n            ),\n        )\n\n    @override\n    def get_asset_key(self, stream_definition: Mapping[str, Any]) -&gt; dg.AssetKey:\n        config = stream_definition.get(\"config\") or {}\n        meta = config.get(\"meta\") or {}\n        dagster = meta.get(\"dagster\") or {}\n        asset_key = dagster.get(\"asset_key\", None)\n\n        if asset_key:\n            if self.sanitize_stream_name(asset_key) != asset_key:\n                raise ValueError(\n                    f\"Asset key {asset_key} for stream {stream_definition['name']} \"\n                    \"is not sanitized. Please use only alphanumeric characters \"\n                    \"and underscores.\"\n                )\n            return dg.AssetKey(asset_key.split(\".\"))\n\n        # You can override the Sling Replication default object with an object key\n        stream_name = stream_definition[\"name\"]\n        schema, table = self.sanitize_stream_name(stream_name).split(\".\")\n        return dg.AssetKey([schema, \"raw\", table])\n\n    @override\n    def get_deps_asset_key(\n        self, stream_definition: Mapping[str, Any]\n    ) -&gt; Iterable[dg.AssetKey]:\n        config = stream_definition.get(\"config\", {}) or {}\n        meta = config.get(\"meta\", {}) or {}\n        deps = meta.get(\"dagster\", {}).get(\"deps\")\n        deps_out = []\n        if deps and isinstance(deps, str):\n            deps = [deps]\n        if deps:\n            assert isinstance(deps, list)\n            for asset_key in deps:\n                if self.sanitize_stream_name(asset_key) != asset_key:\n                    raise ValueError(\n                        f\"Deps Asset key {asset_key} for stream  \"\n                        f\"{stream_definition['name']} is not sanitized. \"\n                        \"Please use only alphanumeric characters and underscores.\"\n                    )\n                deps_out.append(dg.AssetKey(asset_key.split(\".\")))\n            return deps_out\n\n        stream_name = stream_definition[\"name\"]\n        schema, table = self.sanitize_stream_name(stream_name).split(\".\")\n        return [dg.AssetKey([schema, \"src\", table])]\n\n    @override\n    def get_group_name(self, stream_definition: Mapping[str, Any]) -&gt; str:\n        try:\n            group = stream_definition[\"config\"][\"meta\"][\"dagster\"][\"group\"]\n            if group:\n                return group\n        except Exception:\n            ...\n\n        stream_name = stream_definition[\"name\"]\n        schema, _ = self.sanitize_stream_name(stream_name).split(\".\")\n        return schema\n\n    @override\n    def get_tags(self, stream_definition: Mapping[str, Any]) -&gt; Mapping[str, Any]:\n        try:\n            tags = stream_definition[\"config\"][\"meta\"][\"dagster\"][\"tags\"]\n            return {tag: \"\" for tag in tags if is_valid_tag_key(tag)}\n        except Exception:\n            ...\n        return {}\n\n    def get_automation_condition(\n        self, stream_definition: Mapping[str, Any]\n    ) -&gt; None | dg.AutomationCondition:\n        try:\n            meta = stream_definition[\"config\"][\"meta\"][\"dagster\"]\n            automation_condition = get_automation_condition_from_meta(meta)\n            return automation_condition\n        except Exception:\n            ...\n        return None\n\n    def get_partitions_def(\n        self, stream_definition: Mapping[str, Any]\n    ) -&gt; None | dg.PartitionsDefinition:\n        try:\n            meta = stream_definition[\"config\"][\"meta\"][\"dagster\"]\n            automation_condition = get_partitions_def_from_meta(meta)\n            return automation_condition\n        except Exception:\n            ...\n        return None\n</code></pre>"},{"location":"dagster/utils/automation_conditions/","title":"Automation conditions","text":""},{"location":"dagster/utils/automation_conditions/#data_platform.utils.automation_conditions.CustomAutomationCondition","title":"<code>CustomAutomationCondition</code>","text":"<p>               Bases: <code>AutomationCondition</code></p> Source code in <code>data_platform\\utils\\automation_conditions.py</code> <pre><code>class CustomAutomationCondition(AutomationCondition):\n    @classmethod\n    def get_automation_condition(\n        cls, automation_condition_name: str\n    ) -&gt; AutomationCondition | None:\n        methods = AutomationCondition.__dict__ | cls.__dict__\n        return methods.get(automation_condition_name, None)\n\n    @staticmethod\n    def manual() -&gt; None:\n        \"\"\"Returns no AutomationCondition that will require a user to manually trigger.\n        Used for overriding default automations for static assets.\n        \"\"\"\n        return None\n\n    @staticmethod\n    def missing_or_changed() -&gt; AutomationCondition:\n        \"\"\"Returns no AutomationCondition that will trigger only if the asset has never\n        been materialized, or if its definition has changed.\n\n        Common use for dbt seeds that only need to be reloaded when the underlying csv\n        file changes.\n        \"\"\"\n        return (\n            AutomationCondition.in_latest_time_window()\n            &amp; (\n                AutomationCondition.code_version_changed()\n                | AutomationCondition.newly_missing()\n            ).since_last_handled()\n            &amp; ~AutomationCondition.in_progress()\n        ).with_label(\"missing_or_changed\")\n\n    @override\n    @staticmethod\n    def eager() -&gt; AndAutomationCondition:\n        \"\"\"Returns an AutomationCondition which will cause a target to be executed if\n        any of its dependencies update, and will execute missing partitions if they\n        become missing after this condition is applied to the target. This will not\n        execute targets that have any missing or in progress dependencies, or are\n        currently in progress.\n\n        For time partitioned assets, only the latest time partition will be considered.\n        Commonly used for assets that are far downstream and have users that directly\n        interact with them, and do not have sensitivity to late arriving dimensions.\n        \"\"\"\n        return (\n            AutomationCondition.in_latest_time_window()\n            &amp; (\n                AutomationCondition.newly_missing()\n                | AutomationCondition.any_deps_updated()\n            ).since_last_handled()\n            &amp; ~AutomationCondition.any_deps_missing()\n            &amp; ~AutomationCondition.any_deps_in_progress()\n            &amp; ~AutomationCondition.in_progress()\n        ).with_label(\"eager\")\n\n    @staticmethod\n    def eager_with_deps_checks() -&gt; AutomationCondition:\n        \"\"\"Returns an AutomationCondition which will cause a target\n        to be executed if any of its dependencies update but only after,\n        the dependencies blocking checks have passed, and will\n        execute missing partitions if they become missing after this\n        condition is applied to the target. This will not execute targets\n        that have any missing or in progress dependencies,\n        or are currently in progress.\n\n        For time partitioned assets, only the latest time partition will be considered.\n        Commonly used for assets that are far downstream and have users that directly\n        interact with them, and do not have sensitivity to late arriving dimensions.\n        \"\"\"\n        return (\n            AutomationCondition.eager()\n            &amp; AutomationCondition.all_deps_blocking_checks_passed()\n        )\n\n    @classmethod\n    def lazy(cls) -&gt; AutomationCondition:\n        \"\"\"Returns an AutomationCondition which will cause a target to be executed if\n        any downstream conditions are true or the partition is missing or changed.\n\n        Commonly used for intermediate assets that are used for downstream\n        materializations.\n        \"\"\"\n        return (\n            AutomationCondition.any_downstream_conditions() | cls.missing_or_changed()\n        ).with_label(\"lazy\")\n\n    @staticmethod\n    def lazy_on_cron(\n        cron_schedule: str,\n        cron_timezone: str = \"UTC\",\n        ignore_asset_keys: list[list[str]] | None = None,\n    ) -&gt; AutomationCondition:\n        \"\"\"Returns an AutomationCondition which will cause a target to be\n        executed if any downstream conditions are true or the partition is missing or\n        changed. Will limit to only one execution for the given cron_schedule.\n\n        Commonly used for intermediate assets that are used for downstream\n        materializations, that have high frequency upstream assets, but themselves do\n        not need to be updated as frequently.\n        \"\"\"\n        ignore_asset_keys = ignore_asset_keys or []\n        return (\n            AutomationCondition.in_latest_time_window()\n            &amp; AutomationCondition.cron_tick_passed(\n                cron_schedule, cron_timezone\n            ).since_last_handled()\n            &amp; AutomationCondition.all_deps_updated_since_cron(\n                cron_schedule, cron_timezone\n            ).ignore(AssetSelection.assets(*ignore_asset_keys))\n            &amp; ~AutomationCondition.in_progress()\n        ).with_label(f\"lazy_on_cron({cron_schedule}, {cron_timezone})\")\n\n    @staticmethod\n    @override\n    def on_cron(\n        cron_schedule: str,\n        cron_timezone: str = \"UTC\",\n        ignore_asset_keys: list[list[str]] | None = None,\n    ) -&gt; AndAutomationCondition:\n        \"\"\"Returns an AutomationCondition which will cause a target to be executed on a\n        given cron schedule, after all of its dependencies have been updated since the\n        latest tick of that cron schedule.\n\n        For time partitioned assets, only the latest time partition will be considered.\n\n        Commonly used for assets that are far downstream and have users that directly\n        interact with them, and have sensitivity to late arriving dimensions.\n        \"\"\"\n        ignore_asset_keys = ignore_asset_keys or []\n        return AutomationCondition.on_cron(cron_schedule, cron_timezone).ignore(\n            AssetSelection.assets(*ignore_asset_keys)\n        )\n\n    @staticmethod\n    def on_schedule(\n        cron_schedule: str, cron_timezone: str = \"utc\"\n    ) -&gt; AutomationCondition:\n        \"\"\"Returns an AutomationCondition which will cause a target to be executed on a\n        given cron schedule, regardless of the state of its dependencies\n\n        For time partitioned assets, only the latest time partition will be considered.\n\n        Commonly used for assets in the ingestion layer that should always run on\n        a scheduled basis, and have no way of knowing when the source system has\n        updates.\n        \"\"\"\n        return (\n            AutomationCondition.in_latest_time_window()\n            &amp; AutomationCondition.cron_tick_passed(\n                cron_schedule, cron_timezone\n            ).since_last_handled()\n        ).with_label(f\"on_schedule({cron_schedule}, {cron_timezone})\")\n</code></pre>"},{"location":"dagster/utils/automation_conditions/#data_platform.utils.automation_conditions.CustomAutomationCondition.eager","title":"<code>eager()</code>  <code>staticmethod</code>","text":"<p>Returns an AutomationCondition which will cause a target to be executed if any of its dependencies update, and will execute missing partitions if they become missing after this condition is applied to the target. This will not execute targets that have any missing or in progress dependencies, or are currently in progress.</p> <p>For time partitioned assets, only the latest time partition will be considered. Commonly used for assets that are far downstream and have users that directly interact with them, and do not have sensitivity to late arriving dimensions.</p> Source code in <code>data_platform\\utils\\automation_conditions.py</code> <pre><code>@override\n@staticmethod\ndef eager() -&gt; AndAutomationCondition:\n    \"\"\"Returns an AutomationCondition which will cause a target to be executed if\n    any of its dependencies update, and will execute missing partitions if they\n    become missing after this condition is applied to the target. This will not\n    execute targets that have any missing or in progress dependencies, or are\n    currently in progress.\n\n    For time partitioned assets, only the latest time partition will be considered.\n    Commonly used for assets that are far downstream and have users that directly\n    interact with them, and do not have sensitivity to late arriving dimensions.\n    \"\"\"\n    return (\n        AutomationCondition.in_latest_time_window()\n        &amp; (\n            AutomationCondition.newly_missing()\n            | AutomationCondition.any_deps_updated()\n        ).since_last_handled()\n        &amp; ~AutomationCondition.any_deps_missing()\n        &amp; ~AutomationCondition.any_deps_in_progress()\n        &amp; ~AutomationCondition.in_progress()\n    ).with_label(\"eager\")\n</code></pre>"},{"location":"dagster/utils/automation_conditions/#data_platform.utils.automation_conditions.CustomAutomationCondition.eager_with_deps_checks","title":"<code>eager_with_deps_checks()</code>  <code>staticmethod</code>","text":"<p>Returns an AutomationCondition which will cause a target to be executed if any of its dependencies update but only after, the dependencies blocking checks have passed, and will execute missing partitions if they become missing after this condition is applied to the target. This will not execute targets that have any missing or in progress dependencies, or are currently in progress.</p> <p>For time partitioned assets, only the latest time partition will be considered. Commonly used for assets that are far downstream and have users that directly interact with them, and do not have sensitivity to late arriving dimensions.</p> Source code in <code>data_platform\\utils\\automation_conditions.py</code> <pre><code>@staticmethod\ndef eager_with_deps_checks() -&gt; AutomationCondition:\n    \"\"\"Returns an AutomationCondition which will cause a target\n    to be executed if any of its dependencies update but only after,\n    the dependencies blocking checks have passed, and will\n    execute missing partitions if they become missing after this\n    condition is applied to the target. This will not execute targets\n    that have any missing or in progress dependencies,\n    or are currently in progress.\n\n    For time partitioned assets, only the latest time partition will be considered.\n    Commonly used for assets that are far downstream and have users that directly\n    interact with them, and do not have sensitivity to late arriving dimensions.\n    \"\"\"\n    return (\n        AutomationCondition.eager()\n        &amp; AutomationCondition.all_deps_blocking_checks_passed()\n    )\n</code></pre>"},{"location":"dagster/utils/automation_conditions/#data_platform.utils.automation_conditions.CustomAutomationCondition.lazy","title":"<code>lazy()</code>  <code>classmethod</code>","text":"<p>Returns an AutomationCondition which will cause a target to be executed if any downstream conditions are true or the partition is missing or changed.</p> <p>Commonly used for intermediate assets that are used for downstream materializations.</p> Source code in <code>data_platform\\utils\\automation_conditions.py</code> <pre><code>@classmethod\ndef lazy(cls) -&gt; AutomationCondition:\n    \"\"\"Returns an AutomationCondition which will cause a target to be executed if\n    any downstream conditions are true or the partition is missing or changed.\n\n    Commonly used for intermediate assets that are used for downstream\n    materializations.\n    \"\"\"\n    return (\n        AutomationCondition.any_downstream_conditions() | cls.missing_or_changed()\n    ).with_label(\"lazy\")\n</code></pre>"},{"location":"dagster/utils/automation_conditions/#data_platform.utils.automation_conditions.CustomAutomationCondition.lazy_on_cron","title":"<code>lazy_on_cron(cron_schedule, cron_timezone='UTC', ignore_asset_keys=None)</code>  <code>staticmethod</code>","text":"<p>Returns an AutomationCondition which will cause a target to be executed if any downstream conditions are true or the partition is missing or changed. Will limit to only one execution for the given cron_schedule.</p> <p>Commonly used for intermediate assets that are used for downstream materializations, that have high frequency upstream assets, but themselves do not need to be updated as frequently.</p> Source code in <code>data_platform\\utils\\automation_conditions.py</code> <pre><code>@staticmethod\ndef lazy_on_cron(\n    cron_schedule: str,\n    cron_timezone: str = \"UTC\",\n    ignore_asset_keys: list[list[str]] | None = None,\n) -&gt; AutomationCondition:\n    \"\"\"Returns an AutomationCondition which will cause a target to be\n    executed if any downstream conditions are true or the partition is missing or\n    changed. Will limit to only one execution for the given cron_schedule.\n\n    Commonly used for intermediate assets that are used for downstream\n    materializations, that have high frequency upstream assets, but themselves do\n    not need to be updated as frequently.\n    \"\"\"\n    ignore_asset_keys = ignore_asset_keys or []\n    return (\n        AutomationCondition.in_latest_time_window()\n        &amp; AutomationCondition.cron_tick_passed(\n            cron_schedule, cron_timezone\n        ).since_last_handled()\n        &amp; AutomationCondition.all_deps_updated_since_cron(\n            cron_schedule, cron_timezone\n        ).ignore(AssetSelection.assets(*ignore_asset_keys))\n        &amp; ~AutomationCondition.in_progress()\n    ).with_label(f\"lazy_on_cron({cron_schedule}, {cron_timezone})\")\n</code></pre>"},{"location":"dagster/utils/automation_conditions/#data_platform.utils.automation_conditions.CustomAutomationCondition.manual","title":"<code>manual()</code>  <code>staticmethod</code>","text":"<p>Returns no AutomationCondition that will require a user to manually trigger. Used for overriding default automations for static assets.</p> Source code in <code>data_platform\\utils\\automation_conditions.py</code> <pre><code>@staticmethod\ndef manual() -&gt; None:\n    \"\"\"Returns no AutomationCondition that will require a user to manually trigger.\n    Used for overriding default automations for static assets.\n    \"\"\"\n    return None\n</code></pre>"},{"location":"dagster/utils/automation_conditions/#data_platform.utils.automation_conditions.CustomAutomationCondition.missing_or_changed","title":"<code>missing_or_changed()</code>  <code>staticmethod</code>","text":"<p>Returns no AutomationCondition that will trigger only if the asset has never been materialized, or if its definition has changed.</p> <p>Common use for dbt seeds that only need to be reloaded when the underlying csv file changes.</p> Source code in <code>data_platform\\utils\\automation_conditions.py</code> <pre><code>@staticmethod\ndef missing_or_changed() -&gt; AutomationCondition:\n    \"\"\"Returns no AutomationCondition that will trigger only if the asset has never\n    been materialized, or if its definition has changed.\n\n    Common use for dbt seeds that only need to be reloaded when the underlying csv\n    file changes.\n    \"\"\"\n    return (\n        AutomationCondition.in_latest_time_window()\n        &amp; (\n            AutomationCondition.code_version_changed()\n            | AutomationCondition.newly_missing()\n        ).since_last_handled()\n        &amp; ~AutomationCondition.in_progress()\n    ).with_label(\"missing_or_changed\")\n</code></pre>"},{"location":"dagster/utils/automation_conditions/#data_platform.utils.automation_conditions.CustomAutomationCondition.on_cron","title":"<code>on_cron(cron_schedule, cron_timezone='UTC', ignore_asset_keys=None)</code>  <code>staticmethod</code>","text":"<p>Returns an AutomationCondition which will cause a target to be executed on a given cron schedule, after all of its dependencies have been updated since the latest tick of that cron schedule.</p> <p>For time partitioned assets, only the latest time partition will be considered.</p> <p>Commonly used for assets that are far downstream and have users that directly interact with them, and have sensitivity to late arriving dimensions.</p> Source code in <code>data_platform\\utils\\automation_conditions.py</code> <pre><code>@staticmethod\n@override\ndef on_cron(\n    cron_schedule: str,\n    cron_timezone: str = \"UTC\",\n    ignore_asset_keys: list[list[str]] | None = None,\n) -&gt; AndAutomationCondition:\n    \"\"\"Returns an AutomationCondition which will cause a target to be executed on a\n    given cron schedule, after all of its dependencies have been updated since the\n    latest tick of that cron schedule.\n\n    For time partitioned assets, only the latest time partition will be considered.\n\n    Commonly used for assets that are far downstream and have users that directly\n    interact with them, and have sensitivity to late arriving dimensions.\n    \"\"\"\n    ignore_asset_keys = ignore_asset_keys or []\n    return AutomationCondition.on_cron(cron_schedule, cron_timezone).ignore(\n        AssetSelection.assets(*ignore_asset_keys)\n    )\n</code></pre>"},{"location":"dagster/utils/automation_conditions/#data_platform.utils.automation_conditions.CustomAutomationCondition.on_schedule","title":"<code>on_schedule(cron_schedule, cron_timezone='utc')</code>  <code>staticmethod</code>","text":"<p>Returns an AutomationCondition which will cause a target to be executed on a given cron schedule, regardless of the state of its dependencies</p> <p>For time partitioned assets, only the latest time partition will be considered.</p> <p>Commonly used for assets in the ingestion layer that should always run on a scheduled basis, and have no way of knowing when the source system has updates.</p> Source code in <code>data_platform\\utils\\automation_conditions.py</code> <pre><code>@staticmethod\ndef on_schedule(\n    cron_schedule: str, cron_timezone: str = \"utc\"\n) -&gt; AutomationCondition:\n    \"\"\"Returns an AutomationCondition which will cause a target to be executed on a\n    given cron schedule, regardless of the state of its dependencies\n\n    For time partitioned assets, only the latest time partition will be considered.\n\n    Commonly used for assets in the ingestion layer that should always run on\n    a scheduled basis, and have no way of knowing when the source system has\n    updates.\n    \"\"\"\n    return (\n        AutomationCondition.in_latest_time_window()\n        &amp; AutomationCondition.cron_tick_passed(\n            cron_schedule, cron_timezone\n        ).since_last_handled()\n    ).with_label(f\"on_schedule({cron_schedule}, {cron_timezone})\")\n</code></pre>"},{"location":"dagster/utils/helpers/","title":"Helpers","text":""},{"location":"dagster/utils/helpers/#data_platform.utils.helpers.get_automation_condition_from_meta","title":"<code>get_automation_condition_from_meta(meta)</code>","text":"<p>Return an AutomationCondition if valid configuartion is provided in the meta. Meta should be of format dict in the following structure: .. code-block:: python     \"meta\":{         \"dagster\":{             \"automation_condition\": condition,             \"automation_condition_config\": {argument: value}         }     }</p> Source code in <code>data_platform\\utils\\helpers.py</code> <pre><code>def get_automation_condition_from_meta(\n    meta: dict[str, Any],\n) -&gt; dg.AutomationCondition | None:\n    \"\"\"Return an AutomationCondition if valid configuartion is provided in the meta.\n    Meta should be of format dict in the following structure:\n    .. code-block:: python\n        \"meta\":{\n            \"dagster\":{\n                \"automation_condition\": condition,\n                \"automation_condition_config\": {argument: value}\n            }\n        }\n    \"\"\"\n    condition_name = meta.get(\"automation_condition\")\n    if not condition_name:\n        return None\n\n    condition = CustomAutomationCondition.get_automation_condition(condition_name)\n    if not isinstance(condition, Callable):\n        raise KeyError(f\"Automation condition not found for key '{condition_name}'\")\n\n    condition_config = meta.get(\"automation_condition_config\", {}) or {}\n    if not isinstance(condition_config, dict):\n        raise ValueError(f\"Invalid condition config: '{condition_config}'\")\n\n    condition_config = sanitize_input_signature(condition, condition_config)\n    try:\n        return condition(**condition_config)\n    except Exception as e:\n        e.add_note(\n            \"'condition_config' is missing required keys\"\n            f\"for condition '{condition_name}'\"\n        )\n        raise\n</code></pre>"},{"location":"dagster/utils/helpers/#data_platform.utils.helpers.get_nested","title":"<code>get_nested(config, path)</code>","text":"<p>Helper function to safely traverse a nested dictionary that may have null values for a set key that is expected to be a dict. helpful because stream definitions that use only the default configs behave this way. .. code-block:: yaml streams:     source.table_one:     source.table_two:</p> Source code in <code>data_platform\\utils\\helpers.py</code> <pre><code>def get_nested(config: dict, path: list) -&gt; Any:\n    \"\"\"Helper function to safely traverse a nested dictionary that may have null values\n    for a set key that is expected to be a dict. helpful because stream definitions that\n    use only the default configs behave this way.\n    .. code-block:: yaml\n    streams:\n        source.table_one:\n        source.table_two:\n    \"\"\"\n    try:\n        for item in path:\n            config = config[item]\n        return config\n    except Exception:\n        ...\n    return None\n</code></pre>"},{"location":"dagster/utils/helpers/#data_platform.utils.helpers.get_partitions_def_from_meta","title":"<code>get_partitions_def_from_meta(meta)</code>","text":"<p>Return an TimeWindowPartitionsDefinition if valid configuartion is provided in the meta. - partition accepts the values: hourly, daily, weekly, monthly. - partition_start_date should be a iso format date, or timestamp.</p> <p>Meta should be of format dict in the following structure: .. code-block:: python    \"meta\":{        \"dagster\":{            \"partition\": \"daily\",            \"partition_start_date\": \"2025-01-01\"        }    }</p> Source code in <code>data_platform\\utils\\helpers.py</code> <pre><code>def get_partitions_def_from_meta(\n    meta: dict[str, Any],\n) -&gt; dg.TimeWindowPartitionsDefinition | None:\n    \"\"\"Return an TimeWindowPartitionsDefinition if valid configuartion is provided in\n    the meta.\n    - partition accepts the values: hourly, daily, weekly, monthly.\n    - partition_start_date should be a iso format date, or timestamp.\n\n    Meta should be of format dict in the following structure:\n    .. code-block:: python\n       \"meta\":{\n           \"dagster\":{\n               \"partition\": \"daily\",\n               \"partition_start_date\": \"2025-01-01\"\n           }\n       }\n    \"\"\"\n    try:\n        partition = meta.get(\"partition\")\n        partition_start_date = meta.get(\"partition_start_date\")\n\n        if partition and partition_start_date:\n            start_date = datetime.fromisoformat(partition_start_date)\n            if partition == \"hourly\":\n                return dg.HourlyPartitionsDefinition(\n                    start_date=start_date.strftime(\"%Y-%m-%d-%H:%M\")\n                )\n            if partition == \"daily\":\n                return dg.DailyPartitionsDefinition(\n                    start_date=start_date.strftime(\"%Y-%m-%d\")\n                )\n            if partition == \"weekly\":\n                return dg.WeeklyPartitionsDefinition(\n                    start_date=start_date.strftime(\"%Y-%m-%d\")\n                )\n            if partition == \"monthly\":\n                return dg.MonthlyPartitionsDefinition(\n                    start_date=start_date.strftime(\"%Y-%m-%d\")\n                )\n    except Exception:\n        ...\n    return None\n</code></pre>"},{"location":"dagster/utils/helpers/#data_platform.utils.helpers.sanitize_input_signature","title":"<code>sanitize_input_signature(func, kwargs)</code>","text":"<p>Remove any arguments that are not expected by the recieving function.</p> Source code in <code>data_platform\\utils\\helpers.py</code> <pre><code>def sanitize_input_signature(func: Callable, kwargs: dict) -&gt; dict:\n    \"\"\"Remove any arguments that are not expected by the recieving function.\"\"\"\n    sig = signature(func)\n    key_words = list(kwargs.keys())\n    expected_arguments = {argument for argument, _ in sig.parameters.items()}\n\n    for argument in key_words:\n        if argument not in expected_arguments:\n            kwargs.pop(argument)\n\n    return kwargs\n</code></pre>"},{"location":"dagster/utils/keyvault_stub/","title":"Keyvault stub","text":""},{"location":"dagster/utils/keyvault_stub/#data_platform.utils.keyvault_stub.SecretClient","title":"<code>SecretClient</code>","text":"<p>A stub keyvault to simulate an integration with Azure Keyvault. This would be replaced by a keyvault library.</p> Source code in <code>data_platform\\utils\\keyvault_stub.py</code> <pre><code>class SecretClient:\n    \"\"\"A stub keyvault to simulate an integration with Azure Keyvault. This would be\n    replaced by a keyvault library.\n    \"\"\"\n\n    def get_secret(self, secret_name: str) -&gt; str:\n        \"\"\"returns a secret from the keyvault\"\"\"\n        secrets = self.__secrets\n        location, _, attribute = secret_name.split(\"__\")\n        secret = secrets.get(location, {}).get(attribute)\n\n        return secret or \"\"\n\n    def __init__(\n        self, vault_url: str | None = None, credential: str | None = None\n    ) -&gt; None:\n        secrets = {\"SOURCE\": {}, \"DESTINATION\": {}}\n\n        env_path = Path(__file__).joinpath(*[\"..\"] * 3, \".env\").resolve()\n        set_env = os.getenv(\"TARGET\", \"\")\n        with open(env_path) as env:\n            for line in env:\n                line = line.strip()\n                if line.startswith(set_env.upper()) or line.startswith(\"ANY\"):\n                    key, secret = line.split(\"=\")\n                    env, location, attribute = key.split(\"__\")\n                    secrets[location][attribute] = secret\n\n        self.__secrets = secrets\n</code></pre>"},{"location":"dagster/utils/keyvault_stub/#data_platform.utils.keyvault_stub.SecretClient.get_secret","title":"<code>get_secret(secret_name)</code>","text":"<p>returns a secret from the keyvault</p> Source code in <code>data_platform\\utils\\keyvault_stub.py</code> <pre><code>def get_secret(self, secret_name: str) -&gt; str:\n    \"\"\"returns a secret from the keyvault\"\"\"\n    secrets = self.__secrets\n    location, _, attribute = secret_name.split(\"__\")\n    secret = secrets.get(location, {}).get(attribute)\n\n    return secret or \"\"\n</code></pre>"},{"location":"dagster/utils/secrets/","title":"Secrets","text":""},{"location":"dagster/utils/secrets/#data_platform.utils.secrets.get_secret","title":"<code>get_secret(env_var_name)</code>","text":"<p>A wrapper for a keyvault to integrate with the Dagster EnvVar class.</p> <p>Returns a secret from the keyvault and set it to an environment variable that can be used securly with dagsters EnvVar class.</p> Source code in <code>data_platform\\utils\\secrets.py</code> <pre><code>def get_secret(env_var_name: str) -&gt; dg.EnvVar:\n    \"\"\"A wrapper for a keyvault to integrate with the Dagster EnvVar class.\n\n    Returns a secret from the keyvault and set it to an environment variable that can be\n    used securly with dagsters EnvVar class.\n    \"\"\"\n    if secret := keyvault.get_secret(env_var_name):\n        os.environ[env_var_name] = secret\n        return dg.EnvVar(env_var_name)\n\n    raise ValueError(\n        f\"Secret for key '{env_var_name}' not found.\"\n        \"Please check that this is the correct key.\"\n    )\n</code></pre>"},{"location":"dbt/dbt_docs_site/","title":"dbt Docs Site","text":"click here if you are not redirected."},{"location":"dbt/getting_started/","title":"Getting Started","text":""},{"location":"dbt/getting_started/#what-is-dbt","title":"What is dbt","text":"<p>dbt (data build tool) is an open-source tool that empowers data analysts and engineers to transform data within their data warehouses by writing and executing SQL code. It focuses on the \"transform\" part of the ETL (Extract, Load, Transform) process, allowing users to define, test, and deploy data transformations using SQL. Essentially, dbt enables teams to build and manage their data models in a collaborative, version-controlled, and testable way, ensuring data quality and reliability.</p>"},{"location":"dbt/getting_started/#dbt-learning-material","title":"dbt Learning Material","text":"<p>dbt Fundamentals dbt Certified Developer</p>"},{"location":"dbt/getting_started/#contributing","title":"Contributing","text":"<p>Create a feature branch based off the <code>develop</code> branch. <code>feat/feature_name</code> Your branch name should be all lowercase, terse, and descriptive: <code>feat/adding_model_dim_product</code></p> <p>Once your changes are complete, submit a pull request to merge the changes into the develop branch, where it will be reviewed by your peers, and feedback will be provided.  Once approved it will be merged into the develop branch, and will be put into production on the next deployment.</p>"},{"location":"dbt/getting_started/#project-structure","title":"Project Structure","text":""},{"location":"dbt/getting_started/#naming-conventions","title":"Naming Conventions","text":"<p><code>type</code>_<code>schema</code>__<code>model_name</code>.sql</p> <p>Proper naming of models is important, as the file names are used to determine the schema and table names that are used to materialize the model on the warehouse.  It is also used to understand upstream and downstream dependencies within the orchestration tool.</p>"},{"location":"dbt/getting_started/#types","title":"Types","text":"<ul> <li>stg: Staged tables, that have had light transformations applied to be standardized in the warehouse, but generally are very close to the raw data.</li> <li>int:  Intermediate models that are used in service to apply common transformations that will be used by dims and facts.</li> <li>dim: Dimension models.</li> <li>fct: Fact models which provide measures.</li> </ul>"},{"location":"dbt/getting_started/#models","title":"Models","text":"<p>Models are sql or python code used to define a table to be materialized on the warehouse.  By defining a model using a select statement, dbt will handel the dml and ddl needed to create, and refresh the model.</p> <p>Models are organized into three folders: <code>staging</code>, <code>intermediate</code>, <code>marts</code>, as well as  two special kinds of materializations called <code>snapshots</code> and <code>seeds</code>.</p> <pre><code>---\nconfig:\n  theme: neutral\n---\ngraph LR\n  n1[\"raw\"]--&gt;n2[\"pii\"]\n  n1--&gt;n3[\"stg\"]\n  n1--&gt;snp\n  n3--&gt;n4[\"int\"]\n  n4--&gt;n5[\"mrt\"]\n</code></pre>"},{"location":"dbt/getting_started/#staging","title":"staging","text":"<p>Staging is a silver layer where raw data is made ready for general use.</p> <p>Models are organized by the source system they originate from.  Standardization happens at this layer: - All timestamp columns are converted to utc - Columns are renamed to be more explicit in what they represent, for example, a column named 'id' would be renamed to 'product_id' so it is easy to understand what it represents downstream. - data cleaning is applied, such as deduplication, or filtering of invalid records, for example, bot traffic is removed from website click data. - conversions are applied to standardize, for example if a source uses integers to represent monetary values, it should be converted to a decimal type. - PII data is anonymized / pseudo-anonymized and split off from non-pii data.  Vertical partitioning. - Light normalization and denormalization where it makes sense for example pre-joining tables with one to one cardinality, or converting a business key to a more usable key.</p> <p>Raw tables are also defined in the staging zone through a <code>__sources.yml</code> file, however they will be materialized upstream of dbt.</p>"},{"location":"dbt/getting_started/#intermediate","title":"intermediate","text":"<p>Intermediate is the layer between staging and marts, where common data transformations are applied so that code can be reused by multiple downstream tables.</p> <p>Models are organized by the primary mart that they intend to supply data to.  Models with general use cases across marts are put into a common grouping.</p>"},{"location":"dbt/getting_started/#marts","title":"marts","text":"<p>Marts are the gold layer where models are groups by business unit.  These models are primarily in service of a semantic layer, and general BI.  Data modeling best practices are followed (dimensional, cube, data vault, kimball, etc.) so that they can be queried easily and efficiently by business users.</p> <p>Models are organized by the business domain in which they serve.</p>"},{"location":"dbt/getting_started/#snapshots","title":"snapshots","text":"<p>Snapshots are used to capture the state of a table at a point in time.  Each time a snapshot is run, any records that have changed will have a new record inserted, the old version of the record will be marked as inactive, and will have a time period of when it was active from, and to.  This is commonly referred to as Slowly Changing Dimension Type Two (SCD2).</p>"},{"location":"dbt/getting_started/#seeds","title":"seeds","text":"<p>Seeds are CSV files that will be materialized as tables on the warehouse.  These are used for small datasets that do not contain sensitive information, and do not change often.  For example, a reference table maintained by the business that would not warrant a heavy pipeline to maintain.</p>"},{"location":"dbt/getting_started/#macros","title":"Macros","text":"<p>DBT utilizes jinja macros to programmatically inject values into your sql at run time.  By using macros you can apply powerful programmatic patterns that are not otherwise possible in normal sql.</p>"},{"location":"dbt/getting_started/#dagster-specific-config","title":"Dagster Specific Config","text":"<p>A meta key has been introduced to extend the functionality of dbt in dagster.  It is used to define automation's, checks, and partitions for finer control on how and when assets are materialized.</p> <p>Example config block: </p> <pre><code>{{-\n  config(\n    materialized = \"incremental\",\n    incremental_strategy = \"delete+insert\",\n    unique_key = \"id\",\n    meta = {\n      \"dagster\":{\n        \"partition\": \"daily\",\n        \"partition_start_date\": \"2025-07-01\",\n        \"automation_condition\": \"on_cron_no_deps\",\n        \"automation_condition_config\": {\n          \"cron_schedule\":\"@daily\",\n          \"cron_timezone\":\"utc\"\n        },\n        \"freshness_check\": {\n          \"lower_bound_delta_seconds\": 18000,\n          \"deadline_cron\": \"0 0 * * *\",\n          \"severity\": \"WARN\"\n        }\n      }\n    }\n  )\n-}}\n</code></pre>"},{"location":"dbt/getting_started/#partition-definitions","title":"Partition Definitions","text":"<p>When specified dagster will automatically create a partition for a given time span, allowing for incremental loading and backfills.  Typically an incremental materialization strategy is used, with a special jinja incremental macro that will accept time ranges from dagster.  See jinja below.</p>"},{"location":"dbt/getting_started/#partition","title":"partition","text":"<p>When specified dagster will automatically create a partition for a given time span, allowing for incremental loading and backfills.</p> <p>Accepted values: <code>hourly</code> <code>daily</code> <code>weekly</code> <code>monthly</code></p>"},{"location":"dbt/getting_started/#partition_start_date","title":"partition_start_date","text":"<p>When a partition is specified, you must also define the earliest time available in the upstream dependency.  This is given in ISO format:</p> <p>Accepted values: <code>YYYY-mm-dd</code> <code>YYYY-mm-dd HH:mm:ss</code></p>"},{"location":"dbt/getting_started/#jinja","title":"Jinja","text":"<p>When a partition is defined, a jinja macro is used to allow dagster to insert the time range of the backfill at runtime.  This is similar to the typical is_incremental pattern, but uses <code>var(\"min_date)</code> and <code>var(\"max_date\")</code>.</p> <pre><code>{% if is_incremental() -%}\n  where 1=1 \n    and hit_at &gt;= '{{ var(\"min_date\") }}'\n    and hit_at &lt;= '{{ var(\"max_date\") }}'\n{%- endif %}\n</code></pre>"},{"location":"dbt/getting_started/#asset-checks","title":"Asset Checks","text":""},{"location":"dbt/getting_started/#freshness_check","title":"freshness_check","text":""},{"location":"dbt/getting_started/#cron_deadline","title":"cron_deadline","text":"<p>Provide a deadline for when it is expected the asset to be available.  Required for partitioned modeles.</p> <p>Accepted values: <code>str</code> cron expression.</p>"},{"location":"dbt/getting_started/#lower_bound_delta_seconds","title":"lower_bound_delta_seconds","text":"<p>Provide a delta for when it is expected the asset to be refreshed since last materialization, or cron deadline.</p> <p>Accepted values: <code>float</code> seconds since last successful materialization, or cron deadline</p>"},{"location":"dbt/getting_started/#severity","title":"severity","text":"<p>Define the severity of the check in the case of failure.</p> <p>Accepted values: <code>WARN</code> <code>ERROR</code></p>"},{"location":"dbt/getting_started/#declarative-automation-definitions","title":"Declarative Automation Definitions","text":"<p>Instead of using fixed schedules, declarative automation is used.  With this methodology, the model will be aware of upstream assets, and will materialize as soon as it is able given the condition set.</p>"},{"location":"dbt/getting_started/#automation_condition","title":"automation_condition","text":"<p>Set under what conditions the asset should be materialized.  When the condition requires additional configuration, automation_condition_config is used.</p> <p>accepted_values: <code>manual</code> <code>on_cron_no_deps</code> <code>eager</code> <code>missing_or_changed</code> <code>lazy</code> <code>lazy_on_cron</code> <code>eager_with_deps_checks</code> <code>...</code></p> <p>Reference: elt_core/defs/automation_conditions.py</p>"},{"location":"dbt/getting_started/#automation_condition_config","title":"automation_condition_config","text":"<p>Some automation conditions require configuration, when this is the case, the configuration will be passed through this argument.</p> <p>accepted_values: <code>list</code> <code>dict</code></p> <p>Reference: elt_core/defs/automation_conditions.py</p>"},{"location":"dbt/getting_started/#available-automation-conditions","title":"Available Automation Conditions:","text":"<ul> <li>eager: Will materialize every time an upstream dependency is materialized.</li> <li>eager_with_deps_checks: Will materialize when any upstream dependency refreshes, but only after the upstream asset passes its quality checks.</li> <li>lazy: Will materialize only when upstream assets require it.</li> <li>lazy_on_cron: Will materialize only when upstream assets require it, but only once per cron schedule.</li> <li>missing_or_changed: Will materialize when the code has changed since the last run.</li> <li>manual: Asset will only materialize from a manual run in dagster.</li> <li>on_cron_no_deps: Will materialize on the cron schedule regardless of the state of upstream assets.</li> </ul> <p>additional conditions will be added over time, for the most up to date list, reference: elt_core/defs/automation_conditions.py </p>"}]}